{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jiFSwxk5cWTa"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qPaUL1ZltkFd"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque, defaultdict\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "De0mzFn4eQ47"
      },
      "outputs": [],
      "source": [
        "###TODO: DELETE THIS FOR FINAL VERSION\n",
        "GITHUB_USER = \"julialopezgomez\"\n",
        "GITHUB_TOKEN = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL3sMC4ezjv4",
        "outputId": "4cd7fe7b-d329-4426-d889-f295a38ee5ec"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3911631072.py, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    cat > ~/.netrc <<EOF\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "###TODO: DELETE THIS FOR FINAL VERSION\n",
        "%%bash\n",
        "cat > ~/.netrc <<EOF\n",
        "machine github.com\n",
        "  login julialopezgomez\n",
        "  password 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\n",
        "EOF\n",
        "\n",
        "# Secure it so only you can read it:\n",
        "chmod 600 ~/.netrc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRxMk0yTfIgp",
        "outputId": "a4baeca3-e2c7-43c9-d2c7-68d5a95f2000"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'monte-carlo-manipulation'...\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "###TODO: DELETE THIS FOR FINAL VERSION\n",
        "# configure git to use your token over HTTPS\n",
        "git config --global user.email \"s2107370@ed.ac.uk\"\n",
        "git config --global user.name  \"julialopezgomez\"\n",
        "###\n",
        "\n",
        "\n",
        "# clone via token\n",
        "git clone https://github.com/julialopezgomez/monte-carlo-manipulation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWOo5C-xfq_V",
        "outputId": "7027bb2f-1eff-41f7-e93e-7f28d3d93f71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Collecting torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl (from -r src/requirements.txt (line 2))\n",
            "  Using cached https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl (1843.9 MB)\n",
            "Collecting torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl (from -r src/requirements.txt (line 3))\n",
            "  Using cached https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "Requirement already satisfied: torchaudio==2.0.2 in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: ipywidgets==8.0.4 in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 7)) (8.0.4)\n",
            "Requirement already satisfied: widgetsnbextension==4.0.5 in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 8)) (4.0.5)\n",
            "Requirement already satisfied: jupyterlab-widgets in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 9)) (3.0.14)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 12)) (1.26.4)\n",
            "Collecting scipy<2.0.0,>=1.11.4 (from -r src/requirements.txt (line 13))\n",
            "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: pymcubes==0.1.4 in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 14)) (0.1.4)\n",
            "Requirement already satisfied: quadprog in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 15)) (0.1.13)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 18)) (1.6.0)\n",
            "Requirement already satisfied: mosek in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 19)) (11.0.20)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 23)) (5.24.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 24)) (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 25)) (0.19.10)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 28)) (1.1.1)\n",
            "Requirement already satisfied: gurobipy in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 29)) (12.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (11.2.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (6.17.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (5.7.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (18.1.8)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy->-r src/requirements.txt (line 18)) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy->-r src/requirements.txt (line 18)) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy->-r src/requirements.txt (line 18)) (3.2.7.post2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->-r src/requirements.txt (line 23)) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly->-r src/requirements.txt (line 23)) (24.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (6.0.2)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (75.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r src/requirements.txt (line 25)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r src/requirements.txt (line 25)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r src/requirements.txt (line 25)) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (2025.4.26)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r src/requirements.txt (line 28)) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r src/requirements.txt (line 28)) (0.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb->-r src/requirements.txt (line 25)) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r src/requirements.txt (line 25)) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r src/requirements.txt (line 25)) (5.0.2)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (1.6.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (6.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (4.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (5.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (2.9.0.post0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy->-r src/requirements.txt (line 18)) (1.4.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (1.3.0)\n",
            "Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37.7/37.7 MB 130.0 MB/s eta 0:00:00\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "Successfully installed scipy-1.15.3\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "cd monte-carlo-manipulation\n",
        "pip install --upgrade pip\n",
        "pip install -r src/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w9RZ1UOrtkFg"
      },
      "outputs": [],
      "source": [
        "# MCTS / AlphaZero params\n",
        "NUM_SIMS     = 800       # simulations per move\n",
        "CPUCT        = 1.41       # PUCT exploration constant\n",
        "TAU          = 1.0       # temperature for π = N^(1/τ)\n",
        "# Training params\n",
        "BATCH_SIZE   = 64\n",
        "LR           = 1e-3\n",
        "EVAL_INTERVAL= 10       # eval every 100 self-play games\n",
        "TARGET_SR    = 0.95      # stop when success rate ≥ 95%\n",
        "REGULARIZATION = 1e-4    # L2 regularization weight decay constant\n",
        "MAX_EPISODES = 100 # max number of self-play episodes\n",
        "\n",
        "NUM_EVAL     = 50\n",
        "BUFFER_SIZE   = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Ae98cEP9tkFg",
        "outputId": "82969164-ed93-4648-f660-13785dd0b010"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e0376b73fa1f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAlphaZeroNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "class AlphaZeroNet(nn.Module):\n",
        "    def __init__(self, n_states, n_actions, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(n_states, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # policy head\n",
        "        self.policy_head = nn.Linear(hidden_dim, n_actions)\n",
        "        # value head\n",
        "        self.value_head  = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: one-hot or feature vector of shape (batch, n_states)\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        p = F.log_softmax(self.policy_head(h), dim=1)  # log-probs\n",
        "        v = torch.tanh(self.value_head(h))             # in [-1,1]\n",
        "        return p, v.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "QxPpY3j7tkFh",
        "outputId": "1cf812f6-1976-477e-b297-47429c38183b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-200785dfb7f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FrozenLake-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_slippery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ansi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using device:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "# --- Main Execution ---\n",
        "def make_env():\n",
        "    return gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "env    = make_env()\n",
        "nS, nA = env.observation_space.n, env.action_space.n\n",
        "net    = AlphaZeroNet(nS, nA).to(device)\n",
        "opt    = optim.Adam(net.parameters(), lr=LR, weight_decay=REGULARIZATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eTx5U6oHtkFh"
      },
      "outputs": [],
      "source": [
        "# --- MCTS Node Class ---\n",
        "class MCTSNode:\n",
        "    def __init__(self,\n",
        "                 state,\n",
        "                 make_env,\n",
        "                 parent=None,\n",
        "                 action=None,\n",
        "                 prior=0.0,\n",
        "                 cpuct=1.41,\n",
        "                 device='cpu',\n",
        "                 verbose=False):\n",
        "\n",
        "        self.state = state\n",
        "        self.make_env = make_env\n",
        "        self.env = make_env()\n",
        "        self.nS = env.observation_space.n\n",
        "        self.nA = env.action_space.n\n",
        "        self.parent = parent\n",
        "        self.action = action         # action taken to reach this node\n",
        "        self.prior   = prior          # P(s,a) from network\n",
        "        self.cpuct  = cpuct          # exploration constant\n",
        "        self.children= {}             # action → child_node\n",
        "        self.N       = defaultdict(int)   # visit counts per child\n",
        "        self.W       = defaultdict(float) # total value per child\n",
        "        self.Q       = defaultdict(float) # mean value per child\n",
        "        self.device = device\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return len(self.children) == 0\n",
        "\n",
        "    def expand(self, net):\n",
        "        # get network outputs for this state\n",
        "        s_tensor = F.one_hot(torch.tensor([self.state], device=self.device), self.nS).float()\n",
        "        s_tensor = s_tensor.to(self.device)\n",
        "        logp, value = net(s_tensor)\n",
        "        p = logp.exp().detach().cpu().numpy()[0]\n",
        "\n",
        "        # create children with priors\n",
        "        for action in range(self.nA):\n",
        "            if action not in self.children:\n",
        "\n",
        "\n",
        "                # Copy the environment by creating a new instance\n",
        "                env_copy = self.make_env()\n",
        "                env_copy.reset()\n",
        "                env_copy.unwrapped.s = self.state\n",
        "\n",
        "                # Perform the action in the copied environment\n",
        "                obs, reward, terminated, truncated, _ = env_copy.step(action)\n",
        "\n",
        "                if np.array_equal(obs, self.state):\n",
        "                    # If the state is the same, we don't need to create a new child\n",
        "                    continue\n",
        "\n",
        "                self.children[action] = MCTSNode(\n",
        "                    state=obs,\n",
        "                    make_env=self.make_env,\n",
        "                    parent=self,\n",
        "                    action=action,\n",
        "                    prior=p[action],\n",
        "                    cpuct=self.cpuct,\n",
        "                    device=self.device,\n",
        "                    verbose=self.verbose\n",
        "                    )\n",
        "\n",
        "                if self.verbose:\n",
        "                    print(f\"probabilities: {p}\")\n",
        "                    print(f\"Expanding node {self.state} with action {action} and prior {p[action]}\")\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Expanded node {self.state} with children: {[c.state for c in self.children.values()]}\")\n",
        "            print(f\"Node {self.state} has value {value.item()} and prior {p}\")\n",
        "        return value.item()\n",
        "\n",
        "    def select(self, c_puct=CPUCT):\n",
        "        # pick action that maximizes Q + U\n",
        "        total_N = sum(self.N.values())\n",
        "        best_action, best_score = None, -float('inf')\n",
        "        for action, child in self.children.items():\n",
        "            U = c_puct * child.prior * math.sqrt(total_N) / (1 + self.N[action])\n",
        "            score = self.Q[action] + U\n",
        "            if score > best_score:\n",
        "                best_score, best_action = score, action\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Selected action {best_action} with score {best_score} (Q={self.Q[best_action]}, U={U})\")\n",
        "\n",
        "        return best_action, self.children[best_action]\n",
        "\n",
        "    def backpropagate(self, action, value):\n",
        "        \"\"\" Backpropagate the value of the simulation to this node.\n",
        "\n",
        "        Args:\n",
        "            action (int): The action taken to reach this node.\n",
        "            value (float): The network-predicted value for child node.\n",
        "        \"\"\"\n",
        "        self.W[action] += value\n",
        "        self.N[action] += 1\n",
        "        self.Q[action] = self.W[action] / self.N[action]\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Backpropagating value {value} for action {action} in node {self.state}\")\n",
        "            print(f\"Node {self.state} updated: W={self.W[action]}, N={self.N[action]}, Q={self.Q[action]}\")\n",
        "\n",
        "        node = self\n",
        "        if node.parent:\n",
        "            node.parent.backpropagate(self.action, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "5K6BJ8vktkFi"
      },
      "outputs": [],
      "source": [
        "def run_mcts(root_state,\n",
        "             net,\n",
        "             make_env,\n",
        "             num_sims=1000,\n",
        "             cpuct=1.41,\n",
        "             tau=1.,\n",
        "             device='cpu',\n",
        "             verbose=False):\n",
        "\n",
        "    root = MCTSNode(\n",
        "                    state=root_state,\n",
        "                    make_env=make_env,\n",
        "                    cpuct=cpuct,\n",
        "                    device=device,\n",
        "                    verbose=verbose\n",
        "                    )\n",
        "\n",
        "    # expand root with network evaluation:\n",
        "    value = root.expand(net)\n",
        "    if verbose:\n",
        "        print(f\"Root node {root.state} expanded with value {value}\")\n",
        "\n",
        "    for _ in range(num_sims):\n",
        "        node = root\n",
        "\n",
        "        # Selection\n",
        "        while not node.is_leaf():\n",
        "            prev_node = node\n",
        "            action, node = node.select()\n",
        "            if verbose:\n",
        "                print(f\"Selected action {action} for node {prev_node.state} -> {node.state}\")\n",
        "\n",
        "        # Expansion and evaluation with network\n",
        "        value = node.expand(net)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Node {node.state} expanded with value {value}\")\n",
        "\n",
        "        # Backpropagation (up one level)\n",
        "        assert node.action == action, f\"Expected action {action}, but got {node.action}\"\n",
        "        node.parent.backpropagate(node.action, value)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Finished backpropagation of value {value} for action {action} in node {node.state}\")\n",
        "\n",
        "    # build visit‐count distribution π\n",
        "    counts = np.array([root.N[a] for a in range(root.nA)], dtype=np.float32)\n",
        "    # apply temperature\n",
        "    counts = counts**(1/tau)\n",
        "    pi = counts / counts.sum()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Visit counts: {root.N}\")\n",
        "        print(f\"Action probabilities: {pi}\")\n",
        "\n",
        "    return pi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "DUksqoLLtkFi"
      },
      "outputs": [],
      "source": [
        "def self_play_episode(net, make_env, device='cpu', verbose=False):\n",
        "    \"\"\"Run one game of self-play, return list of training tuples (s,π), and z.\"\"\"\n",
        "    data = []\n",
        "    env = make_env()\n",
        "    nA = env.action_space.n\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    i = 0\n",
        "\n",
        "    while not done:\n",
        "        if verbose:\n",
        "            print(f\"Step {i}: state {state}\")\n",
        "\n",
        "        # get action probabilities from MCTS\n",
        "        if verbose:\n",
        "            print(f\"Running MCTS for state {state}\")\n",
        "        pi = run_mcts(state,\n",
        "                        net,\n",
        "                        make_env=make_env,\n",
        "                        num_sims=NUM_SIMS,\n",
        "                        cpuct=CPUCT,\n",
        "                        tau=TAU,\n",
        "                        device=device,\n",
        "                        verbose=verbose\n",
        "                      )\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Action probabilities: {pi}\")\n",
        "\n",
        "        # store (s,π) pair\n",
        "        data.append((state, pi))\n",
        "\n",
        "        # pick action (you can sample or argmax)\n",
        "        action = np.random.choice(nA, p=pi)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        if verbose:\n",
        "            print(f\"Action {action} taken, new state {state}, reward {reward}\")\n",
        "\n",
        "            if done:\n",
        "                print(f\"Game ended with reward {reward}\")\n",
        "\n",
        "        # update state\n",
        "        state = obs\n",
        "\n",
        "    # z = final reward (0 for fail, 1 for success)\n",
        "    z = float(reward)\n",
        "    return data, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "gHRsEKZGtkFi"
      },
      "outputs": [],
      "source": [
        "def train_on_batch(batch, net, env, opt, device='cpu'):\n",
        "    states, pis, zs = zip(*batch)\n",
        "    nS = env.observation_space.n\n",
        "\n",
        "    # one-hot encode states\n",
        "    X = F.one_hot(torch.tensor(states), nS).float()\n",
        "\n",
        "    target_pi = torch.tensor(pis, dtype=torch.float32, device=device)\n",
        "    target_z  = torch.tensor(zs, dtype=torch.float32, device=device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    logp, v = net(X)\n",
        "    loss_p = (-target_pi * logp).sum(dim=1).mean()\n",
        "    loss_v = F.mse_loss(v, target_z)\n",
        "    loss   = loss_p + loss_v\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    return loss_p.item(), loss_v.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "7AXP1snFtkFi"
      },
      "outputs": [],
      "source": [
        "# def evaluate(net, make_env, n_games=100, device='cpu', verbose=False):\n",
        "#     \"\"\"\n",
        "#     Runs n_games episodes using greedy AlphaZero MCTS (argmax over visit counts)\n",
        "#     and returns the fraction of games that reach the goal (reward==1).\n",
        "#     \"\"\"\n",
        "#     wins = 0\n",
        "#     env = make_env()\n",
        "#     for _ in tqdm(range(n_games), desc=\"Evaluation\", leave=True):\n",
        "#         state, _ = env.reset()\n",
        "#         done = False\n",
        "#         while not done:\n",
        "#             # 1) Run MCTS to get the search policy π\n",
        "#             pi = run_mcts(\n",
        "#                 state,\n",
        "#                 net,\n",
        "#                 make_env=make_env,\n",
        "#                 num_sims=NUM_SIMS,\n",
        "#                 cpuct=CPUCT,\n",
        "#                 tau=TAU,\n",
        "#                 device=device,\n",
        "#                 verbose=verbose\n",
        "#                 )       # returns a vector of length nA\n",
        "\n",
        "#             # 2) Pick the action with highest probability (greedy)\n",
        "#             action = int(np.argmax(pi))\n",
        "\n",
        "#             # 3) Step the env\n",
        "#             state, reward, terminated, truncated, _ = env.step(action)\n",
        "#             done = terminated or truncated\n",
        "\n",
        "#         # FrozenLake returns reward=1.0 only if you reach the goal\n",
        "#         if reward == 1.0:\n",
        "#             wins += 1\n",
        "\n",
        "\n",
        "\n",
        "#     success_rate = wins / n_games\n",
        "\n",
        "#     if verbose:\n",
        "#         print(f\"Success rate: {success_rate:.2f} ({wins}/{n_games})\")\n",
        "\n",
        "#     return success_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egovqcc-tkFj",
        "outputId": "2f0eff59-c576-4cb2-9d90-14195898f144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episodes:  17%|█▋        | 1/6 [01:47<08:59, 107.99s/ep]\n",
            "Episodes:  17%|█▋        | 1/6 [00:14<01:12, 14.51s/ep]\n",
            "Evaluation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "                                                  \u001b[A\n",
            "Evaluation:   5%|▌         | 1/20 [00:08<02:35,  8.19s/it]\u001b[A\n",
            "Evaluation:  10%|█         | 2/20 [00:16<02:31,  8.39s/it]\u001b[A\n",
            "Evaluation:  15%|█▌        | 3/20 [00:24<02:20,  8.27s/it]\u001b[A\n",
            "Evaluation:  20%|██        | 4/20 [00:33<02:13,  8.32s/it]\u001b[A\n",
            "Evaluation:  25%|██▌       | 5/20 [00:41<02:03,  8.23s/it]\u001b[A\n",
            "Evaluation:  30%|███       | 6/20 [00:49<01:55,  8.28s/it]\u001b[A\n",
            "Evaluation:  35%|███▌      | 7/20 [00:57<01:46,  8.21s/it]\u001b[A\n",
            "Evaluation:  40%|████      | 8/20 [01:06<01:39,  8.27s/it]\u001b[A\n",
            "Evaluation:  45%|████▌     | 9/20 [01:14<01:31,  8.31s/it]\u001b[A\n",
            "Evaluation:  50%|█████     | 10/20 [01:22<01:22,  8.24s/it]\u001b[A\n",
            "Evaluation:  55%|█████▌    | 11/20 [01:31<01:14,  8.28s/it]\u001b[A\n",
            "Evaluation:  60%|██████    | 12/20 [01:39<01:05,  8.23s/it]\u001b[A\n",
            "Evaluation:  65%|██████▌   | 13/20 [01:47<00:57,  8.28s/it]\u001b[A\n",
            "Evaluation:  70%|███████   | 14/20 [01:55<00:49,  8.24s/it]\u001b[A\n",
            "Evaluation:  75%|███████▌  | 15/20 [02:04<00:41,  8.27s/it]\u001b[A\n",
            "Evaluation:  80%|████████  | 16/20 [02:12<00:32,  8.22s/it]\u001b[A\n",
            "Evaluation:  85%|████████▌ | 17/20 [02:20<00:24,  8.27s/it]\u001b[A\n",
            "Evaluation:  90%|█████████ | 18/20 [02:28<00:16,  8.23s/it]\u001b[A\n",
            "Evaluation:  95%|█████████▌| 19/20 [02:37<00:08,  8.28s/it]\u001b[A\n",
            "Evaluation: 100%|██████████| 20/20 [02:45<00:00,  8.23s/it]\u001b[A\n",
            "Episodes:  50%|█████     | 3/6 [03:31<03:28, 69.37s/ep, last_sr=0.00, best_sr=0.00] \n",
            "Evaluation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluation:   5%|▌         | 1/20 [00:08<02:33,  8.09s/it]\u001b[A\n",
            "Evaluation:  10%|█         | 2/20 [00:16<02:25,  8.09s/it]\u001b[A\n",
            "Evaluation:  15%|█▌        | 3/20 [00:24<02:19,  8.23s/it]\u001b[A\n",
            "Evaluation:  20%|██        | 4/20 [00:32<02:12,  8.28s/it]\u001b[A\n",
            "Evaluation:  25%|██▌       | 5/20 [00:41<02:03,  8.23s/it]\u001b[A\n",
            "Evaluation:  30%|███       | 6/20 [00:49<01:54,  8.18s/it]\u001b[A\n",
            "Evaluation:  35%|███▌      | 7/20 [00:57<01:47,  8.26s/it]\u001b[A\n",
            "Evaluation:  40%|████      | 8/20 [01:05<01:39,  8.30s/it]\u001b[A\n",
            "Evaluation:  45%|████▌     | 9/20 [01:14<01:30,  8.25s/it]\u001b[A\n",
            "Evaluation:  50%|█████     | 10/20 [01:22<01:21,  8.20s/it]\u001b[A\n",
            "Evaluation:  55%|█████▌    | 11/20 [01:30<01:14,  8.26s/it]\u001b[A\n",
            "Evaluation:  60%|██████    | 12/20 [01:38<01:06,  8.28s/it]\u001b[A\n",
            "Evaluation:  65%|██████▌   | 13/20 [01:47<00:57,  8.22s/it]\u001b[A\n",
            "Evaluation:  70%|███████   | 14/20 [01:55<00:49,  8.17s/it]\u001b[A\n",
            "Evaluation:  75%|███████▌  | 15/20 [02:03<00:41,  8.23s/it]\u001b[A\n",
            "Evaluation:  80%|████████  | 16/20 [02:11<00:33,  8.28s/it]\u001b[A\n",
            "Evaluation:  85%|████████▌ | 17/20 [02:19<00:24,  8.23s/it]\u001b[A\n",
            "Evaluation:  90%|█████████ | 18/20 [02:28<00:16,  8.18s/it]\u001b[A\n",
            "Evaluation:  95%|█████████▌| 19/20 [02:36<00:08,  8.26s/it]\u001b[A\n",
            "Evaluation: 100%|██████████| 20/20 [02:44<00:00,  8.29s/it]\u001b[A\n",
            "Episodes:  83%|████████▎ | 5/6 [06:57<01:20, 80.76s/ep, last_sr=0.00, best_sr=0.00] \n",
            "Evaluation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluation:   5%|▌         | 1/20 [00:08<02:34,  8.11s/it]\u001b[A\n",
            "Evaluation:  10%|█         | 2/20 [00:16<02:25,  8.09s/it]\u001b[A\n",
            "Evaluation:  15%|█▌        | 3/20 [00:24<02:19,  8.23s/it]\u001b[A\n",
            "Evaluation:  20%|██        | 4/20 [00:32<02:10,  8.16s/it]\u001b[A\n",
            "Evaluation:  25%|██▌       | 5/20 [00:41<02:03,  8.25s/it]\u001b[A\n",
            "Evaluation:  30%|███       | 6/20 [00:49<01:56,  8.29s/it]\u001b[A\n",
            "Evaluation:  35%|███▌      | 7/20 [00:57<01:46,  8.23s/it]\u001b[A\n",
            "Evaluation:  40%|████      | 8/20 [01:05<01:38,  8.17s/it]\u001b[A\n",
            "Evaluation:  45%|████▌     | 9/20 [01:13<01:30,  8.24s/it]\u001b[A\n",
            "Evaluation:  50%|█████     | 10/20 [01:22<01:23,  8.31s/it]\u001b[A\n",
            "Evaluation:  55%|█████▌    | 11/20 [01:30<01:14,  8.25s/it]\u001b[A\n",
            "Evaluation:  60%|██████    | 12/20 [01:38<01:05,  8.19s/it]\u001b[A\n",
            "Evaluation:  65%|██████▌   | 13/20 [01:47<00:57,  8.26s/it]\u001b[A\n",
            "Evaluation:  70%|███████   | 14/20 [01:55<00:49,  8.21s/it]\u001b[A\n",
            "Evaluation:  75%|███████▌  | 15/20 [02:03<00:41,  8.27s/it]\u001b[A\n",
            "Evaluation:  80%|████████  | 16/20 [02:11<00:33,  8.32s/it]\u001b[A\n",
            "Evaluation:  85%|████████▌ | 17/20 [02:20<00:24,  8.27s/it]\u001b[A\n",
            "Evaluation:  90%|█████████ | 18/20 [02:28<00:16,  8.21s/it]\u001b[A\n",
            "Evaluation:  95%|█████████▌| 19/20 [02:36<00:08,  8.27s/it]\u001b[A\n",
            "Evaluation: 100%|██████████| 20/20 [02:44<00:00,  8.30s/it]\u001b[A\n",
            "Episodes: 100%|██████████| 6/6 [09:55<00:00, 99.27s/ep, last_sr=0.00, best_sr=0.00] \n"
          ]
        }
      ],
      "source": [
        "verbose = False\n",
        "replay     = deque(maxlen=BUFFER_SIZE)\n",
        "best_sr    = 0.0\n",
        "all_losses = []\n",
        "\n",
        "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ───────── OUTER BAR: Episodes ─────────\n",
        "episode_bar = tqdm(total=MAX_EPISODES,\n",
        "                   desc=\"Episodes\",\n",
        "                   position=0,\n",
        "                   unit=\"ep\")\n",
        "\n",
        "for episode in range(1, MAX_EPISODES+1):\n",
        "    # ---- 1) SELF-PLAY / DATA COLLECTION ----\n",
        "    data, z = self_play_episode(net, make_env, device=device)\n",
        "    for (s, pi) in data:\n",
        "        replay.append((s, pi, z))\n",
        "\n",
        "    # ---- 2) TRAINING BATCHES ----\n",
        "    if len(replay) >= BATCH_SIZE:\n",
        "        n_batches = len(replay) // BATCH_SIZE\n",
        "\n",
        "        # create an inner bar at position=1\n",
        "        batch_bar = tqdm(total=n_batches,\n",
        "                         desc=\"Batches\",\n",
        "                         position=1,\n",
        "                         leave=False,\n",
        "                         unit=\"batch\")\n",
        "\n",
        "        for _ in range(n_batches):\n",
        "            batch = random.sample(replay, BATCH_SIZE)\n",
        "            p_loss, v_loss = train_on_batch(\n",
        "                batch, net, make_env(), opt, device\n",
        "            )\n",
        "            all_losses.append((p_loss, v_loss))\n",
        "\n",
        "            # stream losses into the batch bar\n",
        "            batch_bar.set_postfix({\n",
        "                \"p_loss\": f\"{p_loss:.3f}\",\n",
        "                \"v_loss\": f\"{v_loss:.3f}\"\n",
        "            })\n",
        "            batch_bar.update()\n",
        "\n",
        "        # batch_bar.close()\n",
        "\n",
        "    # ---- 3) PERIODIC EVALUATION ----\n",
        "    if episode % EVAL_INTERVAL == 0:\n",
        "        # evaluation bar nested at position=1\n",
        "        eval_bar = tqdm(total=NUM_EVAL,\n",
        "                        desc=\"Evaluation\",\n",
        "                        position=1,\n",
        "                        leave=True,\n",
        "                        unit=\"it\")\n",
        "        wins = 0\n",
        "        for _ in range(NUM_EVAL):\n",
        "            env = make_env()\n",
        "            state, _ = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                pi = run_mcts(\n",
        "                    state, net, make_env=make_env,\n",
        "                    num_sims=NUM_SIMS, cpuct=CPUCT, tau=TAU,\n",
        "                    device=device, verbose=False\n",
        "                )\n",
        "                a = int(np.argmax(pi))\n",
        "                state, reward, term, trunc, _ = env.step(a)\n",
        "                done = term or trunc\n",
        "            if reward == 1.0:\n",
        "                wins += 1\n",
        "\n",
        "            eval_bar.update()\n",
        "        eval_bar.close()\n",
        "\n",
        "        sr = wins / NUM_EVAL\n",
        "        # save best\n",
        "        if sr > best_sr:\n",
        "            best_sr = sr\n",
        "            torch.save(net.state_dict(),\n",
        "                       f\"models/checkpoint_ep{episode}.pt\")\n",
        "\n",
        "        # stream SR info into the episode bar\n",
        "        episode_bar.set_postfix({\n",
        "            \"last_sr\": f\"{sr:.2f}\",\n",
        "            \"best_sr\": f\"{best_sr:.2f}\"\n",
        "        })\n",
        "\n",
        "        if sr >= TARGET_SR:\n",
        "            episode_bar.write(f\"✔ Target SR reached at ep {episode} ({sr:.2f})\")\n",
        "            break\n",
        "\n",
        "    # advance the episode bar by 1\n",
        "    episode_bar.update()\n",
        "\n",
        "episode_bar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPFtAilatkFj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfLn8zLC3d0J",
        "outputId": "87022334-ca52-4675-ff2f-e230aa17a08f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expanded node 0 with children: [4, 1]\n",
            "Simulating from 4 → 5 with action 2 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 0, visits=1, value=0.0\n",
            "Simulating from 1 → 2 with action 2 reward 0.0\n",
            "Simulating from 1 → 6 with action 1 reward 0.0\n",
            "Simulating from 1 → 10 with action 1 reward 0.0\n",
            "Simulating from 1 → 14 with action 1 reward 0.0\n",
            "Simulating from 1 → 10 with action 3 reward 0.0\n",
            "Simulating from 1 → 14 with action 1 reward 0.0\n",
            "Simulating from 1 → 14 with action 1 reward 0.0\n",
            "Simulating from 1 → 10 with action 3 reward 0.0\n",
            "Simulating from 1 → 6 with action 3 reward 0.0\n",
            "Simulating from 1 → 5 with action 0 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 0, visits=2, value=0.0\n",
            "Expanded node 4 with children: [8, 0]\n",
            "Simulating from 0 → 4 with action 1 reward 0.0\n",
            "Simulating from 0 → 4 with action 0 reward 0.0\n",
            "Simulating from 0 → 0 with action 3 reward 0.0\n",
            "Simulating from 0 → 4 with action 1 reward 0.0\n",
            "Simulating from 0 → 4 with action 0 reward 0.0\n",
            "Simulating from 0 → 8 with action 1 reward 0.0\n",
            "Simulating from 0 → 4 with action 3 reward 0.0\n",
            "Simulating from 0 → 5 with action 2 reward 0.0\n",
            "Backprop node 0, visits=1, value=0.0\n",
            "Backprop node 4, visits=2, value=0.0\n",
            "Backprop node 0, visits=3, value=0.0\n",
            "Expanded node 1 with children: [0, 2]\n",
            "Simulating from 2 → 6 with action 1 reward 0.0\n",
            "Simulating from 2 → 5 with action 0 reward 0.0\n",
            "Backprop node 2, visits=1, value=0.0\n",
            "Backprop node 1, visits=2, value=0.0\n",
            "Backprop node 0, visits=4, value=0.0\n",
            "Simulating from 8 → 8 with action 0 reward 0.0\n",
            "Simulating from 8 → 4 with action 3 reward 0.0\n",
            "Simulating from 8 → 0 with action 3 reward 0.0\n",
            "Simulating from 8 → 0 with action 3 reward 0.0\n",
            "Simulating from 8 → 0 with action 0 reward 0.0\n",
            "Simulating from 8 → 0 with action 0 reward 0.0\n",
            "Simulating from 8 → 4 with action 1 reward 0.0\n",
            "Simulating from 8 → 0 with action 3 reward 0.0\n",
            "Simulating from 8 → 4 with action 1 reward 0.0\n",
            "Simulating from 8 → 4 with action 0 reward 0.0\n",
            "Simulating from 8 → 0 with action 3 reward 0.0\n",
            "Simulating from 8 → 1 with action 2 reward 0.0\n",
            "Simulating from 8 → 0 with action 0 reward 0.0\n",
            "Simulating from 8 → 0 with action 3 reward 0.0\n",
            "Simulating from 8 → 0 with action 0 reward 0.0\n",
            "Simulating from 8 → 1 with action 2 reward 0.0\n",
            "Simulating from 8 → 5 with action 1 reward 0.0\n",
            "Backprop node 8, visits=1, value=0.0\n",
            "Backprop node 4, visits=3, value=0.0\n",
            "Backprop node 0, visits=5, value=0.0\n",
            "Simulating from 0 → 4 with action 1 reward 0.0\n",
            "Simulating from 0 → 8 with action 1 reward 0.0\n",
            "Simulating from 0 → 8 with action 0 reward 0.0\n",
            "Simulating from 0 → 4 with action 3 reward 0.0\n",
            "Simulating from 0 → 8 with action 1 reward 0.0\n",
            "Simulating from 0 → 9 with action 2 reward 0.0\n",
            "Simulating from 0 → 5 with action 3 reward 0.0\n",
            "Backprop node 0, visits=1, value=0.0\n",
            "Backprop node 1, visits=3, value=0.0\n",
            "Backprop node 0, visits=6, value=0.0\n",
            "Expanded node 8 with children: [9, 4]\n",
            "Simulating from 9 → 13 with action 1 reward 0.0\n",
            "Simulating from 9 → 9 with action 3 reward 0.0\n",
            "Simulating from 9 → 8 with action 0 reward 0.0\n",
            "Simulating from 9 → 4 with action 3 reward 0.0\n",
            "Simulating from 9 → 8 with action 1 reward 0.0\n",
            "Simulating from 9 → 4 with action 3 reward 0.0\n",
            "Simulating from 9 → 8 with action 1 reward 0.0\n",
            "Simulating from 9 → 12 with action 1 reward 0.0\n",
            "Backprop node 9, visits=1, value=0.0\n",
            "Backprop node 8, visits=2, value=0.0\n",
            "Backprop node 4, visits=4, value=0.0\n",
            "Backprop node 0, visits=7, value=0.0\n",
            "Expanded node 0 with children: [4, 1]\n",
            "Simulating from 1 → 2 with action 2 reward 0.0\n",
            "Simulating from 1 → 6 with action 1 reward 0.0\n",
            "Simulating from 1 → 5 with action 0 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 0, visits=2, value=0.0\n",
            "Backprop node 1, visits=4, value=0.0\n",
            "Backprop node 0, visits=8, value=0.0\n",
            "Expanded node 0 with children: [4, 1]\n",
            "Simulating from 4 → 0 with action 3 reward 0.0\n",
            "Simulating from 4 → 0 with action 0 reward 0.0\n",
            "Simulating from 4 → 4 with action 1 reward 0.0\n",
            "Simulating from 4 → 5 with action 2 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 0, visits=2, value=0.0\n",
            "Backprop node 4, visits=5, value=0.0\n",
            "Backprop node 0, visits=9, value=0.0\n",
            "Expanded node 2 with children: [1, 6, 3]\n",
            "Simulating from 3 → 2 with action 0 reward 0.0\n",
            "Simulating from 3 → 6 with action 1 reward 0.0\n",
            "Simulating from 3 → 10 with action 1 reward 0.0\n",
            "Simulating from 3 → 6 with action 3 reward 0.0\n",
            "Simulating from 3 → 7 with action 2 reward 0.0\n",
            "Backprop node 3, visits=1, value=0.0\n",
            "Backprop node 2, visits=2, value=0.0\n",
            "Backprop node 1, visits=5, value=0.0\n",
            "Backprop node 0, visits=10, value=0.0\n",
            "Simulating from 4 → 5 with action 2 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 8, visits=3, value=0.0\n",
            "Backprop node 4, visits=6, value=0.0\n",
            "Backprop node 0, visits=11, value=0.0\n",
            "Simulating from 4 → 8 with action 1 reward 0.0\n",
            "Simulating from 4 → 12 with action 1 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 0, visits=3, value=0.0\n",
            "Backprop node 1, visits=6, value=0.0\n",
            "Backprop node 0, visits=12, value=0.0\n",
            "Simulating from 1 → 2 with action 2 reward 0.0\n",
            "Simulating from 1 → 1 with action 0 reward 0.0\n",
            "Simulating from 1 → 5 with action 1 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 0, visits=3, value=0.0\n",
            "Backprop node 4, visits=7, value=0.0\n",
            "Backprop node 0, visits=13, value=0.0\n",
            "Simulating from 1 → 0 with action 0 reward 0.0\n",
            "Simulating from 1 → 0 with action 0 reward 0.0\n",
            "Simulating from 1 → 4 with action 1 reward 0.0\n",
            "Simulating from 1 → 8 with action 1 reward 0.0\n",
            "Simulating from 1 → 9 with action 2 reward 0.0\n",
            "Simulating from 1 → 13 with action 1 reward 0.0\n",
            "Simulating from 1 → 9 with action 3 reward 0.0\n",
            "Simulating from 1 → 10 with action 2 reward 0.0\n",
            "Simulating from 1 → 6 with action 3 reward 0.0\n",
            "Simulating from 1 → 5 with action 0 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 2, visits=3, value=0.0\n",
            "Backprop node 1, visits=7, value=0.0\n",
            "Backprop node 0, visits=14, value=0.0\n",
            "Expanded node 9 with children: [8, 13, 10]\n",
            "Simulating from 13 → 9 with action 3 reward 0.0\n",
            "Simulating from 13 → 5 with action 3 reward 0.0\n",
            "Backprop node 13, visits=1, value=0.0\n",
            "Backprop node 9, visits=2, value=0.0\n",
            "Backprop node 8, visits=4, value=0.0\n",
            "Backprop node 4, visits=8, value=0.0\n",
            "Backprop node 0, visits=15, value=0.0\n",
            "Expanded node 4 with children: [8, 0]\n",
            "Simulating from 8 → 12 with action 1 reward 0.0\n",
            "Backprop node 8, visits=1, value=0.0\n",
            "Backprop node 4, visits=2, value=0.0\n",
            "Backprop node 0, visits=4, value=0.0\n",
            "Backprop node 1, visits=8, value=0.0\n",
            "Backprop node 0, visits=16, value=0.0\n",
            "Expanded node 4 with children: [8, 0]\n",
            "Simulating from 0 → 0 with action 0 reward 0.0\n",
            "Simulating from 0 → 1 with action 2 reward 0.0\n",
            "Simulating from 0 → 1 with action 3 reward 0.0\n",
            "Simulating from 0 → 2 with action 2 reward 0.0\n",
            "Simulating from 0 → 1 with action 0 reward 0.0\n",
            "Simulating from 0 → 1 with action 3 reward 0.0\n",
            "Simulating from 0 → 0 with action 0 reward 0.0\n",
            "Simulating from 0 → 1 with action 2 reward 0.0\n",
            "Simulating from 0 → 2 with action 2 reward 0.0\n",
            "Simulating from 0 → 1 with action 0 reward 0.0\n",
            "Simulating from 0 → 2 with action 2 reward 0.0\n",
            "Simulating from 0 → 3 with action 2 reward 0.0\n",
            "Simulating from 0 → 2 with action 0 reward 0.0\n",
            "Simulating from 0 → 3 with action 2 reward 0.0\n",
            "Simulating from 0 → 7 with action 1 reward 0.0\n",
            "Backprop node 0, visits=1, value=0.0\n",
            "Backprop node 4, visits=2, value=0.0\n",
            "Backprop node 0, visits=4, value=0.0\n",
            "Backprop node 4, visits=9, value=0.0\n",
            "Backprop node 0, visits=17, value=0.0\n",
            "Simulating from 6 → 2 with action 3 reward 0.0\n",
            "Simulating from 6 → 1 with action 0 reward 0.0\n",
            "Simulating from 6 → 1 with action 3 reward 0.0\n",
            "Simulating from 6 → 1 with action 3 reward 0.0\n",
            "Simulating from 6 → 5 with action 1 reward 0.0\n",
            "Backprop node 6, visits=1, value=0.0\n",
            "Backprop node 2, visits=4, value=0.0\n",
            "Backprop node 1, visits=9, value=0.0\n",
            "Backprop node 0, visits=18, value=0.0\n",
            "Expanded node 4 with children: [8, 0]\n",
            "Simulating from 0 → 1 with action 2 reward 0.0\n",
            "Simulating from 0 → 0 with action 0 reward 0.0\n",
            "Simulating from 0 → 4 with action 1 reward 0.0\n",
            "Simulating from 0 → 8 with action 1 reward 0.0\n",
            "Simulating from 0 → 8 with action 0 reward 0.0\n",
            "Simulating from 0 → 9 with action 2 reward 0.0\n",
            "Simulating from 0 → 13 with action 1 reward 0.0\n",
            "Simulating from 0 → 9 with action 3 reward 0.0\n",
            "Simulating from 0 → 10 with action 2 reward 0.0\n",
            "Simulating from 0 → 11 with action 2 reward 0.0\n",
            "Backprop node 0, visits=1, value=0.0\n",
            "Backprop node 4, visits=2, value=0.0\n",
            "Backprop node 8, visits=5, value=0.0\n",
            "Backprop node 4, visits=10, value=0.0\n",
            "Backprop node 0, visits=19, value=0.0\n",
            "Expanded node 1 with children: [0, 2]\n",
            "Simulating from 2 → 3 with action 2 reward 0.0\n",
            "Simulating from 2 → 7 with action 1 reward 0.0\n",
            "Backprop node 2, visits=1, value=0.0\n",
            "Backprop node 1, visits=2, value=0.0\n",
            "Backprop node 0, visits=5, value=0.0\n",
            "Backprop node 1, visits=10, value=0.0\n",
            "Backprop node 0, visits=20, value=0.0\n",
            "Expanded node 1 with children: [0, 2]\n",
            "Simulating from 2 → 6 with action 1 reward 0.0\n",
            "Simulating from 2 → 2 with action 3 reward 0.0\n",
            "Simulating from 2 → 3 with action 2 reward 0.0\n",
            "Simulating from 2 → 3 with action 3 reward 0.0\n",
            "Simulating from 2 → 3 with action 2 reward 0.0\n",
            "Simulating from 2 → 3 with action 3 reward 0.0\n",
            "Simulating from 2 → 2 with action 0 reward 0.0\n",
            "Simulating from 2 → 2 with action 3 reward 0.0\n",
            "Simulating from 2 → 6 with action 1 reward 0.0\n",
            "Simulating from 2 → 2 with action 3 reward 0.0\n",
            "Simulating from 2 → 3 with action 2 reward 0.0\n",
            "Simulating from 2 → 3 with action 3 reward 0.0\n",
            "Simulating from 2 → 3 with action 3 reward 0.0\n",
            "Simulating from 2 → 2 with action 0 reward 0.0\n",
            "Simulating from 2 → 1 with action 0 reward 0.0\n",
            "Simulating from 2 → 1 with action 3 reward 0.0\n",
            "Simulating from 2 → 5 with action 1 reward 0.0\n",
            "Backprop node 2, visits=1, value=0.0\n",
            "Backprop node 1, visits=2, value=0.0\n",
            "Backprop node 0, visits=5, value=0.0\n",
            "Backprop node 4, visits=11, value=0.0\n",
            "Backprop node 0, visits=21, value=0.0\n",
            "Expanded node 1 with children: [0, 2]\n",
            "Simulating from 0 → 0 with action 0 reward 0.0\n",
            "Simulating from 0 → 1 with action 2 reward 0.0\n",
            "Simulating from 0 → 2 with action 2 reward 0.0\n",
            "Simulating from 0 → 3 with action 2 reward 0.0\n",
            "Simulating from 0 → 2 with action 0 reward 0.0\n",
            "Simulating from 0 → 6 with action 1 reward 0.0\n",
            "Simulating from 0 → 2 with action 3 reward 0.0\n",
            "Simulating from 0 → 6 with action 1 reward 0.0\n",
            "Simulating from 0 → 10 with action 1 reward 0.0\n",
            "Simulating from 0 → 6 with action 3 reward 0.0\n",
            "Simulating from 0 → 2 with action 3 reward 0.0\n",
            "Simulating from 0 → 1 with action 0 reward 0.0\n",
            "Simulating from 0 → 5 with action 1 reward 0.0\n",
            "Backprop node 0, visits=1, value=0.0\n",
            "Backprop node 1, visits=2, value=0.0\n",
            "Backprop node 2, visits=5, value=0.0\n",
            "Backprop node 1, visits=11, value=0.0\n",
            "Backprop node 0, visits=22, value=0.0\n",
            "Simulating from 8 → 4 with action 3 reward 0.0\n",
            "Simulating from 8 → 5 with action 2 reward 0.0\n",
            "Backprop node 8, visits=1, value=0.0\n",
            "Backprop node 9, visits=3, value=0.0\n",
            "Backprop node 8, visits=6, value=0.0\n",
            "Backprop node 4, visits=12, value=0.0\n",
            "Backprop node 0, visits=23, value=0.0\n",
            "Simulating from 0 → 4 with action 1 reward 0.0\n",
            "Simulating from 0 → 5 with action 2 reward 0.0\n",
            "Backprop node 0, visits=1, value=0.0\n",
            "Backprop node 4, visits=3, value=0.0\n",
            "Backprop node 0, visits=6, value=0.0\n",
            "Backprop node 1, visits=12, value=0.0\n",
            "Backprop node 0, visits=24, value=0.0\n",
            "Simulating from 8 → 8 with action 0 reward 0.0\n",
            "Simulating from 8 → 9 with action 2 reward 0.0\n",
            "Simulating from 8 → 13 with action 1 reward 0.0\n",
            "Simulating from 8 → 14 with action 2 reward 0.0\n",
            "Simulating from 8 → 14 with action 1 reward 0.0\n",
            "Simulating from 8 → 10 with action 3 reward 0.0\n",
            "Simulating from 8 → 9 with action 0 reward 0.0\n",
            "Simulating from 8 → 13 with action 1 reward 0.0\n",
            "Simulating from 8 → 13 with action 1 reward 0.0\n",
            "Simulating from 8 → 13 with action 1 reward 0.0\n",
            "Simulating from 8 → 12 with action 0 reward 0.0\n",
            "Backprop node 8, visits=1, value=0.0\n",
            "Backprop node 4, visits=3, value=0.0\n",
            "Backprop node 0, visits=6, value=0.0\n",
            "Backprop node 4, visits=13, value=0.0\n",
            "Backprop node 0, visits=25, value=0.0\n",
            "Expanded node 6 with children: [10, 2]\n",
            "Simulating from 10 → 9 with action 0 reward 0.0\n",
            "Simulating from 10 → 10 with action 2 reward 0.0\n",
            "Simulating from 10 → 11 with action 2 reward 0.0\n",
            "Backprop node 10, visits=1, value=0.0\n",
            "Backprop node 6, visits=2, value=0.0\n",
            "Backprop node 2, visits=6, value=0.0\n",
            "Backprop node 1, visits=13, value=0.0\n",
            "Backprop node 0, visits=26, value=0.0\n",
            "Simulating from 8 → 9 with action 2 reward 0.0\n",
            "Simulating from 8 → 10 with action 2 reward 0.0\n",
            "Simulating from 8 → 9 with action 0 reward 0.0\n",
            "Simulating from 8 → 13 with action 1 reward 0.0\n",
            "Simulating from 8 → 13 with action 1 reward 0.0\n",
            "Simulating from 8 → 9 with action 3 reward 0.0\n",
            "Simulating from 8 → 13 with action 1 reward 0.0\n",
            "Simulating from 8 → 12 with action 0 reward 0.0\n",
            "Backprop node 8, visits=1, value=0.0\n",
            "Backprop node 4, visits=3, value=0.0\n",
            "Backprop node 8, visits=7, value=0.0\n",
            "Backprop node 4, visits=14, value=0.0\n",
            "Backprop node 0, visits=27, value=0.0\n",
            "Simulating from 0 → 1 with action 2 reward 0.0\n",
            "Simulating from 0 → 2 with action 2 reward 0.0\n",
            "Simulating from 0 → 1 with action 0 reward 0.0\n",
            "Simulating from 0 → 5 with action 1 reward 0.0\n",
            "Backprop node 0, visits=1, value=0.0\n",
            "Backprop node 1, visits=3, value=0.0\n",
            "Backprop node 0, visits=7, value=0.0\n",
            "Backprop node 1, visits=14, value=0.0\n",
            "Backprop node 0, visits=28, value=0.0\n",
            "Simulating from 0 → 4 with action 1 reward 0.0\n",
            "Simulating from 0 → 0 with action 3 reward 0.0\n",
            "Simulating from 0 → 1 with action 2 reward 0.0\n",
            "Simulating from 0 → 1 with action 3 reward 0.0\n",
            "Simulating from 0 → 0 with action 0 reward 0.0\n",
            "Simulating from 0 → 0 with action 0 reward 0.0\n",
            "Simulating from 0 → 0 with action 3 reward 0.0\n",
            "Simulating from 0 → 0 with action 0 reward 0.0\n",
            "Simulating from 0 → 0 with action 0 reward 0.0\n",
            "Simulating from 0 → 1 with action 2 reward 0.0\n",
            "Simulating from 0 → 2 with action 2 reward 0.0\n",
            "Simulating from 0 → 6 with action 1 reward 0.0\n",
            "Simulating from 0 → 7 with action 2 reward 0.0\n",
            "Backprop node 0, visits=1, value=0.0\n",
            "Backprop node 1, visits=3, value=0.0\n",
            "Backprop node 0, visits=7, value=0.0\n",
            "Backprop node 4, visits=15, value=0.0\n",
            "Backprop node 0, visits=29, value=0.0\n",
            "Expanded node 3 with children: [2]\n",
            "Simulating from 2 → 2 with action 3 reward 0.0\n",
            "Simulating from 2 → 1 with action 0 reward 0.0\n",
            "Simulating from 2 → 0 with action 0 reward 0.0\n",
            "Simulating from 2 → 4 with action 1 reward 0.0\n",
            "Simulating from 2 → 0 with action 3 reward 0.0\n",
            "Simulating from 2 → 4 with action 1 reward 0.0\n",
            "Simulating from 2 → 4 with action 0 reward 0.0\n",
            "Simulating from 2 → 8 with action 1 reward 0.0\n",
            "Simulating from 2 → 4 with action 3 reward 0.0\n",
            "Simulating from 2 → 4 with action 0 reward 0.0\n",
            "Simulating from 2 → 0 with action 3 reward 0.0\n",
            "Simulating from 2 → 0 with action 3 reward 0.0\n",
            "Simulating from 2 → 1 with action 2 reward 0.0\n",
            "Simulating from 2 → 0 with action 0 reward 0.0\n",
            "Simulating from 2 → 1 with action 2 reward 0.0\n",
            "Simulating from 2 → 1 with action 3 reward 0.0\n",
            "Simulating from 2 → 1 with action 3 reward 0.0\n",
            "Simulating from 2 → 0 with action 0 reward 0.0\n",
            "Simulating from 2 → 1 with action 2 reward 0.0\n",
            "Simulating from 2 → 5 with action 1 reward 0.0\n",
            "Backprop node 2, visits=1, value=0.0\n",
            "Backprop node 3, visits=2, value=0.0\n",
            "Backprop node 2, visits=7, value=0.0\n",
            "Backprop node 1, visits=15, value=0.0\n",
            "Backprop node 0, visits=30, value=0.0\n",
            "Simulating from 10 → 9 with action 0 reward 0.0\n",
            "Simulating from 10 → 8 with action 0 reward 0.0\n",
            "Simulating from 10 → 8 with action 0 reward 0.0\n",
            "Simulating from 10 → 9 with action 2 reward 0.0\n",
            "Simulating from 10 → 8 with action 0 reward 0.0\n",
            "Simulating from 10 → 8 with action 0 reward 0.0\n",
            "Simulating from 10 → 9 with action 2 reward 0.0\n",
            "Simulating from 10 → 10 with action 2 reward 0.0\n",
            "Simulating from 10 → 9 with action 0 reward 0.0\n",
            "Simulating from 10 → 13 with action 1 reward 0.0\n",
            "Simulating from 10 → 13 with action 1 reward 0.0\n",
            "Simulating from 10 → 14 with action 2 reward 0.0\n",
            "Simulating from 10 → 10 with action 3 reward 0.0\n",
            "Simulating from 10 → 9 with action 0 reward 0.0\n",
            "Simulating from 10 → 5 with action 3 reward 0.0\n",
            "Backprop node 10, visits=1, value=0.0\n",
            "Backprop node 9, visits=4, value=0.0\n",
            "Backprop node 8, visits=8, value=0.0\n",
            "Backprop node 4, visits=16, value=0.0\n",
            "Backprop node 0, visits=31, value=0.0\n",
            "Expanded node 8 with children: [9, 4]\n",
            "Simulating from 9 → 13 with action 1 reward 0.0\n",
            "Simulating from 9 → 14 with action 2 reward 0.0\n",
            "Simulating from 9 → 14 with action 1 reward 0.0\n",
            "Simulating from 9 → 15 with action 2 reward 1.0\n",
            "Backprop node 9, visits=1, value=1.0\n",
            "Backprop node 8, visits=2, value=1.0\n",
            "Backprop node 4, visits=4, value=1.0\n",
            "Backprop node 0, visits=8, value=1.0\n",
            "Backprop node 1, visits=16, value=1.0\n",
            "Backprop node 0, visits=32, value=1.0\n",
            "Simulating from 4 → 8 with action 1 reward 0.0\n",
            "Simulating from 4 → 8 with action 0 reward 0.0\n",
            "Simulating from 4 → 4 with action 3 reward 0.0\n",
            "Simulating from 4 → 5 with action 2 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 8, visits=3, value=1.0\n",
            "Backprop node 4, visits=5, value=1.0\n",
            "Backprop node 0, visits=9, value=1.0\n",
            "Backprop node 1, visits=17, value=1.0\n",
            "Backprop node 0, visits=33, value=1.0\n",
            "Expanded node 0 with children: [4, 1]\n",
            "Simulating from 4 → 8 with action 1 reward 0.0\n",
            "Simulating from 4 → 4 with action 3 reward 0.0\n",
            "Simulating from 4 → 4 with action 0 reward 0.0\n",
            "Simulating from 4 → 5 with action 2 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 0, visits=2, value=0.0\n",
            "Backprop node 1, visits=4, value=0.0\n",
            "Backprop node 0, visits=10, value=1.0\n",
            "Backprop node 1, visits=18, value=1.0\n",
            "Backprop node 0, visits=34, value=1.0\n",
            "Simulating from 2 → 1 with action 0 reward 0.0\n",
            "Simulating from 2 → 1 with action 3 reward 0.0\n",
            "Simulating from 2 → 2 with action 2 reward 0.0\n",
            "Simulating from 2 → 6 with action 1 reward 0.0\n",
            "Simulating from 2 → 7 with action 2 reward 0.0\n",
            "Backprop node 2, visits=1, value=0.0\n",
            "Backprop node 1, visits=3, value=0.0\n",
            "Backprop node 2, visits=8, value=0.0\n",
            "Backprop node 1, visits=19, value=1.0\n",
            "Backprop node 0, visits=35, value=1.0\n",
            "Expanded node 8 with children: [9, 4]\n",
            "Simulating from 9 → 8 with action 0 reward 0.0\n",
            "Simulating from 9 → 12 with action 1 reward 0.0\n",
            "Backprop node 9, visits=1, value=0.0\n",
            "Backprop node 8, visits=2, value=0.0\n",
            "Backprop node 4, visits=4, value=0.0\n",
            "Backprop node 0, visits=8, value=0.0\n",
            "Backprop node 4, visits=17, value=0.0\n",
            "Backprop node 0, visits=36, value=1.0\n",
            "Expanded node 0 with children: [4, 1]\n",
            "Simulating from 4 → 5 with action 2 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 0, visits=2, value=0.0\n",
            "Backprop node 4, visits=6, value=1.0\n",
            "Backprop node 0, visits=11, value=1.0\n",
            "Backprop node 1, visits=20, value=1.0\n",
            "Backprop node 0, visits=37, value=1.0\n",
            "Expanded node 8 with children: [9, 4]\n",
            "Simulating from 4 → 5 with action 2 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 8, visits=2, value=0.0\n",
            "Backprop node 4, visits=4, value=0.0\n",
            "Backprop node 8, visits=9, value=0.0\n",
            "Backprop node 4, visits=18, value=0.0\n",
            "Backprop node 0, visits=38, value=1.0\n",
            "Simulating from 2 → 2 with action 3 reward 0.0\n",
            "Simulating from 2 → 2 with action 3 reward 0.0\n",
            "Simulating from 2 → 1 with action 0 reward 0.0\n",
            "Simulating from 2 → 1 with action 3 reward 0.0\n",
            "Simulating from 2 → 2 with action 2 reward 0.0\n",
            "Simulating from 2 → 2 with action 3 reward 0.0\n",
            "Simulating from 2 → 1 with action 0 reward 0.0\n",
            "Simulating from 2 → 1 with action 3 reward 0.0\n",
            "Simulating from 2 → 5 with action 1 reward 0.0\n",
            "Backprop node 2, visits=1, value=0.0\n",
            "Backprop node 6, visits=3, value=0.0\n",
            "Backprop node 2, visits=9, value=0.0\n",
            "Backprop node 1, visits=21, value=1.0\n",
            "Backprop node 0, visits=39, value=1.0\n",
            "Expanded node 2 with children: [1, 6, 3]\n",
            "Simulating from 3 → 3 with action 3 reward 0.0\n",
            "Simulating from 3 → 3 with action 3 reward 0.0\n",
            "Simulating from 3 → 2 with action 0 reward 0.0\n",
            "Simulating from 3 → 1 with action 0 reward 0.0\n",
            "Simulating from 3 → 1 with action 3 reward 0.0\n",
            "Simulating from 3 → 1 with action 3 reward 0.0\n",
            "Simulating from 3 → 1 with action 3 reward 0.0\n",
            "Simulating from 3 → 1 with action 3 reward 0.0\n",
            "Simulating from 3 → 0 with action 0 reward 0.0\n",
            "Simulating from 3 → 0 with action 0 reward 0.0\n",
            "Simulating from 3 → 1 with action 2 reward 0.0\n",
            "Simulating from 3 → 5 with action 1 reward 0.0\n",
            "Backprop node 3, visits=1, value=0.0\n",
            "Backprop node 2, visits=2, value=0.0\n",
            "Backprop node 1, visits=5, value=0.0\n",
            "Backprop node 0, visits=12, value=1.0\n",
            "Backprop node 1, visits=22, value=1.0\n",
            "Backprop node 0, visits=40, value=1.0\n",
            "Expanded node 0 with children: [4, 1]\n",
            "Simulating from 1 → 5 with action 1 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 0, visits=2, value=0.0\n",
            "Backprop node 1, visits=4, value=0.0\n",
            "Backprop node 0, visits=9, value=0.0\n",
            "Backprop node 4, visits=19, value=0.0\n",
            "Backprop node 0, visits=41, value=1.0\n",
            "Expanded node 2 with children: [1, 6, 3]\n",
            "Simulating from 3 → 3 with action 3 reward 0.0\n",
            "Simulating from 3 → 7 with action 1 reward 0.0\n",
            "Backprop node 3, visits=1, value=0.0\n",
            "Backprop node 2, visits=2, value=0.0\n",
            "Backprop node 3, visits=3, value=0.0\n",
            "Backprop node 2, visits=10, value=0.0\n",
            "Backprop node 1, visits=23, value=1.0\n",
            "Backprop node 0, visits=42, value=1.0\n",
            "Expanded node 8 with children: [9, 4]\n",
            "Simulating from 9 → 10 with action 2 reward 0.0\n",
            "Simulating from 9 → 14 with action 1 reward 0.0\n",
            "Simulating from 9 → 13 with action 0 reward 0.0\n",
            "Simulating from 9 → 12 with action 0 reward 0.0\n",
            "Backprop node 9, visits=1, value=0.0\n",
            "Backprop node 8, visits=2, value=0.0\n",
            "Backprop node 9, visits=5, value=0.0\n",
            "Backprop node 8, visits=10, value=0.0\n",
            "Backprop node 4, visits=20, value=0.0\n",
            "Backprop node 0, visits=43, value=1.0\n",
            "Expanded node 9 with children: [8, 13, 10]\n",
            "Simulating from 10 → 6 with action 3 reward 0.0\n",
            "Simulating from 10 → 7 with action 2 reward 0.0\n",
            "Backprop node 10, visits=1, value=0.0\n",
            "Backprop node 9, visits=2, value=1.0\n",
            "Backprop node 8, visits=4, value=1.0\n",
            "Backprop node 4, visits=7, value=1.0\n",
            "Backprop node 0, visits=13, value=1.0\n",
            "Backprop node 1, visits=24, value=1.0\n",
            "Backprop node 0, visits=44, value=1.0\n",
            "Expanded node 0 with children: [4, 1]\n",
            "Simulating from 1 → 1 with action 3 reward 0.0\n",
            "Simulating from 1 → 0 with action 0 reward 0.0\n",
            "Simulating from 1 → 1 with action 2 reward 0.0\n",
            "Simulating from 1 → 1 with action 3 reward 0.0\n",
            "Simulating from 1 → 1 with action 3 reward 0.0\n",
            "Simulating from 1 → 2 with action 2 reward 0.0\n",
            "Simulating from 1 → 1 with action 0 reward 0.0\n",
            "Simulating from 1 → 5 with action 1 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 0, visits=2, value=0.0\n",
            "Backprop node 4, visits=5, value=0.0\n",
            "Backprop node 0, visits=10, value=0.0\n",
            "Backprop node 4, visits=21, value=0.0\n",
            "Backprop node 0, visits=45, value=1.0\n",
            "Expanded node 0 with children: [4, 1]\n",
            "Simulating from 4 → 4 with action 0 reward 0.0\n",
            "Simulating from 4 → 8 with action 1 reward 0.0\n",
            "Simulating from 4 → 12 with action 1 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 0, visits=2, value=0.0\n",
            "Backprop node 1, visits=4, value=0.0\n",
            "Backprop node 2, visits=11, value=0.0\n",
            "Backprop node 1, visits=25, value=1.0\n",
            "Backprop node 0, visits=46, value=1.0\n",
            "Expanded node 0 with children: [4, 1]\n",
            "Simulating from 4 → 8 with action 1 reward 0.0\n",
            "Simulating from 4 → 4 with action 3 reward 0.0\n",
            "Simulating from 4 → 4 with action 0 reward 0.0\n",
            "Simulating from 4 → 4 with action 0 reward 0.0\n",
            "Simulating from 4 → 0 with action 3 reward 0.0\n",
            "Simulating from 4 → 0 with action 0 reward 0.0\n",
            "Simulating from 4 → 1 with action 2 reward 0.0\n",
            "Simulating from 4 → 5 with action 1 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 0, visits=2, value=0.0\n",
            "Backprop node 4, visits=5, value=0.0\n",
            "Backprop node 8, visits=11, value=0.0\n",
            "Backprop node 4, visits=22, value=0.0\n",
            "Backprop node 0, visits=47, value=1.0\n",
            "Simulating from 1 → 5 with action 1 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 0, visits=3, value=0.0\n",
            "Backprop node 1, visits=6, value=0.0\n",
            "Backprop node 0, visits=14, value=1.0\n",
            "Backprop node 1, visits=26, value=1.0\n",
            "Backprop node 0, visits=48, value=1.0\n",
            "Expanded node 2 with children: [1, 6, 3]\n",
            "Simulating from 6 → 10 with action 1 reward 0.0\n",
            "Simulating from 6 → 14 with action 1 reward 0.0\n",
            "Simulating from 6 → 15 with action 2 reward 1.0\n",
            "Backprop node 6, visits=1, value=1.0\n",
            "Backprop node 2, visits=2, value=1.0\n",
            "Backprop node 1, visits=5, value=1.0\n",
            "Backprop node 0, visits=11, value=1.0\n",
            "Backprop node 4, visits=23, value=1.0\n",
            "Backprop node 0, visits=49, value=2.0\n",
            "Simulating from 1 → 2 with action 2 reward 0.0\n",
            "Simulating from 1 → 3 with action 2 reward 0.0\n",
            "Simulating from 1 → 3 with action 2 reward 0.0\n",
            "Simulating from 1 → 2 with action 0 reward 0.0\n",
            "Simulating from 1 → 1 with action 0 reward 0.0\n",
            "Simulating from 1 → 1 with action 3 reward 0.0\n",
            "Simulating from 1 → 0 with action 0 reward 0.0\n",
            "Simulating from 1 → 1 with action 2 reward 0.0\n",
            "Simulating from 1 → 1 with action 3 reward 0.0\n",
            "Simulating from 1 → 1 with action 3 reward 0.0\n",
            "Simulating from 1 → 5 with action 1 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 2, visits=3, value=1.0\n",
            "Backprop node 1, visits=6, value=1.0\n",
            "Backprop node 0, visits=12, value=1.0\n",
            "Backprop node 4, visits=24, value=1.0\n",
            "Backprop node 0, visits=50, value=2.0\n",
            "Simulating from 3 → 7 with action 1 reward 0.0\n",
            "Backprop node 3, visits=1, value=0.0\n",
            "Backprop node 2, visits=4, value=1.0\n",
            "Backprop node 1, visits=7, value=1.0\n",
            "Backprop node 0, visits=13, value=1.0\n",
            "Backprop node 4, visits=25, value=1.0\n",
            "Backprop node 0, visits=51, value=2.0\n",
            "Simulating from 4 → 8 with action 1 reward 0.0\n",
            "Simulating from 4 → 12 with action 1 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 8, visits=3, value=0.0\n",
            "Backprop node 4, visits=6, value=0.0\n",
            "Backprop node 0, visits=14, value=1.0\n",
            "Backprop node 4, visits=26, value=1.0\n",
            "Backprop node 0, visits=52, value=2.0\n",
            "Expanded node 13 with children: [14, 9]\n",
            "Simulating from 14 → 14 with action 1 reward 0.0\n",
            "Simulating from 14 → 14 with action 1 reward 0.0\n",
            "Simulating from 14 → 13 with action 0 reward 0.0\n",
            "Simulating from 14 → 14 with action 2 reward 0.0\n",
            "Simulating from 14 → 14 with action 1 reward 0.0\n",
            "Simulating from 14 → 14 with action 1 reward 0.0\n",
            "Simulating from 14 → 14 with action 1 reward 0.0\n",
            "Simulating from 14 → 13 with action 0 reward 0.0\n",
            "Simulating from 14 → 14 with action 2 reward 0.0\n",
            "Simulating from 14 → 15 with action 2 reward 1.0\n",
            "Backprop node 14, visits=1, value=1.0\n",
            "Backprop node 13, visits=2, value=1.0\n",
            "Backprop node 9, visits=6, value=1.0\n",
            "Backprop node 8, visits=12, value=1.0\n",
            "Backprop node 4, visits=27, value=2.0\n",
            "Backprop node 0, visits=53, value=3.0\n",
            "Expanded node 10 with children: [9, 14, 6]\n",
            "Simulating from 9 → 13 with action 1 reward 0.0\n",
            "Simulating from 9 → 9 with action 3 reward 0.0\n",
            "Simulating from 9 → 8 with action 0 reward 0.0\n",
            "Simulating from 9 → 9 with action 2 reward 0.0\n",
            "Simulating from 9 → 13 with action 1 reward 0.0\n",
            "Simulating from 9 → 12 with action 0 reward 0.0\n",
            "Backprop node 9, visits=1, value=0.0\n",
            "Backprop node 10, visits=2, value=0.0\n",
            "Backprop node 9, visits=7, value=1.0\n",
            "Backprop node 8, visits=13, value=1.0\n",
            "Backprop node 4, visits=28, value=2.0\n",
            "Backprop node 0, visits=54, value=3.0\n",
            "Simulating from 9 → 5 with action 3 reward 0.0\n",
            "Backprop node 9, visits=1, value=0.0\n",
            "Backprop node 8, visits=3, value=0.0\n",
            "Backprop node 4, visits=6, value=0.0\n",
            "Backprop node 8, visits=14, value=1.0\n",
            "Backprop node 4, visits=29, value=2.0\n",
            "Backprop node 0, visits=55, value=3.0\n",
            "Simulating from 9 → 10 with action 2 reward 0.0\n",
            "Simulating from 9 → 11 with action 2 reward 0.0\n",
            "Backprop node 9, visits=1, value=0.0\n",
            "Backprop node 13, visits=3, value=1.0\n",
            "Backprop node 9, visits=8, value=1.0\n",
            "Backprop node 8, visits=15, value=1.0\n",
            "Backprop node 4, visits=30, value=2.0\n",
            "Backprop node 0, visits=56, value=3.0\n",
            "Expanded node 10 with children: [9, 14, 6]\n",
            "Simulating from 9 → 10 with action 2 reward 0.0\n",
            "Simulating from 9 → 6 with action 3 reward 0.0\n",
            "Simulating from 9 → 10 with action 1 reward 0.0\n",
            "Simulating from 9 → 14 with action 1 reward 0.0\n",
            "Simulating from 9 → 13 with action 0 reward 0.0\n",
            "Simulating from 9 → 14 with action 2 reward 0.0\n",
            "Simulating from 9 → 14 with action 1 reward 0.0\n",
            "Simulating from 9 → 15 with action 2 reward 1.0\n",
            "Backprop node 9, visits=1, value=1.0\n",
            "Backprop node 10, visits=2, value=1.0\n",
            "Backprop node 6, visits=4, value=1.0\n",
            "Backprop node 2, visits=12, value=1.0\n",
            "Backprop node 1, visits=27, value=2.0\n",
            "Backprop node 0, visits=57, value=4.0\n",
            "Simulating from 14 → 14 with action 1 reward 0.0\n",
            "Simulating from 14 → 13 with action 0 reward 0.0\n",
            "Simulating from 14 → 9 with action 3 reward 0.0\n",
            "Simulating from 14 → 8 with action 0 reward 0.0\n",
            "Simulating from 14 → 12 with action 1 reward 0.0\n",
            "Backprop node 14, visits=1, value=0.0\n",
            "Backprop node 10, visits=3, value=1.0\n",
            "Backprop node 6, visits=5, value=1.0\n",
            "Backprop node 2, visits=13, value=1.0\n",
            "Backprop node 1, visits=28, value=2.0\n",
            "Backprop node 0, visits=58, value=4.0\n",
            "Simulating from 1 → 1 with action 3 reward 0.0\n",
            "Simulating from 1 → 5 with action 1 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 2, visits=3, value=0.0\n",
            "Backprop node 3, visits=4, value=0.0\n",
            "Backprop node 2, visits=14, value=1.0\n",
            "Backprop node 1, visits=29, value=2.0\n",
            "Backprop node 0, visits=59, value=4.0\n",
            "Simulating from 1 → 5 with action 1 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 0, visits=3, value=0.0\n",
            "Backprop node 4, visits=8, value=1.0\n",
            "Backprop node 0, visits=15, value=1.0\n",
            "Backprop node 1, visits=30, value=2.0\n",
            "Backprop node 0, visits=60, value=4.0\n",
            "Simulating from 4 → 8 with action 1 reward 0.0\n",
            "Simulating from 4 → 4 with action 3 reward 0.0\n",
            "Simulating from 4 → 4 with action 0 reward 0.0\n",
            "Simulating from 4 → 4 with action 0 reward 0.0\n",
            "Simulating from 4 → 5 with action 2 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 0, visits=3, value=0.0\n",
            "Backprop node 1, visits=8, value=1.0\n",
            "Backprop node 0, visits=15, value=1.0\n",
            "Backprop node 4, visits=31, value=2.0\n",
            "Backprop node 0, visits=61, value=4.0\n",
            "Expanded node 2 with children: [1, 6, 3]\n",
            "Simulating from 3 → 2 with action 0 reward 0.0\n",
            "Simulating from 3 → 2 with action 3 reward 0.0\n",
            "Simulating from 3 → 1 with action 0 reward 0.0\n",
            "Simulating from 3 → 1 with action 3 reward 0.0\n",
            "Simulating from 3 → 5 with action 1 reward 0.0\n",
            "Backprop node 3, visits=1, value=0.0\n",
            "Backprop node 2, visits=2, value=0.0\n",
            "Backprop node 6, visits=6, value=1.0\n",
            "Backprop node 2, visits=15, value=1.0\n",
            "Backprop node 1, visits=31, value=2.0\n",
            "Backprop node 0, visits=62, value=4.0\n",
            "Simulating from 1 → 0 with action 0 reward 0.0\n",
            "Simulating from 1 → 4 with action 1 reward 0.0\n",
            "Simulating from 1 → 5 with action 2 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 0, visits=3, value=0.0\n",
            "Backprop node 4, visits=7, value=0.0\n",
            "Backprop node 8, visits=16, value=1.0\n",
            "Backprop node 4, visits=32, value=2.0\n",
            "Backprop node 0, visits=63, value=4.0\n",
            "Simulating from 1 → 5 with action 1 reward 0.0\n",
            "Backprop node 1, visits=1, value=0.0\n",
            "Backprop node 2, visits=3, value=0.0\n",
            "Backprop node 1, visits=7, value=0.0\n",
            "Backprop node 0, visits=16, value=1.0\n",
            "Backprop node 1, visits=32, value=2.0\n",
            "Backprop node 0, visits=64, value=4.0\n",
            "Simulating from 4 → 0 with action 3 reward 0.0\n",
            "Simulating from 4 → 1 with action 2 reward 0.0\n",
            "Simulating from 4 → 0 with action 0 reward 0.0\n",
            "Simulating from 4 → 1 with action 2 reward 0.0\n",
            "Simulating from 4 → 2 with action 2 reward 0.0\n",
            "Simulating from 4 → 6 with action 1 reward 0.0\n",
            "Simulating from 4 → 7 with action 2 reward 0.0\n",
            "Backprop node 4, visits=1, value=0.0\n",
            "Backprop node 0, visits=3, value=0.0\n",
            "Backprop node 4, visits=7, value=0.0\n",
            "Backprop node 0, visits=16, value=1.0\n",
            "Backprop node 4, visits=33, value=2.0\n",
            "Backprop node 0, visits=65, value=4.0\n",
            "Expanded node 2 with children: [1, 6, 3]\n",
            "Simulating from 3 → 2 with action 0 reward 0.0\n",
            "Simulating from 3 → 1 with action 0 reward 0.0\n",
            "Simulating from 3 → 5 with action 1 reward 0.0\n",
            "Backprop node 3, visits=1, value=0.0\n",
            "Backprop node 2, visits=2, value=0.0\n",
            "Backprop node 1, visits=5, value=0.0\n",
            "Backprop node 2, visits=16, value=1.0\n",
            "Backprop node 1, visits=33, value=2.0\n",
            "Backprop node 0, visits=66, value=4.0\n",
            "Goal found from state 14 with action 2 → 15\n",
            "Backprop node 15, visits=1, value=1.0\n",
            "Backprop node 14, visits=2, value=2.0\n",
            "Backprop node 13, visits=4, value=2.0\n",
            "Backprop node 9, visits=9, value=2.0\n",
            "Backprop node 8, visits=17, value=2.0\n",
            "Backprop node 4, visits=34, value=3.0\n",
            "Backprop node 0, visits=67, value=5.0\n",
            "Goal reached at state 15\n",
            "Finished 1000 iterations.\n",
            "Initial state:\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Best action from state 0 to state 4 with value 3.0\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Best action from state 4 to state 8 with value 2.0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "Best action from state 8 to state 9 with value 2.0\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "Best action from state 9 to state 13 with value 2.0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "Best action from state 13 to state 14 with value 2.0\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "Best action from state 14 to state 15 with value 1.0\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Main Execution ---\n",
        "def make_env():\n",
        "    return gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n",
        "\n",
        "# Run MCTS\n",
        "mcts = MCTS(make_env=make_env, num_iterations=1000, num_simulations=100, exploration=1.41, verbose=True)\n",
        "mcts.run()\n",
        "\n",
        "# Visualise best path\n",
        "env = make_env()\n",
        "env.reset()\n",
        "print(\"Initial state:\")\n",
        "print(env.render())\n",
        "\n",
        "trajectory = []\n",
        "node = mcts.root\n",
        "trajectory.append((node.state, node.action, node.value))\n",
        "while not node.is_leaf():\n",
        "    prev_node = node\n",
        "    node = prev_node.best_child()\n",
        "    trajectory.append((node.state, node.action, node.value))\n",
        "    print(f\"Best action from state {prev_node.state} to state {node.state} with value {node.value}\")\n",
        "    env.step(node.action)\n",
        "    print(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b33WuiTw3k1B",
        "outputId": "58496479-8d45-4bde-b2ad-a6e359237abe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "bash: line 1: cd: monte-carlo-manipulation/: No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[main 94d7ca9] added collab runner file\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "To https://github.com/julialopezgomez/monte-carlo-manipulation.git\n",
            "   5a56108..94d7ca9  main -> main\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "cd monte-carlo-manipulation/\n",
        "git add .\n",
        "git commit -m \"added collab runner file\"\n",
        "git push\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVkKu70B4CQS"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cp drive/MyDrive/Universidad/MCTS_colab_runner.ipynb monte-carlo-manipulation/src/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbcu_KVE7u2Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}