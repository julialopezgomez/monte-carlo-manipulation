{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":127,"status":"ok","timestamp":1746992540731,"user":{"displayName":"Julia López Gómez","userId":"04858267532126932932"},"user_tz":-60},"id":"jiFSwxk5cWTa"},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import random\n","import copy\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from collections import deque, defaultdict\n","import tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1746990428064,"user":{"displayName":"Julia López Gómez","userId":"04858267532126932932"},"user_tz":-60},"id":"De0mzFn4eQ47"},"outputs":[],"source":["###TODO: DELETE THIS FOR FINAL VERSION\n","GITHUB_USER = \"julialopezgomez\"\n","GITHUB_TOKEN = \"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\""]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1746991483271,"user":{"displayName":"Julia López Gómez","userId":"04858267532126932932"},"user_tz":-60},"id":"YL3sMC4ezjv4"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (3911631072.py, line 3)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[4], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    cat > ~/.netrc <<EOF\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["###TODO: DELETE THIS FOR FINAL VERSION\n","%%bash\n","cat > ~/.netrc <<EOF\n","machine github.com\n","  login julialopezgomez\n","  password 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\n","EOF\n","\n","# Secure it so only you can read it:\n","chmod 600 ~/.netrc"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2849,"status":"ok","timestamp":1746990296392,"user":{"displayName":"Julia López Gómez","userId":"04858267532126932932"},"user_tz":-60},"id":"dRxMk0yTfIgp","outputId":"a4baeca3-e2c7-43c9-d2c7-68d5a95f2000"},"outputs":[{"name":"stderr","output_type":"stream","text":["Cloning into 'monte-carlo-manipulation'...\n"]}],"source":["%%bash\n","###TODO: DELETE THIS FOR FINAL VERSION\n","# configure git to use your token over HTTPS\n","git config --global user.email \"s2107370@ed.ac.uk\"\n","git config --global user.name  \"julialopezgomez\"\n","###\n","\n","\n","# clone via token\n","git clone https://github.com/julialopezgomez/monte-carlo-manipulation.git"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15480,"status":"ok","timestamp":1746991322161,"user":{"displayName":"Julia López Gómez","userId":"04858267532126932932"},"user_tz":-60},"id":"qWOo5C-xfq_V","outputId":"7027bb2f-1eff-41f7-e93e-7f28d3d93f71"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n","Collecting torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl (from -r src/requirements.txt (line 2))\n","  Using cached https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl (1843.9 MB)\n","Collecting torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl (from -r src/requirements.txt (line 3))\n","  Using cached https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl (6.1 MB)\n","Requirement already satisfied: torchaudio==2.0.2 in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 4)) (2.0.2)\n","Requirement already satisfied: ipywidgets==8.0.4 in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 7)) (8.0.4)\n","Requirement already satisfied: widgetsnbextension==4.0.5 in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 8)) (4.0.5)\n","Requirement already satisfied: jupyterlab-widgets in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 9)) (3.0.14)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 12)) (1.26.4)\n","Collecting scipy<2.0.0,>=1.11.4 (from -r src/requirements.txt (line 13))\n","  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","Requirement already satisfied: pymcubes==0.1.4 in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 14)) (0.1.4)\n","Requirement already satisfied: quadprog in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 15)) (0.1.13)\n","Requirement already satisfied: cvxpy in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 18)) (1.6.0)\n","Requirement already satisfied: mosek in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 19)) (11.0.20)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 23)) (5.24.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 24)) (4.67.1)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 25)) (0.19.10)\n","Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 28)) (1.1.1)\n","Requirement already satisfied: gurobipy in /usr/local/lib/python3.11/dist-packages (from -r src/requirements.txt (line 29)) (12.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (3.18.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (4.13.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (3.1.6)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (2.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (2.32.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (11.2.1)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (6.17.1)\n","Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (7.34.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (5.7.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (3.31.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (18.1.8)\n","Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy->-r src/requirements.txt (line 18)) (1.0.3)\n","Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy->-r src/requirements.txt (line 18)) (0.10.0)\n","Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy->-r src/requirements.txt (line 18)) (3.2.7.post2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->-r src/requirements.txt (line 23)) (9.1.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly->-r src/requirements.txt (line 23)) (24.2)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (8.1.8)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (4.3.7)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (5.29.4)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (5.9.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (2.11.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (6.0.2)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (2.27.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (1.3.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->-r src/requirements.txt (line 25)) (75.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r src/requirements.txt (line 25)) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r src/requirements.txt (line 25)) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r src/requirements.txt (line 25)) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision@ https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 3)) (2025.4.26)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r src/requirements.txt (line 28)) (3.1.1)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r src/requirements.txt (line 28)) (0.0.4)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb->-r src/requirements.txt (line 25)) (1.17.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r src/requirements.txt (line 25)) (4.0.12)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r src/requirements.txt (line 25)) (5.0.2)\n","Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (1.8.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (6.1.12)\n","Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.1.7)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (1.6.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (24.0.1)\n","Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (6.4.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (4.9.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.2.13)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.8.4)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (5.7.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (2.9.0.post0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy->-r src/requirements.txt (line 18)) (1.4.2)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.0.4->-r src/requirements.txt (line 7)) (0.7.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch@ https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl->-r src/requirements.txt (line 2)) (1.3.0)\n","Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37.7/37.7 MB 130.0 MB/s eta 0:00:00\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.10.1\n","    Uninstalling scipy-1.10.1:\n","      Successfully uninstalled scipy-1.10.1\n","Successfully installed scipy-1.15.3\n"]}],"source":["%%bash\n","cd monte-carlo-manipulation\n","pip install --upgrade pip\n","pip instal  l -r src/requirements.txt"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# MCTS / AlphaZero params\n","NUM_SIMS     = 800       # simulations per move\n","CPUCT        = 1.0       # PUCT exploration constant\n","TAU          = 1.0       # temperature for π = N^(1/τ)\n","# Training params\n","BATCH_SIZE   = 64\n","LR           = 1e-3\n","EVAL_INTERVAL= 100       # eval every 100 self-play games\n","TARGET_SR    = 0.95      # stop when success rate ≥ 95%"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["class AlphaZeroNet(nn.Module):\n","    def __init__(self, n_states, n_actions, hidden_dim=128):\n","        super().__init__()\n","        self.fc1 = nn.Linear(n_states, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        # policy head\n","        self.policy_head = nn.Linear(hidden_dim, n_actions)\n","        # value head\n","        self.value_head  = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x):\n","        # x: one-hot or feature vector of shape (batch, n_states)\n","        h = F.relu(self.fc1(x))\n","        h = F.relu(self.fc2(h))\n","        p = F.log_softmax(self.policy_head(h), dim=1)  # log-probs\n","        v = torch.tanh(self.value_head(h))             # in [-1,1] \n","        return p, v.squeeze(-1) "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# --- Main Execution ---\n","def make_env():\n","    return gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n","\n","\n","env    = make_env()\n","nS, nA = env.observation_space.n, env.action_space.n\n","net    = AlphaZeroNet(nS, nA)\n","opt    = optim.Adam(net.parameters(), lr=LR)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# --- MCTS Node Class ---\n","class MCTSNode:\n","    def __init__(self, \n","                 state, \n","                 parent=None, \n","                 prior=0.0,\n","                 device='cpu', \n","                 verbose=False):\n","        \n","        self.state = state\n","        self.parent = parent\n","        self.prior   = prior          # P(s,a) from network\n","        self.children= {}             # action → child_node\n","        self.N       = defaultdict(int)   # visit counts per child\n","        self.W       = defaultdict(float) # total value per child\n","        self.Q       = defaultdict(float) # mean value per child\n","        self.device = device\n","        self.verbose = verbose\n","\n","    def is_leaf(self):\n","        return len(self.children) == 0\n","    \n","    \n","\n","    def uct(self, exploration_weight=1.41, heuristic_weight=0.1):\n","        if self.visits == 0 or self.parent is None:\n","            return float(\"inf\")\n","        \n","        # Use value network prediction as heuristic\n","        with torch.no_grad():\n","            state_tensor = torch.FloatTensor(self.state).unsqueeze(0).to(self.device)\n","            _, value = self.value_net(state_tensor)\n","            heuristic = value.item() # item() to get the scalar value from the tensor\n","        \n","        exploitation = self.value / self.visits\n","        exploration = exploration_weight * self.prior * math.sqrt(self.parent.visits) / (1 + self.visits)\n","        return exploitation + exploration + heuristic_weight * heuristic\n","\n","    def best_uct_child(self):\n","        return max(self.children, key=lambda child: child.uct())\n","\n","    def best_child(self):\n","        return max(self.children, key=lambda child: child.value)\n","\n","    def selection(self):\n","        \"\"\"\n","        Traverse the tree to select a promising node to expand.\n","        The selection is based on the Upper Confidence Bound for Trees (UCT) algorithm.\n","        \n","        Returns:\n","            A tuple containing the selected node and a boolean indicating if it is a goal node.\n","        \"\"\"\n","        node = self\n","        \n","        # Select the best child based on UCT until reaching a leaf node\n","        while not node.is_leaf():\n","            node = node.best_uct_child()\n","            \n","        # If the node has no visits, return it as is\n","        if node.visits == 0:\n","            return node, False\n","        \n","        # Expand the node to find a goal state\n","        goal_node = node.expand()\n","        \n","        # If a goal node is found, return it; otherwise, return a random child\n","        node = goal_node if goal_node else random.choice(node.children)\n","        is_goal = goal_node is not None\n","        \n","        return node, is_goal\n","\n","    def expand(self):\n","        \"\"\"\n","        Expand the node by trying all possible actions.\n","        \n","        Returns:\n","            A child node if a goal state is found, otherwise None.\n","        \"\"\"\n","        # Get action probabilities from the policy network\n","        with torch.no_grad():\n","            state_tensor = torch.FloatTensor(self.state).unsqueeze(0).to(self.device)\n","            policy, _ = self.policy_net(state_tensor) # get policy logits\n","            action_probs = torch.exp(policy).squeeze(0).cpu().numpy().flatten() # convert to numpy array\n","        \n","        for action in range(self.action_space.n):\n","            # Copy the environment by creating a new instance\n","            env_copy = self.make_env()\n","            env_copy.reset()\n","            env_copy.unwrapped.s = self.state\n","            \n","            # Perform the action in the copied environment\n","            obs, reward, terminated, truncated, _ = env_copy.step(action)\n","\n","            # Only add child if it is not a (non successful) terminal state or if it is not the same state\n","            if reward == 0 and terminated or self.state == obs:\n","                continue\n","\n","            child = MCTSNode(\n","                obs, \n","                parent=self, \n","                action=action, \n","                prioty=action_probs[action],\n","                policy_net=self.policy_net,\n","                value_net=self.value_net,\n","                make_env=self.make_env,\n","                device=self.device, \n","                verbose=self.verbose\n","                )\n","            self.children.append(child)\n","\n","            if reward == 1:\n","                if self.verbose:\n","                    print(f\"Goal found from state {self.state} with action {action} → {obs}\")\n","                return child\n","\n","        if self.verbose:\n","            print(f\"Expanded node {self.state} with children: {[c.state for c in self.children]}\")\n","        return None\n","\n","    def simulation(self, max_steps=10000):\n","        \"\"\"Perform a rollout from the current node.\"\"\"\n","        # Copy the environment to avoid modifying the original\n","        env_copy = self.make_env()\n","        env_copy.reset()\n","        env_copy.unwrapped.s = self.state\n","        obs = self.state\n","        \n","        total_reward = 0\n","        \n","        for _ in range(max_steps):\n","            # Use policy network for rollout actions (with some exploration/randomness)\n","            state_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n","            with torch.no_grad():\n","                policy, _ = self.policy_net(state_tensor)\n","                action_probs = torch.exp(policy).squeeze(0).cpu().numpy().flatten()\n","                \n","            # Add exploration noise            \n","            action = np.random.choice(self.action_space.n, p=action_probs)\n","            \n","            # Perform the action in the copied environment\n","            obs, reward, terminated, truncated, _ = env_copy.step(action)\n","            \n","            total_reward += reward\n","\n","            if self.verbose:\n","                print(f\"Simulating from {self.state} → {obs} with action {action} reward {reward}\")\n","\n","            # If the episode is terminated or truncated, return the total reward\n","            if terminated or truncated:\n","                break\n","        \n","        # Return the total reward obtained during the simulation\n","        return total_reward\n","\n","    def backpropagation(self, reward):\n","        \"\"\"Propagate the simulation result back up the tree.\"\"\"\n","        node = self\n","        while node:\n","            node.visits += 1\n","            node.value += reward\n","            if self.verbose:\n","                print(f\"Backprop node {node.state}, visits={node.visits}, value={node.value}\")\n","            node = node.parent"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["class PolicyValueNetwork(nn.Module):\n","    \"\"\"Neural network for policy and value prediction\"\"\"\n","    def __init__(self, state_dim, action_dim):\n","        super(PolicyValueNetwork, self).__init__()\n","\n","        # Shared feature extraction layers\n","        self.fc1 = nn.Linear(state_dim, 128)\n","        self.fc2 = nn.Linear(128, 128)\n","\n","        # Policy head\n","        self.policy_head = nn.Linear(128, action_dim)\n","\n","        # Value head\n","        self.value_head = nn.Linear(128, 1)\n","\n","        # Initialize weights\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.xavier_uniform_(m.weight)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, x):\n","        # Shared features\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","\n","        # Policy output (log probabilities)\n","        policy = F.log_softmax(self.policy_head(x), dim=-1)\n","\n","        # Value output (scalar)\n","        value = torch.tanh(self.value_head(x))\n","\n","        return policy, value"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1746992547182,"user":{"displayName":"Julia López Gómez","userId":"04858267532126932932"},"user_tz":-60},"id":"GgAyOxEWfxTr"},"outputs":[],"source":["# --- MCTS Node Class ---\n","class MCTSNode:\n","    def __init__(self, state, parent=None, action=None, prior=0, policy_net=None, value_net=None, make_env=None, device='cpu', verbose=False):\n","        self.state = state\n","        self.parent = parent\n","        self.action = action\n","        self.prior = prior\n","        self.children = []\n","        self.visits = 0\n","        self.value = 0.0\n","        self.make_env = make_env\n","        self.action_space = make_env().action_space\n","        self.policy_net = policy_net\n","        self.value_net = value_net\n","        self.device = device\n","        self.verbose = verbose\n","\n","    def is_leaf(self):\n","        return len(self.children) == 0\n","\n","    def uct(self, exploration_weight=1.41, heuristic_weight=0.1):\n","        if self.visits == 0 or self.parent is None:\n","            return float(\"inf\")\n","        \n","        # Use value network prediction as heuristic\n","        with torch.no_grad():\n","            state_tensor = torch.FloatTensor(self.state).unsqueeze(0).to(self.device)\n","            _, value = self.value_net(state_tensor)\n","            heuristic = value.item() # item() to get the scalar value from the tensor\n","        \n","        exploitation = self.value / self.visits\n","        exploration = exploration_weight * self.prior * math.sqrt(self.parent.visits) / (1 + self.visits)\n","        return exploitation + exploration + heuristic_weight * heuristic\n","\n","    def best_uct_child(self):\n","        return max(self.children, key=lambda child: child.uct())\n","\n","    def best_child(self):\n","        return max(self.children, key=lambda child: child.value)\n","\n","    def selection(self):\n","        \"\"\"\n","        Traverse the tree to select a promising node to expand.\n","        The selection is based on the Upper Confidence Bound for Trees (UCT) algorithm.\n","        \n","        Returns:\n","            A tuple containing the selected node and a boolean indicating if it is a goal node.\n","        \"\"\"\n","        node = self\n","        \n","        # Select the best child based on UCT until reaching a leaf node\n","        while not node.is_leaf():\n","            node = node.best_uct_child()\n","            \n","        # If the node has no visits, return it as is\n","        if node.visits == 0:\n","            return node, False\n","        \n","        # Expand the node to find a goal state\n","        goal_node = node.expand()\n","        \n","        # If a goal node is found, return it; otherwise, return a random child\n","        node = goal_node if goal_node else random.choice(node.children)\n","        is_goal = goal_node is not None\n","        \n","        return node, is_goal\n","\n","    def expand(self):\n","        \"\"\"\n","        Expand the node by trying all possible actions.\n","        \n","        Returns:\n","            A child node if a goal state is found, otherwise None.\n","        \"\"\"\n","        # Get action probabilities from the policy network\n","        with torch.no_grad():\n","            state_tensor = torch.FloatTensor(self.state).unsqueeze(0).to(self.device)\n","            policy, _ = self.policy_net(state_tensor) # get policy logits\n","            action_probs = torch.exp(policy).squeeze(0).cpu().numpy().flatten() # convert to numpy array\n","        \n","        for action in range(self.action_space.n):\n","            # Copy the environment by creating a new instance\n","            env_copy = self.make_env()\n","            env_copy.reset()\n","            env_copy.unwrapped.s = self.state\n","            \n","            # Perform the action in the copied environment\n","            obs, reward, terminated, truncated, _ = env_copy.step(action)\n","\n","            # Only add child if it is not a (non successful) terminal state or if it is not the same state\n","            if reward == 0 and terminated or self.state == obs:\n","                continue\n","\n","            child = MCTSNode(\n","                obs, \n","                parent=self, \n","                action=action, \n","                prioty=action_probs[action],\n","                policy_net=self.policy_net,\n","                value_net=self.value_net,\n","                make_env=self.make_env,\n","                device=self.device, \n","                verbose=self.verbose\n","                )\n","            self.children.append(child)\n","\n","            if reward == 1:\n","                if self.verbose:\n","                    print(f\"Goal found from state {self.state} with action {action} → {obs}\")\n","                return child\n","\n","        if self.verbose:\n","            print(f\"Expanded node {self.state} with children: {[c.state for c in self.children]}\")\n","        return None\n","\n","    def simulation(self, max_steps=10000):\n","        \"\"\"Perform a rollout from the current node.\"\"\"\n","        # Copy the environment to avoid modifying the original\n","        env_copy = self.make_env()\n","        env_copy.reset()\n","        env_copy.unwrapped.s = self.state\n","        obs = self.state\n","        \n","        total_reward = 0\n","        \n","        for _ in range(max_steps):\n","            # Use policy network for rollout actions (with some exploration/randomness)\n","            state_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n","            with torch.no_grad():\n","                policy, _ = self.policy_net(state_tensor)\n","                action_probs = torch.exp(policy).squeeze(0).cpu().numpy().flatten()\n","                \n","            # Add exploration noise            \n","            action = np.random.choice(self.action_space.n, p=action_probs)\n","            \n","            # Perform the action in the copied environment\n","            obs, reward, terminated, truncated, _ = env_copy.step(action)\n","            \n","            total_reward += reward\n","\n","            if self.verbose:\n","                print(f\"Simulating from {self.state} → {obs} with action {action} reward {reward}\")\n","\n","            # If the episode is terminated or truncated, return the total reward\n","            if terminated or truncated:\n","                break\n","        \n","        # Return the total reward obtained during the simulation\n","        return total_reward\n","\n","    def backpropagation(self, reward):\n","        \"\"\"Propagate the simulation result back up the tree.\"\"\"\n","        node = self\n","        while node:\n","            node.visits += 1\n","            node.value += reward\n","            if self.verbose:\n","                print(f\"Backprop node {node.state}, visits={node.visits}, value={node.value}\")\n","            node = node.parent\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1746992547431,"user":{"displayName":"Julia López Gómez","userId":"04858267532126932932"},"user_tz":-60},"id":"EJT1sqoV3Wkb"},"outputs":[],"source":["\n","# --- MCTS Search Class ---\n","class LearnedMCTS:\n","    def __init__(self, \n","                 make_env, \n","                 policy_net,\n","                 value_net,\n","                 num_iterations=1000, \n","                 num_simulations=10000, \n","                 exploration_weight=1.41, \n","                 learning_rate=0.001,\n","                 device='cpu',\n","                 verbose=False):\n","        \n","        self.make_env = make_env\n","        self.env = make_env()\n","        self.policy_net = policy_net\n","        self.value_net = value_net\n","        self.num_iterations = num_iterations\n","        self.num_simulations = num_simulations\n","        self.exploration_weight = exploration_weight\n","        self.learning_rate = learning_rate\n","        self.device = device\n","        self.verbose = verbose\n","        self.root.expand(self.env)\n","        \n","        self.root = MCTSNode(self.env.reset()[0], make_env=self.make_env, verbose=verbose)\n","        \n","        # Optimisers for policy and value networks\n","        self.policy_optimiser = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n","        self.value_optimiser = optim.Adam(self.value_net.parameters(), lr=self.learning_rate)\n","        \n","        # Experience buffer: a deque to store (state, action, reward) tuples\n","        self.buffer = deque(maxlen=10000)\n","\n","\n","    def search(self):\n","        \"\"\"Perform the MCTS search for a specified number of iterations.\"\"\"\n","        \n","        for _ in range(self.num_iterations):\n","            \n","            # Select a node using UCT and expand it if it has not been visited\n","            node, is_goal = self.root.selection(self.env)\n","            \n","            #\n","            if is_goal:\n","                node.backpropagation(1)\n","                if self.verbose:\n","                    print(f\"Goal reached at state {node.state}\")\n","                break\n","            reward = node.simulation(max_steps=self.num_simulations)\n","            node.backpropagation(reward)\n","        if self.verbose:\n","            print(f\"Finished {self.num_iterations} iterations.\")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1746992560489,"user":{"displayName":"Julia López Gómez","userId":"04858267532126932932"},"user_tz":-60},"id":"QfLn8zLC3d0J","outputId":"87022334-ca52-4675-ff2f-e230aa17a08f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Expanded node 0 with children: [4, 1]\n","Simulating from 4 → 5 with action 2 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 0, visits=1, value=0.0\n","Simulating from 1 → 2 with action 2 reward 0.0\n","Simulating from 1 → 6 with action 1 reward 0.0\n","Simulating from 1 → 10 with action 1 reward 0.0\n","Simulating from 1 → 14 with action 1 reward 0.0\n","Simulating from 1 → 10 with action 3 reward 0.0\n","Simulating from 1 → 14 with action 1 reward 0.0\n","Simulating from 1 → 14 with action 1 reward 0.0\n","Simulating from 1 → 10 with action 3 reward 0.0\n","Simulating from 1 → 6 with action 3 reward 0.0\n","Simulating from 1 → 5 with action 0 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 0, visits=2, value=0.0\n","Expanded node 4 with children: [8, 0]\n","Simulating from 0 → 4 with action 1 reward 0.0\n","Simulating from 0 → 4 with action 0 reward 0.0\n","Simulating from 0 → 0 with action 3 reward 0.0\n","Simulating from 0 → 4 with action 1 reward 0.0\n","Simulating from 0 → 4 with action 0 reward 0.0\n","Simulating from 0 → 8 with action 1 reward 0.0\n","Simulating from 0 → 4 with action 3 reward 0.0\n","Simulating from 0 → 5 with action 2 reward 0.0\n","Backprop node 0, visits=1, value=0.0\n","Backprop node 4, visits=2, value=0.0\n","Backprop node 0, visits=3, value=0.0\n","Expanded node 1 with children: [0, 2]\n","Simulating from 2 → 6 with action 1 reward 0.0\n","Simulating from 2 → 5 with action 0 reward 0.0\n","Backprop node 2, visits=1, value=0.0\n","Backprop node 1, visits=2, value=0.0\n","Backprop node 0, visits=4, value=0.0\n","Simulating from 8 → 8 with action 0 reward 0.0\n","Simulating from 8 → 4 with action 3 reward 0.0\n","Simulating from 8 → 0 with action 3 reward 0.0\n","Simulating from 8 → 0 with action 3 reward 0.0\n","Simulating from 8 → 0 with action 0 reward 0.0\n","Simulating from 8 → 0 with action 0 reward 0.0\n","Simulating from 8 → 4 with action 1 reward 0.0\n","Simulating from 8 → 0 with action 3 reward 0.0\n","Simulating from 8 → 4 with action 1 reward 0.0\n","Simulating from 8 → 4 with action 0 reward 0.0\n","Simulating from 8 → 0 with action 3 reward 0.0\n","Simulating from 8 → 1 with action 2 reward 0.0\n","Simulating from 8 → 0 with action 0 reward 0.0\n","Simulating from 8 → 0 with action 3 reward 0.0\n","Simulating from 8 → 0 with action 0 reward 0.0\n","Simulating from 8 → 1 with action 2 reward 0.0\n","Simulating from 8 → 5 with action 1 reward 0.0\n","Backprop node 8, visits=1, value=0.0\n","Backprop node 4, visits=3, value=0.0\n","Backprop node 0, visits=5, value=0.0\n","Simulating from 0 → 4 with action 1 reward 0.0\n","Simulating from 0 → 8 with action 1 reward 0.0\n","Simulating from 0 → 8 with action 0 reward 0.0\n","Simulating from 0 → 4 with action 3 reward 0.0\n","Simulating from 0 → 8 with action 1 reward 0.0\n","Simulating from 0 → 9 with action 2 reward 0.0\n","Simulating from 0 → 5 with action 3 reward 0.0\n","Backprop node 0, visits=1, value=0.0\n","Backprop node 1, visits=3, value=0.0\n","Backprop node 0, visits=6, value=0.0\n","Expanded node 8 with children: [9, 4]\n","Simulating from 9 → 13 with action 1 reward 0.0\n","Simulating from 9 → 9 with action 3 reward 0.0\n","Simulating from 9 → 8 with action 0 reward 0.0\n","Simulating from 9 → 4 with action 3 reward 0.0\n","Simulating from 9 → 8 with action 1 reward 0.0\n","Simulating from 9 → 4 with action 3 reward 0.0\n","Simulating from 9 → 8 with action 1 reward 0.0\n","Simulating from 9 → 12 with action 1 reward 0.0\n","Backprop node 9, visits=1, value=0.0\n","Backprop node 8, visits=2, value=0.0\n","Backprop node 4, visits=4, value=0.0\n","Backprop node 0, visits=7, value=0.0\n","Expanded node 0 with children: [4, 1]\n","Simulating from 1 → 2 with action 2 reward 0.0\n","Simulating from 1 → 6 with action 1 reward 0.0\n","Simulating from 1 → 5 with action 0 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 0, visits=2, value=0.0\n","Backprop node 1, visits=4, value=0.0\n","Backprop node 0, visits=8, value=0.0\n","Expanded node 0 with children: [4, 1]\n","Simulating from 4 → 0 with action 3 reward 0.0\n","Simulating from 4 → 0 with action 0 reward 0.0\n","Simulating from 4 → 4 with action 1 reward 0.0\n","Simulating from 4 → 5 with action 2 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 0, visits=2, value=0.0\n","Backprop node 4, visits=5, value=0.0\n","Backprop node 0, visits=9, value=0.0\n","Expanded node 2 with children: [1, 6, 3]\n","Simulating from 3 → 2 with action 0 reward 0.0\n","Simulating from 3 → 6 with action 1 reward 0.0\n","Simulating from 3 → 10 with action 1 reward 0.0\n","Simulating from 3 → 6 with action 3 reward 0.0\n","Simulating from 3 → 7 with action 2 reward 0.0\n","Backprop node 3, visits=1, value=0.0\n","Backprop node 2, visits=2, value=0.0\n","Backprop node 1, visits=5, value=0.0\n","Backprop node 0, visits=10, value=0.0\n","Simulating from 4 → 5 with action 2 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 8, visits=3, value=0.0\n","Backprop node 4, visits=6, value=0.0\n","Backprop node 0, visits=11, value=0.0\n","Simulating from 4 → 8 with action 1 reward 0.0\n","Simulating from 4 → 12 with action 1 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 0, visits=3, value=0.0\n","Backprop node 1, visits=6, value=0.0\n","Backprop node 0, visits=12, value=0.0\n","Simulating from 1 → 2 with action 2 reward 0.0\n","Simulating from 1 → 1 with action 0 reward 0.0\n","Simulating from 1 → 5 with action 1 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 0, visits=3, value=0.0\n","Backprop node 4, visits=7, value=0.0\n","Backprop node 0, visits=13, value=0.0\n","Simulating from 1 → 0 with action 0 reward 0.0\n","Simulating from 1 → 0 with action 0 reward 0.0\n","Simulating from 1 → 4 with action 1 reward 0.0\n","Simulating from 1 → 8 with action 1 reward 0.0\n","Simulating from 1 → 9 with action 2 reward 0.0\n","Simulating from 1 → 13 with action 1 reward 0.0\n","Simulating from 1 → 9 with action 3 reward 0.0\n","Simulating from 1 → 10 with action 2 reward 0.0\n","Simulating from 1 → 6 with action 3 reward 0.0\n","Simulating from 1 → 5 with action 0 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 2, visits=3, value=0.0\n","Backprop node 1, visits=7, value=0.0\n","Backprop node 0, visits=14, value=0.0\n","Expanded node 9 with children: [8, 13, 10]\n","Simulating from 13 → 9 with action 3 reward 0.0\n","Simulating from 13 → 5 with action 3 reward 0.0\n","Backprop node 13, visits=1, value=0.0\n","Backprop node 9, visits=2, value=0.0\n","Backprop node 8, visits=4, value=0.0\n","Backprop node 4, visits=8, value=0.0\n","Backprop node 0, visits=15, value=0.0\n","Expanded node 4 with children: [8, 0]\n","Simulating from 8 → 12 with action 1 reward 0.0\n","Backprop node 8, visits=1, value=0.0\n","Backprop node 4, visits=2, value=0.0\n","Backprop node 0, visits=4, value=0.0\n","Backprop node 1, visits=8, value=0.0\n","Backprop node 0, visits=16, value=0.0\n","Expanded node 4 with children: [8, 0]\n","Simulating from 0 → 0 with action 0 reward 0.0\n","Simulating from 0 → 1 with action 2 reward 0.0\n","Simulating from 0 → 1 with action 3 reward 0.0\n","Simulating from 0 → 2 with action 2 reward 0.0\n","Simulating from 0 → 1 with action 0 reward 0.0\n","Simulating from 0 → 1 with action 3 reward 0.0\n","Simulating from 0 → 0 with action 0 reward 0.0\n","Simulating from 0 → 1 with action 2 reward 0.0\n","Simulating from 0 → 2 with action 2 reward 0.0\n","Simulating from 0 → 1 with action 0 reward 0.0\n","Simulating from 0 → 2 with action 2 reward 0.0\n","Simulating from 0 → 3 with action 2 reward 0.0\n","Simulating from 0 → 2 with action 0 reward 0.0\n","Simulating from 0 → 3 with action 2 reward 0.0\n","Simulating from 0 → 7 with action 1 reward 0.0\n","Backprop node 0, visits=1, value=0.0\n","Backprop node 4, visits=2, value=0.0\n","Backprop node 0, visits=4, value=0.0\n","Backprop node 4, visits=9, value=0.0\n","Backprop node 0, visits=17, value=0.0\n","Simulating from 6 → 2 with action 3 reward 0.0\n","Simulating from 6 → 1 with action 0 reward 0.0\n","Simulating from 6 → 1 with action 3 reward 0.0\n","Simulating from 6 → 1 with action 3 reward 0.0\n","Simulating from 6 → 5 with action 1 reward 0.0\n","Backprop node 6, visits=1, value=0.0\n","Backprop node 2, visits=4, value=0.0\n","Backprop node 1, visits=9, value=0.0\n","Backprop node 0, visits=18, value=0.0\n","Expanded node 4 with children: [8, 0]\n","Simulating from 0 → 1 with action 2 reward 0.0\n","Simulating from 0 → 0 with action 0 reward 0.0\n","Simulating from 0 → 4 with action 1 reward 0.0\n","Simulating from 0 → 8 with action 1 reward 0.0\n","Simulating from 0 → 8 with action 0 reward 0.0\n","Simulating from 0 → 9 with action 2 reward 0.0\n","Simulating from 0 → 13 with action 1 reward 0.0\n","Simulating from 0 → 9 with action 3 reward 0.0\n","Simulating from 0 → 10 with action 2 reward 0.0\n","Simulating from 0 → 11 with action 2 reward 0.0\n","Backprop node 0, visits=1, value=0.0\n","Backprop node 4, visits=2, value=0.0\n","Backprop node 8, visits=5, value=0.0\n","Backprop node 4, visits=10, value=0.0\n","Backprop node 0, visits=19, value=0.0\n","Expanded node 1 with children: [0, 2]\n","Simulating from 2 → 3 with action 2 reward 0.0\n","Simulating from 2 → 7 with action 1 reward 0.0\n","Backprop node 2, visits=1, value=0.0\n","Backprop node 1, visits=2, value=0.0\n","Backprop node 0, visits=5, value=0.0\n","Backprop node 1, visits=10, value=0.0\n","Backprop node 0, visits=20, value=0.0\n","Expanded node 1 with children: [0, 2]\n","Simulating from 2 → 6 with action 1 reward 0.0\n","Simulating from 2 → 2 with action 3 reward 0.0\n","Simulating from 2 → 3 with action 2 reward 0.0\n","Simulating from 2 → 3 with action 3 reward 0.0\n","Simulating from 2 → 3 with action 2 reward 0.0\n","Simulating from 2 → 3 with action 3 reward 0.0\n","Simulating from 2 → 2 with action 0 reward 0.0\n","Simulating from 2 → 2 with action 3 reward 0.0\n","Simulating from 2 → 6 with action 1 reward 0.0\n","Simulating from 2 → 2 with action 3 reward 0.0\n","Simulating from 2 → 3 with action 2 reward 0.0\n","Simulating from 2 → 3 with action 3 reward 0.0\n","Simulating from 2 → 3 with action 3 reward 0.0\n","Simulating from 2 → 2 with action 0 reward 0.0\n","Simulating from 2 → 1 with action 0 reward 0.0\n","Simulating from 2 → 1 with action 3 reward 0.0\n","Simulating from 2 → 5 with action 1 reward 0.0\n","Backprop node 2, visits=1, value=0.0\n","Backprop node 1, visits=2, value=0.0\n","Backprop node 0, visits=5, value=0.0\n","Backprop node 4, visits=11, value=0.0\n","Backprop node 0, visits=21, value=0.0\n","Expanded node 1 with children: [0, 2]\n","Simulating from 0 → 0 with action 0 reward 0.0\n","Simulating from 0 → 1 with action 2 reward 0.0\n","Simulating from 0 → 2 with action 2 reward 0.0\n","Simulating from 0 → 3 with action 2 reward 0.0\n","Simulating from 0 → 2 with action 0 reward 0.0\n","Simulating from 0 → 6 with action 1 reward 0.0\n","Simulating from 0 → 2 with action 3 reward 0.0\n","Simulating from 0 → 6 with action 1 reward 0.0\n","Simulating from 0 → 10 with action 1 reward 0.0\n","Simulating from 0 → 6 with action 3 reward 0.0\n","Simulating from 0 → 2 with action 3 reward 0.0\n","Simulating from 0 → 1 with action 0 reward 0.0\n","Simulating from 0 → 5 with action 1 reward 0.0\n","Backprop node 0, visits=1, value=0.0\n","Backprop node 1, visits=2, value=0.0\n","Backprop node 2, visits=5, value=0.0\n","Backprop node 1, visits=11, value=0.0\n","Backprop node 0, visits=22, value=0.0\n","Simulating from 8 → 4 with action 3 reward 0.0\n","Simulating from 8 → 5 with action 2 reward 0.0\n","Backprop node 8, visits=1, value=0.0\n","Backprop node 9, visits=3, value=0.0\n","Backprop node 8, visits=6, value=0.0\n","Backprop node 4, visits=12, value=0.0\n","Backprop node 0, visits=23, value=0.0\n","Simulating from 0 → 4 with action 1 reward 0.0\n","Simulating from 0 → 5 with action 2 reward 0.0\n","Backprop node 0, visits=1, value=0.0\n","Backprop node 4, visits=3, value=0.0\n","Backprop node 0, visits=6, value=0.0\n","Backprop node 1, visits=12, value=0.0\n","Backprop node 0, visits=24, value=0.0\n","Simulating from 8 → 8 with action 0 reward 0.0\n","Simulating from 8 → 9 with action 2 reward 0.0\n","Simulating from 8 → 13 with action 1 reward 0.0\n","Simulating from 8 → 14 with action 2 reward 0.0\n","Simulating from 8 → 14 with action 1 reward 0.0\n","Simulating from 8 → 10 with action 3 reward 0.0\n","Simulating from 8 → 9 with action 0 reward 0.0\n","Simulating from 8 → 13 with action 1 reward 0.0\n","Simulating from 8 → 13 with action 1 reward 0.0\n","Simulating from 8 → 13 with action 1 reward 0.0\n","Simulating from 8 → 12 with action 0 reward 0.0\n","Backprop node 8, visits=1, value=0.0\n","Backprop node 4, visits=3, value=0.0\n","Backprop node 0, visits=6, value=0.0\n","Backprop node 4, visits=13, value=0.0\n","Backprop node 0, visits=25, value=0.0\n","Expanded node 6 with children: [10, 2]\n","Simulating from 10 → 9 with action 0 reward 0.0\n","Simulating from 10 → 10 with action 2 reward 0.0\n","Simulating from 10 → 11 with action 2 reward 0.0\n","Backprop node 10, visits=1, value=0.0\n","Backprop node 6, visits=2, value=0.0\n","Backprop node 2, visits=6, value=0.0\n","Backprop node 1, visits=13, value=0.0\n","Backprop node 0, visits=26, value=0.0\n","Simulating from 8 → 9 with action 2 reward 0.0\n","Simulating from 8 → 10 with action 2 reward 0.0\n","Simulating from 8 → 9 with action 0 reward 0.0\n","Simulating from 8 → 13 with action 1 reward 0.0\n","Simulating from 8 → 13 with action 1 reward 0.0\n","Simulating from 8 → 9 with action 3 reward 0.0\n","Simulating from 8 → 13 with action 1 reward 0.0\n","Simulating from 8 → 12 with action 0 reward 0.0\n","Backprop node 8, visits=1, value=0.0\n","Backprop node 4, visits=3, value=0.0\n","Backprop node 8, visits=7, value=0.0\n","Backprop node 4, visits=14, value=0.0\n","Backprop node 0, visits=27, value=0.0\n","Simulating from 0 → 1 with action 2 reward 0.0\n","Simulating from 0 → 2 with action 2 reward 0.0\n","Simulating from 0 → 1 with action 0 reward 0.0\n","Simulating from 0 → 5 with action 1 reward 0.0\n","Backprop node 0, visits=1, value=0.0\n","Backprop node 1, visits=3, value=0.0\n","Backprop node 0, visits=7, value=0.0\n","Backprop node 1, visits=14, value=0.0\n","Backprop node 0, visits=28, value=0.0\n","Simulating from 0 → 4 with action 1 reward 0.0\n","Simulating from 0 → 0 with action 3 reward 0.0\n","Simulating from 0 → 1 with action 2 reward 0.0\n","Simulating from 0 → 1 with action 3 reward 0.0\n","Simulating from 0 → 0 with action 0 reward 0.0\n","Simulating from 0 → 0 with action 0 reward 0.0\n","Simulating from 0 → 0 with action 3 reward 0.0\n","Simulating from 0 → 0 with action 0 reward 0.0\n","Simulating from 0 → 0 with action 0 reward 0.0\n","Simulating from 0 → 1 with action 2 reward 0.0\n","Simulating from 0 → 2 with action 2 reward 0.0\n","Simulating from 0 → 6 with action 1 reward 0.0\n","Simulating from 0 → 7 with action 2 reward 0.0\n","Backprop node 0, visits=1, value=0.0\n","Backprop node 1, visits=3, value=0.0\n","Backprop node 0, visits=7, value=0.0\n","Backprop node 4, visits=15, value=0.0\n","Backprop node 0, visits=29, value=0.0\n","Expanded node 3 with children: [2]\n","Simulating from 2 → 2 with action 3 reward 0.0\n","Simulating from 2 → 1 with action 0 reward 0.0\n","Simulating from 2 → 0 with action 0 reward 0.0\n","Simulating from 2 → 4 with action 1 reward 0.0\n","Simulating from 2 → 0 with action 3 reward 0.0\n","Simulating from 2 → 4 with action 1 reward 0.0\n","Simulating from 2 → 4 with action 0 reward 0.0\n","Simulating from 2 → 8 with action 1 reward 0.0\n","Simulating from 2 → 4 with action 3 reward 0.0\n","Simulating from 2 → 4 with action 0 reward 0.0\n","Simulating from 2 → 0 with action 3 reward 0.0\n","Simulating from 2 → 0 with action 3 reward 0.0\n","Simulating from 2 → 1 with action 2 reward 0.0\n","Simulating from 2 → 0 with action 0 reward 0.0\n","Simulating from 2 → 1 with action 2 reward 0.0\n","Simulating from 2 → 1 with action 3 reward 0.0\n","Simulating from 2 → 1 with action 3 reward 0.0\n","Simulating from 2 → 0 with action 0 reward 0.0\n","Simulating from 2 → 1 with action 2 reward 0.0\n","Simulating from 2 → 5 with action 1 reward 0.0\n","Backprop node 2, visits=1, value=0.0\n","Backprop node 3, visits=2, value=0.0\n","Backprop node 2, visits=7, value=0.0\n","Backprop node 1, visits=15, value=0.0\n","Backprop node 0, visits=30, value=0.0\n","Simulating from 10 → 9 with action 0 reward 0.0\n","Simulating from 10 → 8 with action 0 reward 0.0\n","Simulating from 10 → 8 with action 0 reward 0.0\n","Simulating from 10 → 9 with action 2 reward 0.0\n","Simulating from 10 → 8 with action 0 reward 0.0\n","Simulating from 10 → 8 with action 0 reward 0.0\n","Simulating from 10 → 9 with action 2 reward 0.0\n","Simulating from 10 → 10 with action 2 reward 0.0\n","Simulating from 10 → 9 with action 0 reward 0.0\n","Simulating from 10 → 13 with action 1 reward 0.0\n","Simulating from 10 → 13 with action 1 reward 0.0\n","Simulating from 10 → 14 with action 2 reward 0.0\n","Simulating from 10 → 10 with action 3 reward 0.0\n","Simulating from 10 → 9 with action 0 reward 0.0\n","Simulating from 10 → 5 with action 3 reward 0.0\n","Backprop node 10, visits=1, value=0.0\n","Backprop node 9, visits=4, value=0.0\n","Backprop node 8, visits=8, value=0.0\n","Backprop node 4, visits=16, value=0.0\n","Backprop node 0, visits=31, value=0.0\n","Expanded node 8 with children: [9, 4]\n","Simulating from 9 → 13 with action 1 reward 0.0\n","Simulating from 9 → 14 with action 2 reward 0.0\n","Simulating from 9 → 14 with action 1 reward 0.0\n","Simulating from 9 → 15 with action 2 reward 1.0\n","Backprop node 9, visits=1, value=1.0\n","Backprop node 8, visits=2, value=1.0\n","Backprop node 4, visits=4, value=1.0\n","Backprop node 0, visits=8, value=1.0\n","Backprop node 1, visits=16, value=1.0\n","Backprop node 0, visits=32, value=1.0\n","Simulating from 4 → 8 with action 1 reward 0.0\n","Simulating from 4 → 8 with action 0 reward 0.0\n","Simulating from 4 → 4 with action 3 reward 0.0\n","Simulating from 4 → 5 with action 2 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 8, visits=3, value=1.0\n","Backprop node 4, visits=5, value=1.0\n","Backprop node 0, visits=9, value=1.0\n","Backprop node 1, visits=17, value=1.0\n","Backprop node 0, visits=33, value=1.0\n","Expanded node 0 with children: [4, 1]\n","Simulating from 4 → 8 with action 1 reward 0.0\n","Simulating from 4 → 4 with action 3 reward 0.0\n","Simulating from 4 → 4 with action 0 reward 0.0\n","Simulating from 4 → 5 with action 2 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 0, visits=2, value=0.0\n","Backprop node 1, visits=4, value=0.0\n","Backprop node 0, visits=10, value=1.0\n","Backprop node 1, visits=18, value=1.0\n","Backprop node 0, visits=34, value=1.0\n","Simulating from 2 → 1 with action 0 reward 0.0\n","Simulating from 2 → 1 with action 3 reward 0.0\n","Simulating from 2 → 2 with action 2 reward 0.0\n","Simulating from 2 → 6 with action 1 reward 0.0\n","Simulating from 2 → 7 with action 2 reward 0.0\n","Backprop node 2, visits=1, value=0.0\n","Backprop node 1, visits=3, value=0.0\n","Backprop node 2, visits=8, value=0.0\n","Backprop node 1, visits=19, value=1.0\n","Backprop node 0, visits=35, value=1.0\n","Expanded node 8 with children: [9, 4]\n","Simulating from 9 → 8 with action 0 reward 0.0\n","Simulating from 9 → 12 with action 1 reward 0.0\n","Backprop node 9, visits=1, value=0.0\n","Backprop node 8, visits=2, value=0.0\n","Backprop node 4, visits=4, value=0.0\n","Backprop node 0, visits=8, value=0.0\n","Backprop node 4, visits=17, value=0.0\n","Backprop node 0, visits=36, value=1.0\n","Expanded node 0 with children: [4, 1]\n","Simulating from 4 → 5 with action 2 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 0, visits=2, value=0.0\n","Backprop node 4, visits=6, value=1.0\n","Backprop node 0, visits=11, value=1.0\n","Backprop node 1, visits=20, value=1.0\n","Backprop node 0, visits=37, value=1.0\n","Expanded node 8 with children: [9, 4]\n","Simulating from 4 → 5 with action 2 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 8, visits=2, value=0.0\n","Backprop node 4, visits=4, value=0.0\n","Backprop node 8, visits=9, value=0.0\n","Backprop node 4, visits=18, value=0.0\n","Backprop node 0, visits=38, value=1.0\n","Simulating from 2 → 2 with action 3 reward 0.0\n","Simulating from 2 → 2 with action 3 reward 0.0\n","Simulating from 2 → 1 with action 0 reward 0.0\n","Simulating from 2 → 1 with action 3 reward 0.0\n","Simulating from 2 → 2 with action 2 reward 0.0\n","Simulating from 2 → 2 with action 3 reward 0.0\n","Simulating from 2 → 1 with action 0 reward 0.0\n","Simulating from 2 → 1 with action 3 reward 0.0\n","Simulating from 2 → 5 with action 1 reward 0.0\n","Backprop node 2, visits=1, value=0.0\n","Backprop node 6, visits=3, value=0.0\n","Backprop node 2, visits=9, value=0.0\n","Backprop node 1, visits=21, value=1.0\n","Backprop node 0, visits=39, value=1.0\n","Expanded node 2 with children: [1, 6, 3]\n","Simulating from 3 → 3 with action 3 reward 0.0\n","Simulating from 3 → 3 with action 3 reward 0.0\n","Simulating from 3 → 2 with action 0 reward 0.0\n","Simulating from 3 → 1 with action 0 reward 0.0\n","Simulating from 3 → 1 with action 3 reward 0.0\n","Simulating from 3 → 1 with action 3 reward 0.0\n","Simulating from 3 → 1 with action 3 reward 0.0\n","Simulating from 3 → 1 with action 3 reward 0.0\n","Simulating from 3 → 0 with action 0 reward 0.0\n","Simulating from 3 → 0 with action 0 reward 0.0\n","Simulating from 3 → 1 with action 2 reward 0.0\n","Simulating from 3 → 5 with action 1 reward 0.0\n","Backprop node 3, visits=1, value=0.0\n","Backprop node 2, visits=2, value=0.0\n","Backprop node 1, visits=5, value=0.0\n","Backprop node 0, visits=12, value=1.0\n","Backprop node 1, visits=22, value=1.0\n","Backprop node 0, visits=40, value=1.0\n","Expanded node 0 with children: [4, 1]\n","Simulating from 1 → 5 with action 1 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 0, visits=2, value=0.0\n","Backprop node 1, visits=4, value=0.0\n","Backprop node 0, visits=9, value=0.0\n","Backprop node 4, visits=19, value=0.0\n","Backprop node 0, visits=41, value=1.0\n","Expanded node 2 with children: [1, 6, 3]\n","Simulating from 3 → 3 with action 3 reward 0.0\n","Simulating from 3 → 7 with action 1 reward 0.0\n","Backprop node 3, visits=1, value=0.0\n","Backprop node 2, visits=2, value=0.0\n","Backprop node 3, visits=3, value=0.0\n","Backprop node 2, visits=10, value=0.0\n","Backprop node 1, visits=23, value=1.0\n","Backprop node 0, visits=42, value=1.0\n","Expanded node 8 with children: [9, 4]\n","Simulating from 9 → 10 with action 2 reward 0.0\n","Simulating from 9 → 14 with action 1 reward 0.0\n","Simulating from 9 → 13 with action 0 reward 0.0\n","Simulating from 9 → 12 with action 0 reward 0.0\n","Backprop node 9, visits=1, value=0.0\n","Backprop node 8, visits=2, value=0.0\n","Backprop node 9, visits=5, value=0.0\n","Backprop node 8, visits=10, value=0.0\n","Backprop node 4, visits=20, value=0.0\n","Backprop node 0, visits=43, value=1.0\n","Expanded node 9 with children: [8, 13, 10]\n","Simulating from 10 → 6 with action 3 reward 0.0\n","Simulating from 10 → 7 with action 2 reward 0.0\n","Backprop node 10, visits=1, value=0.0\n","Backprop node 9, visits=2, value=1.0\n","Backprop node 8, visits=4, value=1.0\n","Backprop node 4, visits=7, value=1.0\n","Backprop node 0, visits=13, value=1.0\n","Backprop node 1, visits=24, value=1.0\n","Backprop node 0, visits=44, value=1.0\n","Expanded node 0 with children: [4, 1]\n","Simulating from 1 → 1 with action 3 reward 0.0\n","Simulating from 1 → 0 with action 0 reward 0.0\n","Simulating from 1 → 1 with action 2 reward 0.0\n","Simulating from 1 → 1 with action 3 reward 0.0\n","Simulating from 1 → 1 with action 3 reward 0.0\n","Simulating from 1 → 2 with action 2 reward 0.0\n","Simulating from 1 → 1 with action 0 reward 0.0\n","Simulating from 1 → 5 with action 1 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 0, visits=2, value=0.0\n","Backprop node 4, visits=5, value=0.0\n","Backprop node 0, visits=10, value=0.0\n","Backprop node 4, visits=21, value=0.0\n","Backprop node 0, visits=45, value=1.0\n","Expanded node 0 with children: [4, 1]\n","Simulating from 4 → 4 with action 0 reward 0.0\n","Simulating from 4 → 8 with action 1 reward 0.0\n","Simulating from 4 → 12 with action 1 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 0, visits=2, value=0.0\n","Backprop node 1, visits=4, value=0.0\n","Backprop node 2, visits=11, value=0.0\n","Backprop node 1, visits=25, value=1.0\n","Backprop node 0, visits=46, value=1.0\n","Expanded node 0 with children: [4, 1]\n","Simulating from 4 → 8 with action 1 reward 0.0\n","Simulating from 4 → 4 with action 3 reward 0.0\n","Simulating from 4 → 4 with action 0 reward 0.0\n","Simulating from 4 → 4 with action 0 reward 0.0\n","Simulating from 4 → 0 with action 3 reward 0.0\n","Simulating from 4 → 0 with action 0 reward 0.0\n","Simulating from 4 → 1 with action 2 reward 0.0\n","Simulating from 4 → 5 with action 1 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 0, visits=2, value=0.0\n","Backprop node 4, visits=5, value=0.0\n","Backprop node 8, visits=11, value=0.0\n","Backprop node 4, visits=22, value=0.0\n","Backprop node 0, visits=47, value=1.0\n","Simulating from 1 → 5 with action 1 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 0, visits=3, value=0.0\n","Backprop node 1, visits=6, value=0.0\n","Backprop node 0, visits=14, value=1.0\n","Backprop node 1, visits=26, value=1.0\n","Backprop node 0, visits=48, value=1.0\n","Expanded node 2 with children: [1, 6, 3]\n","Simulating from 6 → 10 with action 1 reward 0.0\n","Simulating from 6 → 14 with action 1 reward 0.0\n","Simulating from 6 → 15 with action 2 reward 1.0\n","Backprop node 6, visits=1, value=1.0\n","Backprop node 2, visits=2, value=1.0\n","Backprop node 1, visits=5, value=1.0\n","Backprop node 0, visits=11, value=1.0\n","Backprop node 4, visits=23, value=1.0\n","Backprop node 0, visits=49, value=2.0\n","Simulating from 1 → 2 with action 2 reward 0.0\n","Simulating from 1 → 3 with action 2 reward 0.0\n","Simulating from 1 → 3 with action 2 reward 0.0\n","Simulating from 1 → 2 with action 0 reward 0.0\n","Simulating from 1 → 1 with action 0 reward 0.0\n","Simulating from 1 → 1 with action 3 reward 0.0\n","Simulating from 1 → 0 with action 0 reward 0.0\n","Simulating from 1 → 1 with action 2 reward 0.0\n","Simulating from 1 → 1 with action 3 reward 0.0\n","Simulating from 1 → 1 with action 3 reward 0.0\n","Simulating from 1 → 5 with action 1 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 2, visits=3, value=1.0\n","Backprop node 1, visits=6, value=1.0\n","Backprop node 0, visits=12, value=1.0\n","Backprop node 4, visits=24, value=1.0\n","Backprop node 0, visits=50, value=2.0\n","Simulating from 3 → 7 with action 1 reward 0.0\n","Backprop node 3, visits=1, value=0.0\n","Backprop node 2, visits=4, value=1.0\n","Backprop node 1, visits=7, value=1.0\n","Backprop node 0, visits=13, value=1.0\n","Backprop node 4, visits=25, value=1.0\n","Backprop node 0, visits=51, value=2.0\n","Simulating from 4 → 8 with action 1 reward 0.0\n","Simulating from 4 → 12 with action 1 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 8, visits=3, value=0.0\n","Backprop node 4, visits=6, value=0.0\n","Backprop node 0, visits=14, value=1.0\n","Backprop node 4, visits=26, value=1.0\n","Backprop node 0, visits=52, value=2.0\n","Expanded node 13 with children: [14, 9]\n","Simulating from 14 → 14 with action 1 reward 0.0\n","Simulating from 14 → 14 with action 1 reward 0.0\n","Simulating from 14 → 13 with action 0 reward 0.0\n","Simulating from 14 → 14 with action 2 reward 0.0\n","Simulating from 14 → 14 with action 1 reward 0.0\n","Simulating from 14 → 14 with action 1 reward 0.0\n","Simulating from 14 → 14 with action 1 reward 0.0\n","Simulating from 14 → 13 with action 0 reward 0.0\n","Simulating from 14 → 14 with action 2 reward 0.0\n","Simulating from 14 → 15 with action 2 reward 1.0\n","Backprop node 14, visits=1, value=1.0\n","Backprop node 13, visits=2, value=1.0\n","Backprop node 9, visits=6, value=1.0\n","Backprop node 8, visits=12, value=1.0\n","Backprop node 4, visits=27, value=2.0\n","Backprop node 0, visits=53, value=3.0\n","Expanded node 10 with children: [9, 14, 6]\n","Simulating from 9 → 13 with action 1 reward 0.0\n","Simulating from 9 → 9 with action 3 reward 0.0\n","Simulating from 9 → 8 with action 0 reward 0.0\n","Simulating from 9 → 9 with action 2 reward 0.0\n","Simulating from 9 → 13 with action 1 reward 0.0\n","Simulating from 9 → 12 with action 0 reward 0.0\n","Backprop node 9, visits=1, value=0.0\n","Backprop node 10, visits=2, value=0.0\n","Backprop node 9, visits=7, value=1.0\n","Backprop node 8, visits=13, value=1.0\n","Backprop node 4, visits=28, value=2.0\n","Backprop node 0, visits=54, value=3.0\n","Simulating from 9 → 5 with action 3 reward 0.0\n","Backprop node 9, visits=1, value=0.0\n","Backprop node 8, visits=3, value=0.0\n","Backprop node 4, visits=6, value=0.0\n","Backprop node 8, visits=14, value=1.0\n","Backprop node 4, visits=29, value=2.0\n","Backprop node 0, visits=55, value=3.0\n","Simulating from 9 → 10 with action 2 reward 0.0\n","Simulating from 9 → 11 with action 2 reward 0.0\n","Backprop node 9, visits=1, value=0.0\n","Backprop node 13, visits=3, value=1.0\n","Backprop node 9, visits=8, value=1.0\n","Backprop node 8, visits=15, value=1.0\n","Backprop node 4, visits=30, value=2.0\n","Backprop node 0, visits=56, value=3.0\n","Expanded node 10 with children: [9, 14, 6]\n","Simulating from 9 → 10 with action 2 reward 0.0\n","Simulating from 9 → 6 with action 3 reward 0.0\n","Simulating from 9 → 10 with action 1 reward 0.0\n","Simulating from 9 → 14 with action 1 reward 0.0\n","Simulating from 9 → 13 with action 0 reward 0.0\n","Simulating from 9 → 14 with action 2 reward 0.0\n","Simulating from 9 → 14 with action 1 reward 0.0\n","Simulating from 9 → 15 with action 2 reward 1.0\n","Backprop node 9, visits=1, value=1.0\n","Backprop node 10, visits=2, value=1.0\n","Backprop node 6, visits=4, value=1.0\n","Backprop node 2, visits=12, value=1.0\n","Backprop node 1, visits=27, value=2.0\n","Backprop node 0, visits=57, value=4.0\n","Simulating from 14 → 14 with action 1 reward 0.0\n","Simulating from 14 → 13 with action 0 reward 0.0\n","Simulating from 14 → 9 with action 3 reward 0.0\n","Simulating from 14 → 8 with action 0 reward 0.0\n","Simulating from 14 → 12 with action 1 reward 0.0\n","Backprop node 14, visits=1, value=0.0\n","Backprop node 10, visits=3, value=1.0\n","Backprop node 6, visits=5, value=1.0\n","Backprop node 2, visits=13, value=1.0\n","Backprop node 1, visits=28, value=2.0\n","Backprop node 0, visits=58, value=4.0\n","Simulating from 1 → 1 with action 3 reward 0.0\n","Simulating from 1 → 5 with action 1 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 2, visits=3, value=0.0\n","Backprop node 3, visits=4, value=0.0\n","Backprop node 2, visits=14, value=1.0\n","Backprop node 1, visits=29, value=2.0\n","Backprop node 0, visits=59, value=4.0\n","Simulating from 1 → 5 with action 1 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 0, visits=3, value=0.0\n","Backprop node 4, visits=8, value=1.0\n","Backprop node 0, visits=15, value=1.0\n","Backprop node 1, visits=30, value=2.0\n","Backprop node 0, visits=60, value=4.0\n","Simulating from 4 → 8 with action 1 reward 0.0\n","Simulating from 4 → 4 with action 3 reward 0.0\n","Simulating from 4 → 4 with action 0 reward 0.0\n","Simulating from 4 → 4 with action 0 reward 0.0\n","Simulating from 4 → 5 with action 2 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 0, visits=3, value=0.0\n","Backprop node 1, visits=8, value=1.0\n","Backprop node 0, visits=15, value=1.0\n","Backprop node 4, visits=31, value=2.0\n","Backprop node 0, visits=61, value=4.0\n","Expanded node 2 with children: [1, 6, 3]\n","Simulating from 3 → 2 with action 0 reward 0.0\n","Simulating from 3 → 2 with action 3 reward 0.0\n","Simulating from 3 → 1 with action 0 reward 0.0\n","Simulating from 3 → 1 with action 3 reward 0.0\n","Simulating from 3 → 5 with action 1 reward 0.0\n","Backprop node 3, visits=1, value=0.0\n","Backprop node 2, visits=2, value=0.0\n","Backprop node 6, visits=6, value=1.0\n","Backprop node 2, visits=15, value=1.0\n","Backprop node 1, visits=31, value=2.0\n","Backprop node 0, visits=62, value=4.0\n","Simulating from 1 → 0 with action 0 reward 0.0\n","Simulating from 1 → 4 with action 1 reward 0.0\n","Simulating from 1 → 5 with action 2 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 0, visits=3, value=0.0\n","Backprop node 4, visits=7, value=0.0\n","Backprop node 8, visits=16, value=1.0\n","Backprop node 4, visits=32, value=2.0\n","Backprop node 0, visits=63, value=4.0\n","Simulating from 1 → 5 with action 1 reward 0.0\n","Backprop node 1, visits=1, value=0.0\n","Backprop node 2, visits=3, value=0.0\n","Backprop node 1, visits=7, value=0.0\n","Backprop node 0, visits=16, value=1.0\n","Backprop node 1, visits=32, value=2.0\n","Backprop node 0, visits=64, value=4.0\n","Simulating from 4 → 0 with action 3 reward 0.0\n","Simulating from 4 → 1 with action 2 reward 0.0\n","Simulating from 4 → 0 with action 0 reward 0.0\n","Simulating from 4 → 1 with action 2 reward 0.0\n","Simulating from 4 → 2 with action 2 reward 0.0\n","Simulating from 4 → 6 with action 1 reward 0.0\n","Simulating from 4 → 7 with action 2 reward 0.0\n","Backprop node 4, visits=1, value=0.0\n","Backprop node 0, visits=3, value=0.0\n","Backprop node 4, visits=7, value=0.0\n","Backprop node 0, visits=16, value=1.0\n","Backprop node 4, visits=33, value=2.0\n","Backprop node 0, visits=65, value=4.0\n","Expanded node 2 with children: [1, 6, 3]\n","Simulating from 3 → 2 with action 0 reward 0.0\n","Simulating from 3 → 1 with action 0 reward 0.0\n","Simulating from 3 → 5 with action 1 reward 0.0\n","Backprop node 3, visits=1, value=0.0\n","Backprop node 2, visits=2, value=0.0\n","Backprop node 1, visits=5, value=0.0\n","Backprop node 2, visits=16, value=1.0\n","Backprop node 1, visits=33, value=2.0\n","Backprop node 0, visits=66, value=4.0\n","Goal found from state 14 with action 2 → 15\n","Backprop node 15, visits=1, value=1.0\n","Backprop node 14, visits=2, value=2.0\n","Backprop node 13, visits=4, value=2.0\n","Backprop node 9, visits=9, value=2.0\n","Backprop node 8, visits=17, value=2.0\n","Backprop node 4, visits=34, value=3.0\n","Backprop node 0, visits=67, value=5.0\n","Goal reached at state 15\n","Finished 1000 iterations.\n","Initial state:\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","\n","Best action from state 0 to state 4 with value 3.0\n","  (Down)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","\n","Best action from state 4 to state 8 with value 2.0\n","  (Down)\n","SFFF\n","FHFH\n","\u001b[41mF\u001b[0mFFH\n","HFFG\n","\n","Best action from state 8 to state 9 with value 2.0\n","  (Right)\n","SFFF\n","FHFH\n","F\u001b[41mF\u001b[0mFH\n","HFFG\n","\n","Best action from state 9 to state 13 with value 2.0\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","H\u001b[41mF\u001b[0mFG\n","\n","Best action from state 13 to state 14 with value 2.0\n","  (Right)\n","SFFF\n","FHFH\n","FFFH\n","HF\u001b[41mF\u001b[0mG\n","\n","Best action from state 14 to state 15 with value 1.0\n","  (Right)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","\n"]}],"source":["\n","# --- Main Execution ---\n","def make_env():\n","    return gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n","\n","# Run MCTS\n","mcts = MCTS(make_env=make_env, num_iterations=1000, num_simulations=100, exploration=1.41, verbose=True)\n","mcts.run()\n","\n","# Visualise best path\n","env = make_env()\n","env.reset()\n","print(\"Initial state:\")\n","print(env.render())\n","\n","trajectory = []\n","node = mcts.root\n","trajectory.append((node.state, node.action, node.value))\n","while not node.is_leaf():\n","    prev_node = node\n","    node = prev_node.best_child()\n","    trajectory.append((node.state, node.action, node.value))\n","    print(f\"Best action from state {prev_node.state} to state {node.state} with value {node.value}\")\n","    env.step(node.action)\n","    print(env.render())"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":645,"status":"error","timestamp":1746993663319,"user":{"displayName":"Julia López Gómez","userId":"04858267532126932932"},"user_tz":-60},"id":"b33WuiTw3k1B","outputId":"58496479-8d45-4bde-b2ad-a6e359237abe"},"outputs":[{"name":"stderr","output_type":"stream","text":["bash: line 1: cd: monte-carlo-manipulation/: No such file or directory\n"]},{"name":"stdout","output_type":"stream","text":["[main 94d7ca9] added collab runner file\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"]},{"name":"stderr","output_type":"stream","text":["To https://github.com/julialopezgomez/monte-carlo-manipulation.git\n","   5a56108..94d7ca9  main -> main\n"]}],"source":["%%bash\n","cd monte-carlo-manipulation/\n","git add .\n","git commit -m \"added collab runner file\"\n","git push\n"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":358,"status":"ok","timestamp":1746993606575,"user":{"displayName":"Julia López Gómez","userId":"04858267532126932932"},"user_tz":-60},"id":"SVkKu70B4CQS"},"outputs":[],"source":["%%bash\n","cp drive/MyDrive/Universidad/MCTS_colab_runner.ipynb monte-carlo-manipulation/src/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbcu_KVE7u2Z"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPGr1jERrpzl4gbgAA0kk7d","gpuType":"A100","machine_shape":"hm","mount_file_id":"1poeNGFM_sc4IGEWs302d_iMgKkvBuQjX","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
