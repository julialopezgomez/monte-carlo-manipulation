{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MCTS Node Class ---\n",
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, action=None, make_env=None, verbose=False):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0.0\n",
    "        self.make_env = make_env\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def uct(self, exploration=1.41):\n",
    "        if self.visits == 0 or self.parent is None:\n",
    "            return float(\"inf\")\n",
    "        exploitation = self.value / self.visits\n",
    "        exploration_bonus = exploration * math.sqrt(math.log(self.parent.visits) / self.visits)\n",
    "        return exploitation + exploration_bonus\n",
    "\n",
    "    def best_uct_child(self):\n",
    "        return max(self.children, key=lambda child: child.uct())\n",
    "\n",
    "    def best_child(self):\n",
    "        return max(self.children, key=lambda child: child.value)\n",
    "\n",
    "    def selection(self, env):\n",
    "        \"\"\"Traverse the tree to select a promising node to expand.\"\"\"\n",
    "        node = self\n",
    "        while not node.is_leaf():\n",
    "            node = node.best_uct_child()\n",
    "        if node.visits == 0:\n",
    "            return node, None\n",
    "        goal_node = node.expand(env)\n",
    "        return (goal_node if goal_node else random.choice(node.children), goal_node is not None)\n",
    "\n",
    "    def expand(self, env):\n",
    "        \"\"\"Expand the node by trying all possible actions.\"\"\"\n",
    "        for action in range(env.action_space.n):\n",
    "            env_copy = self.make_env()\n",
    "            env_copy.reset()\n",
    "            env_copy.unwrapped.s = self.state\n",
    "            obs, reward, terminated, truncated, _ = env_copy.step(action)\n",
    "\n",
    "            if reward == 0 and terminated or self.state == obs:\n",
    "                continue\n",
    "\n",
    "            child = MCTSNode(obs, parent=self, action=action, make_env=self.make_env, verbose=self.verbose)\n",
    "            self.children.append(child)\n",
    "\n",
    "            if reward == 1:\n",
    "                if self.verbose:\n",
    "                    print(f\"Goal found from state {self.state} with action {action} → {obs}\")\n",
    "                return child\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Expanded node {self.state} with children: {[c.state for c in self.children]}\")\n",
    "        return None\n",
    "\n",
    "    def simulation(self, max_steps=10):\n",
    "        \"\"\"Perform a rollout from the current node using random actions.\"\"\"\n",
    "        env_copy = self.make_env()\n",
    "        env_copy.reset()\n",
    "        env_copy.unwrapped.s = self.state\n",
    "        obs = self.state\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            action = env_copy.action_space.sample()\n",
    "            obs, reward, terminated, truncated, _ = env_copy.step(action)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Simulating from {self.state} → {obs} with action {action} reward {reward}\")\n",
    "\n",
    "            if reward == 1 or terminated or truncated:\n",
    "                return reward\n",
    "        return 0\n",
    "\n",
    "    def backpropagation(self, reward):\n",
    "        \"\"\"Propagate the simulation result back up the tree.\"\"\"\n",
    "        node = self\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value += reward\n",
    "            if self.verbose:\n",
    "                print(f\"Backprop node {node.state}, visits={node.visits}, value={node.value}\")\n",
    "            node = node.parent\n",
    "\n",
    "\n",
    "# --- MCTS Search Class ---\n",
    "class MCTS:\n",
    "    def __init__(self, make_env, num_iterations=100, num_simulations=10, exploration=1.41, verbose=False):\n",
    "        self.make_env = make_env\n",
    "        self.env = make_env()\n",
    "        self.root = MCTSNode(self.env.reset()[0], make_env=self.make_env, verbose=verbose)\n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_simulations = num_simulations\n",
    "        self.exploration = exploration\n",
    "        self.verbose = verbose\n",
    "        self.root.expand(self.env)\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.num_iterations):\n",
    "            node, goal = self.root.selection(self.env)\n",
    "            if goal:\n",
    "                node.backpropagation(1)\n",
    "                if self.verbose:\n",
    "                    print(f\"Goal reached at state {node.state}\")\n",
    "                break\n",
    "            reward = node.simulation(max_steps=self.num_simulations)\n",
    "            node.backpropagation(reward)\n",
    "        if self.verbose:\n",
    "            print(f\"Finished {self.num_iterations} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManipulationEnv(gym.Env):\n",
    "    \"\"\"Custom GridWorld environment with an agent and movable box.\"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=4, agent_start=0, box_start=1, goal_agent=13, goal_box=15, holes=None):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.n_states = grid_size * grid_size\n",
    "        self.agent_start = agent_start\n",
    "        self.box_start = box_start\n",
    "        self.goal_agent = goal_agent\n",
    "        self.goal_box = goal_box\n",
    "        self.agent_pos = agent_start\n",
    "        self.box_pos = box_start\n",
    "        self.holding = False\n",
    "        self.holes = holes if holes is not None else {5, 7, 11, 12}\n",
    "\n",
    "        # Actions: up, down, left, right, grab, release\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.observation_space = spaces.MultiDiscrete([self.n_states, self.n_states, 2])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.agent_pos = self.agent_start\n",
    "        self.box_pos = self.box_start\n",
    "        self.holding = False\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return (self.agent_pos, self.box_pos, int(self.holding))\n",
    "\n",
    "    def _to_coord(self, pos):\n",
    "        return divmod(pos, self.grid_size)\n",
    "\n",
    "    def _to_index(self, row, col):\n",
    "        return row * self.grid_size + col\n",
    "\n",
    "    def _move(self, pos, action):\n",
    "        row, col = self._to_coord(pos)\n",
    "        if action == 0 and row > 0: row -= 1        # up\n",
    "        elif action == 1 and row < self.grid_size - 1: row += 1  # down\n",
    "        elif action == 2 and col > 0: col -= 1      # left\n",
    "        elif action == 3 and col < self.grid_size - 1: col += 1  # right\n",
    "        return self._to_index(row, col)\n",
    "\n",
    "    def step(self, action):\n",
    "        if action in [0, 1, 2, 3]:  # movement\n",
    "            new_pos = self._move(self.agent_pos, action)\n",
    "            if new_pos in self.holes:\n",
    "                return self._get_obs(), 0.0, True, False, {}\n",
    "            if self.holding:\n",
    "                self.agent_pos = new_pos\n",
    "                self.box_pos = new_pos\n",
    "            else:\n",
    "                self.agent_pos = new_pos\n",
    "\n",
    "        elif action == 4:  # grab\n",
    "            if self.agent_pos == self.box_pos:\n",
    "                self.holding = True\n",
    "\n",
    "        elif action == 5:  # release\n",
    "            self.holding = False\n",
    "\n",
    "        # If agent or box is in a hole, terminate\n",
    "        if self.agent_pos in self.holes or self.box_pos in self.holes:\n",
    "            return self._get_obs(), 0.0, True, False, {}\n",
    "\n",
    "        # Check goal condition\n",
    "        success = (\n",
    "            self.agent_pos == self.goal_agent and\n",
    "            self.box_pos == self.goal_box and\n",
    "            not self.holding\n",
    "        )\n",
    "        reward = 1.0 if success else 0.0\n",
    "        return self._get_obs(), reward, success, False, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:\n",
      "Best action from state (0, 1, 0) to state (4, 1, 0) with value 0.0\n",
      "Best action from state (4, 1, 0) to state (0, 1, 0) with value 0.0\n",
      "Best action from state (0, 1, 0) to state (4, 1, 0) with value 0.0\n",
      "Best action from state (4, 1, 0) to state (0, 1, 0) with value 0.0\n",
      "Best action from state (0, 1, 0) to state (4, 1, 0) with value 0.0\n",
      "Best action from state (4, 1, 0) to state (0, 1, 0) with value 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Execution ---\n",
    "def make_env():\n",
    "    return ManipulationEnv()\n",
    "\n",
    "# Run MCTS\n",
    "mcts = MCTS(make_env=make_env, num_iterations=1000, num_simulations=100, exploration=1.41, verbose=False)\n",
    "mcts.run()\n",
    "\n",
    "# Visualise best path\n",
    "env = make_env()\n",
    "env.reset()\n",
    "print(\"Initial state:\")\n",
    "\n",
    "trajectory = []\n",
    "node = mcts.root\n",
    "trajectory.append((node.state, node.action, node.value))\n",
    "while not node.is_leaf():\n",
    "    prev_node = node\n",
    "    node = prev_node.best_child()\n",
    "    trajectory.append((node.state, node.action, node.value))\n",
    "    print(f\"Best action from state {prev_node.state} to state {node.state} with value {node.value}\")\n",
    "    env.step(node.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved from (0, 1, 0) to (0, 1, 0) with action 2 and reward 0.0\n",
      "Moved from (0, 1, 0) to (0, 1, 0) with action 5 and reward 0.0\n",
      "Moved from (0, 1, 0) to (1, 1, 0) with action 3 and reward 0.0\n",
      "Moved from (1, 1, 0) to (1, 1, 1) with action 4 and reward 0.0\n",
      "Moved from (1, 1, 1) to (1, 1, 1) with action 1 and reward 0.0\n"
     ]
    }
   ],
   "source": [
    "env = ManipulationEnv(goal_agent=2, goal_box=2)\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    prev_obs = obs\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    \n",
    "    print(f\"Moved from {prev_obs} to {obs} with action {action} and reward {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
