{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qPaUL1ZltkFd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Discrete, MultiDiscrete\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import deque, defaultdict\n",
        "from tqdm import tqdm\n",
        "from train import AlphaLoss\n",
        "from environments import FrozenLakeManipulationEnv\n",
        "from data_loading import to_one_hot_encoding, ReplayBuffer, ReplayDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w9RZ1UOrtkFg"
      },
      "outputs": [],
      "source": [
        "# MCTS / AlphaZero params\n",
        "NUM_SIMS     = 5000       # MCTS simulations/iterations per self-play step\n",
        "NUM_SELF_PLAY = 10       # number of self-play games to generate per epoch/episode\n",
        "CPUCT        = 1.41       # PUCT exploration constant\n",
        "TAU          = 1.0       # temperature for π = N^(1/τ)\n",
        "# Training params\n",
        "BATCH_SIZE   = 128\n",
        "LR           = 1e-3\n",
        "EVAL_INTERVAL= 1       # eval every self-play games\n",
        "TARGET_SR    = 0.95      # stop when success rate ≥ 95%\n",
        "REGULARIZATION = 1e-4    # L2 regularization weight decay constant\n",
        "MAX_EPISODES = 20 # max number of self-play episodes\n",
        "\n",
        "NUM_EVAL     = 50\n",
        "BUFFER_SIZE   = 20000\n",
        "SAMPLE_SIZE   = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ae98cEP9tkFg"
      },
      "outputs": [],
      "source": [
        "class AlphaZeroNet(nn.Module):\n",
        "    def __init__(self, n_states, n_actions, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(n_states, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # policy head\n",
        "        self.policy_head = nn.Linear(hidden_dim, n_actions)\n",
        "        # value head\n",
        "        self.value_head  = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: one-hot or feature vector of shape (batch, n_states)\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        p = F.log_softmax(self.policy_head(h), dim=1)  # log-probs\n",
        "        v = torch.tanh(self.value_head(h))             # in [-1,1]\n",
        "        return p, v.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxPpY3j7tkFh",
        "outputId": "df0d9156-ce4a-483c-ffa3-228592439574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# --- Main Execution ---\n",
        "def make_env():\n",
        "    # return gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n",
        "    return FrozenLakeManipulationEnv()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "env    = make_env()\n",
        "nA = env.action_space.n\n",
        "\n",
        "if isinstance(env.observation_space, Discrete):\n",
        "    nS = env.observation_space.n\n",
        "else:\n",
        "    # Assuming the observation space is a tuple of (states, ..., states, holding/not_holding) \n",
        "    nS = (len(env.observation_space.sample()) - 1) * env.n_states + 1\n",
        "net    = AlphaZeroNet(nS, nA).to(device)\n",
        "optimizer   = optim.Adam(net.parameters(), lr=LR, weight_decay=REGULARIZATION)\n",
        "scheduler   = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kbcu_KVE7u2Z"
      },
      "outputs": [],
      "source": [
        "class LearnedMCTSNode:\n",
        "    def __init__(self, \n",
        "                 state,\n",
        "                 make_env,\n",
        "                 net,\n",
        "                 parent=None, \n",
        "                 action=None, \n",
        "                 prior=0.0,\n",
        "                 cpuct=1.41,\n",
        "                 device='cpu',\n",
        "                 verbose=False):\n",
        "        \n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.action = action            # action taken to reach this node\n",
        "        self.prior = prior              # prior probability of this action\n",
        "        self.children = {}\n",
        "        \n",
        "        self.N = defaultdict(int)       # visit counts per action\n",
        "        self.W = defaultdict(float)     # total reward per action\n",
        "        self.Q = defaultdict(float)     # average reward per action (Q = W/N)\n",
        "        self.reward = 0.0\n",
        "        self.terminal = False\n",
        "        \n",
        "        self.puct_constant = cpuct\n",
        "        \n",
        "        self.make_env = make_env\n",
        "        self.env = make_env()\n",
        "        self.net = net\n",
        "        self.device = device\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return len(self.children) == 0\n",
        "\n",
        "    def puct(self):\n",
        "        \"\"\"Calculate the PUCT value for this node.\n",
        "\n",
        "        Returns:\n",
        "            puct_value (float): The PUCT value for this node.\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.parent.N[self.action] == 0:\n",
        "            return float(\"inf\")\n",
        "        \n",
        "        total_N = sum(self.parent.N.values())\n",
        "        \n",
        "        exploitation = self.parent.Q[self.action]\n",
        "        exploration = self.puct_constant * self.prior * math.sqrt(total_N) / (1 + self.parent.N[self.action])\n",
        "        \n",
        "        return exploitation + exploration\n",
        "        \n",
        "\n",
        "    def best_puct_child(self):\n",
        "        return max(self.children.values(), key=lambda child: child.puct())\n",
        "\n",
        "    def best_child(self):\n",
        "        return max(self.children.values(), key=lambda child: self.Q[child.action])\n",
        "\n",
        "    def selection(self):\n",
        "        \"\"\"Traverse the tree to select a promising node to expand.\n",
        "        \n",
        "        Returns:\n",
        "            - node  (MCTSNode): The selected node to expand.\n",
        "            - is_goal (bool): True if the node is a goal state.\n",
        "        \"\"\"\n",
        "        node = self\n",
        "        while not node.is_leaf():\n",
        "            node = node.best_puct_child()\n",
        "            if self.verbose:\n",
        "                print(f\"Selected node {node.state} with visits {node.parent.N[node.action]} and value {node.parent.Q[node.action]}\")\n",
        "        \n",
        "        return node\n",
        "    \n",
        "    \n",
        "    def expand(self):\n",
        "        \"\"\"Expand the current node by simulating the environment and adding child nodes.\n",
        "\n",
        "        Returns:\n",
        "            child (LearnedMCTSNode): The child node that is a goal state, if any.\n",
        "        \"\"\"\n",
        "        \n",
        "        # s_tensor = F.one_hot(torch.tensor(self.state), self.env.observation_space.n).float().to(self.device)\n",
        "        s_tensor = to_one_hot_encoding(self.state, self.env.observation_space).float().to(self.device)\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            logp, _ = self.net(s_tensor.unsqueeze(0))\n",
        "            p = torch.exp(logp).cpu().numpy()[0]\n",
        "        \n",
        "        for action in range(self.env.action_space.n):\n",
        "            env_copy = self.make_env()\n",
        "            env_copy.reset()\n",
        "            env_copy.unwrapped.s = self.state\n",
        "            \n",
        "            obs, reward, terminated, truncated, _ = env_copy.step(action)\n",
        "\n",
        "            # Only add child if it is not a (non successful) terminal state or if it is not the same state\n",
        "            # if reward == 0 and terminated or self.state == obs:\n",
        "            #     continue\n",
        "\n",
        "            child = LearnedMCTSNode(obs,  \n",
        "                                    make_env=self.make_env,\n",
        "                                    parent=self,\n",
        "                                    action=action, \n",
        "                                    prior=p[action],\n",
        "                                    cpuct=self.puct_constant,\n",
        "                                    device=self.device,\n",
        "                                    net=self.net,\n",
        "                                    verbose=self.verbose)\n",
        "            \n",
        "            self.children[action] = child\n",
        "\n",
        "            if terminated or obs == self.state:\n",
        "                child.terminal = True\n",
        "                child.reward = reward if reward == 1 else -1\n",
        "                \n",
        "            if reward == 1:\n",
        "                return child\n",
        "            \n",
        "        if self.verbose:\n",
        "            print(f\"Expanded node {self.state} with children: {[c.state for c in self.children.values()]}\")\n",
        "        \n",
        "        return None\n",
        "    \n",
        "\n",
        "    def evaluation(self):\n",
        "        \"\"\"Evaluate the current node using the neural network.\n",
        "        Returns:\n",
        "            value (float): The value of the current node.\n",
        "        \"\"\"\n",
        "        # s_tensor = F.one_hot(torch.tensor(self.state), self.env.observation_space.n).float().to(self.device)\n",
        "        s_tensor = to_one_hot_encoding(self.state, self.env.observation_space).float().to(self.device)\n",
        "        with torch.no_grad():\n",
        "            _, value = self.net(s_tensor.unsqueeze(0))\n",
        "            \n",
        "        return value.item() \n",
        "\n",
        "    def backpropagation(self, value):\n",
        "        \"\"\"Propagate the simulation result back up the tree.\"\"\"\n",
        "        node = self\n",
        "        while node:\n",
        "            parent = node.parent\n",
        "            if parent:\n",
        "                parent.N[node.action] += 1\n",
        "                parent.W[node.action] += value\n",
        "                parent.Q[node.action] = parent.W[node.action] / parent.N[node.action]\n",
        "                node = parent\n",
        "            else:\n",
        "                break            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def run_mcts(root_node,\n",
        "             tau=1.0, \n",
        "             num_sims=1000,\n",
        "             pipeline_verbose=False):\n",
        "    \"\"\"Run MCTS simulations from the given node.\"\"\"\n",
        "    \n",
        "    root_node.expand()\n",
        "    \n",
        "    for _ in range(num_sims):\n",
        "        node = root_node.selection()\n",
        "\n",
        "        if pipeline_verbose:\n",
        "            print(f\"\\nSELECTED NODE: {node.state}, with visits {node.parent.N[node.action]} and value {node.parent.Q[node.action]}\\n\")\n",
        "        \n",
        "        if node.terminal:\n",
        "            if pipeline_verbose: \n",
        "                print(f\"Terminal node reached: {node.parent.state} -> {node.state}, with reward {node.reward}\")\n",
        "            node.backpropagation(node.reward)\n",
        "            continue\n",
        "        \n",
        "        # Check if the node had been visited before\n",
        "        if node.parent.N[node.action] > 0:\n",
        "            # If the node has been visited before, expand it\n",
        "            goal_node = node.expand()\n",
        "            \n",
        "            if pipeline_verbose:\n",
        "                print(f\"\\nEXPANDED NODE: {node.state}, with children {[c.state for c in node.children.values()]}\")\n",
        "        \n",
        "            # If the node is a goal state, select it, otherwise select a random child\n",
        "            node = goal_node if goal_node is not None else node.best_puct_child()\n",
        "            if pipeline_verbose:\n",
        "                print(f\"selected node {node.state} from children {[c.state for c in node.parent.children.values()]}.\")\n",
        "                print(f\"is goal node: {goal_node is not None}\\n\")\n",
        "            \n",
        "            \n",
        "        # If the node is a terminal state, use its reward as the value\n",
        "        if node.terminal:\n",
        "            value = node.reward\n",
        "            if pipeline_verbose:\n",
        "                print(f\"BACKPROPAGATING REWARD: {value} from terminal node {node.state}\")\n",
        "        else:\n",
        "            value = node.evaluation() # get value from NN\n",
        "            if pipeline_verbose:\n",
        "                print(f\"BACKPROPAGATING VALUE: {value} from non-terminal node {node.state}\")\n",
        "            \n",
        "        node.backpropagation(value)\n",
        "        \n",
        "        \n",
        "        \n",
        "    counts = np.array([root_node.N[a] for a in range(root_node.env.action_space.n)])\n",
        "    \n",
        "    counts = counts**(1 / tau)\n",
        "    \n",
        "    pi = counts / counts.sum()\n",
        "    \n",
        "    return pi\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.07192478 0.36635814 0.06785789 0.36285    0.06658941 0.06441979]\n"
          ]
        }
      ],
      "source": [
        "root_node = LearnedMCTSNode(state=(0,1,0),\n",
        "                            make_env=make_env,\n",
        "                            net=net,\n",
        "                            cpuct=CPUCT,\n",
        "                            device=device,\n",
        "                            verbose=False)\n",
        "\n",
        "pi = run_mcts(root_node, tau=2., num_sims=5000)\n",
        "print(pi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action probabilities: [0.06464873 0.37371588 0.06464873 0.36547027 0.06642026 0.06509613]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Action probabilities: {pi}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def self_play_episode(\n",
        "    make_env,\n",
        "    net,\n",
        "    num_sims=100,\n",
        "    tau=1.,\n",
        "    cpuct=1.41,\n",
        "    device='cpu',\n",
        "    verbose=False\n",
        "):\n",
        "    data = []\n",
        "    env = make_env()\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        root_node = LearnedMCTSNode(state=state,\n",
        "                                    make_env=make_env,\n",
        "                                    net=net,\n",
        "                                    cpuct=cpuct,\n",
        "                                    device=device,\n",
        "                                    verbose=verbose)\n",
        "        \n",
        "        pi = run_mcts(root_node, tau=tau, num_sims=num_sims)\n",
        "        \n",
        "        action = np.random.choice(np.arange(len(pi)), p=pi)\n",
        "        \n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        \n",
        "        data.append((state, pi))\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            done = True\n",
        "            \n",
        "        state = next_state\n",
        "        \n",
        "    return data, reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(net, dataloader, device,\n",
        "          optimizer,\n",
        "          scheduler,\n",
        "          epoch_start=0, epoch_stop=20, cpu=0):\n",
        "    \"\"\"\n",
        "    Train the AlphaZero network using MCTS-generated dataset.\n",
        "\n",
        "    Args:\n",
        "        net: Neural network model.\n",
        "        dataset: Training dataset (raw data to be wrapped with board_data).\n",
        "        device: torch.device (e.g., 'cuda' or 'cpu').\n",
        "        optimizer: torch.optim optimizer (e.g., Adam).\n",
        "        scheduler: torch.optim.lr_scheduler instance.\n",
        "        epoch_start: Starting epoch index.\n",
        "        epoch_stop: Stopping epoch index.\n",
        "        cpu: Random seed / CPU identifier.\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(cpu)\n",
        "    net.train()\n",
        "\n",
        "    # Use custom loss function\n",
        "    criterion = AlphaLoss()\n",
        "\n",
        "    losses_per_epoch = []\n",
        "\n",
        "    # Outer progress bar for epochs\n",
        "    epoch_bar = tqdm(range(epoch_start, epoch_stop), desc=\"Epochs\", position=0)\n",
        "    for epoch in epoch_bar:\n",
        "        scheduler.step()  # Step the learning rate scheduler\n",
        "\n",
        "        total_loss = 0.0\n",
        "        losses_per_batch = []\n",
        "\n",
        "        # Inner progress bar for batches\n",
        "        batch_bar = tqdm(enumerate(dataloader, 0),\n",
        "                         total=len(dataloader),\n",
        "                         desc=f\"Epoch {epoch + 1}\",\n",
        "                         leave=False,\n",
        "                         position=1)\n",
        "\n",
        "        for i, data in batch_bar:\n",
        "            state, policy, value = data\n",
        "\n",
        "            # Move tensors to GPU or CPU\n",
        "            state = state.to(device).float()\n",
        "            policy = policy.to(device).float()\n",
        "            value = value.to(device).float()\n",
        "\n",
        "            # Forward + backward + optimization step\n",
        "            optimizer.zero_grad()\n",
        "            policy_pred, value_pred = net(state)\n",
        "            loss = criterion(value_pred[:, 0], value, policy_pred, policy)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track total loss for this batch\n",
        "            total_loss += loss.item()\n",
        "            batch_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "            # Periodic logging every 10 batches\n",
        "            if i % 10 == 9:\n",
        "                avg_loss = total_loss / 10\n",
        "                tqdm.write(f'[Epoch {epoch + 1}, Batch {i + 1}] Avg Loss: {avg_loss:.4f}')\n",
        "                tqdm.write(f'Policy: GT {policy[0].argmax().item()}, Pred {policy_pred[0].argmax().item()}')\n",
        "                tqdm.write(f'Value:  GT {value[0].item()}, Pred {value_pred[0, 0].item()}')\n",
        "                losses_per_batch.append(avg_loss)\n",
        "                total_loss = 0.0\n",
        "\n",
        "        # Epoch-level loss tracking\n",
        "        if losses_per_batch:\n",
        "            epoch_avg_loss = sum(losses_per_batch) / len(losses_per_batch)\n",
        "            losses_per_epoch.append(epoch_avg_loss)\n",
        "            epoch_bar.set_postfix(avg_epoch_loss=epoch_avg_loss)\n",
        "\n",
        "        # Early stopping criterion (very conservative)\n",
        "        if len(losses_per_epoch) > 100:\n",
        "            recent = sum(losses_per_epoch[-4:-1]) / 3\n",
        "            earlier = sum(losses_per_epoch[-16:-13]) / 3\n",
        "            if abs(recent - earlier) <= 0.01:\n",
        "                tqdm.write(\"Early stopping criterion met.\")\n",
        "                break\n",
        "\n",
        "    # Final loss vs. epoch plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(222)\n",
        "    ax.scatter([e for e in range(1, len(losses_per_epoch) + 1)], losses_per_epoch)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss per batch\")\n",
        "    ax.set_title(\"Loss vs Epoch\")\n",
        "    os.makedirs(\"model_data/\", exist_ok=True)\n",
        "    plt.savefig(os.path.join(\"model_data/\", f\"Loss_vs_Epoch_{datetime.datetime.today().strftime('%Y-%m-%d')}.png\"))\n",
        "    tqdm.write(\"Finished Training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "      \n",
        "def evaluate(net, make_env, num_episodes=50): \n",
        "    \"\"\"Evaluate the trained model on the environment.\n",
        "\n",
        "    Args:\n",
        "        net: Neural network model.\n",
        "        make_env: Function to create a new environment instance.\n",
        "        num_episodes: Number of episodes to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        success_rate: Success rate of the model in the environment.\n",
        "    \"\"\"\n",
        "    \n",
        "    success_count = 0\n",
        "    net.eval()\n",
        "    \n",
        "    with tqdm(total=num_episodes, desc=\"Evaluating\", position=0) as pbar:\n",
        "        for episode in range(num_episodes):\n",
        "            env = make_env()\n",
        "            state, _ = env.reset()\n",
        "            \n",
        "            node = LearnedMCTSNode(state=state,\n",
        "                                    make_env=make_env,\n",
        "                                    net=net,\n",
        "                                    device=device)\n",
        "              \n",
        "            done = False\n",
        "            \n",
        "            while not node.is_leaf():\n",
        "                logp, _ = net(to_one_hot_encoding(state, env.observation_space).float().to(device).unsqueeze(0))\n",
        "                p = torch.exp(logp).cpu().numpy()[0]\n",
        "                action = np.random.choice(np.arange(len(p)), p=p)\n",
        "                node = node.children[action]\n",
        "                \n",
        "            if node.terminal and node.reward == 1:\n",
        "                success_count += 1\n",
        "            \n",
        "            pbar.set_postfix(success_rate=success_count / (episode + 1))\n",
        "            pbar.update(1)\n",
        "            \n",
        "            \n",
        "    success_rate = success_count / num_episodes\n",
        "    \n",
        "    return success_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_pipeline( net,\n",
        "                    make_env,\n",
        "                    optimizer,\n",
        "                    scheduler,\n",
        "                    buffer_size=20000,\n",
        "                    sample_size=2048,\n",
        "                    batch_size=128,\n",
        "                    num_episodes=10,\n",
        "                    num_self_play=10,\n",
        "                    eval_interval=1,\n",
        "                    num_eval=200,\n",
        "                    target_sr=0.90,\n",
        "                    device = 'cpu',\n",
        "                    verbose = False):\n",
        "    \n",
        "    env = make_env()\n",
        "    best_net = copy.deepcopy(net)\n",
        "    \n",
        "    replay_buffer = ReplayBuffer(buffer_size=buffer_size,\n",
        "                                 sample_size=sample_size)\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        \n",
        "        # ------------- Self-play -------------\n",
        "        self_play_bar = tqdm(range(num_self_play),\n",
        "                            desc=f\"Episode {episode+1}/{num_episodes} — Self-play\",\n",
        "                            position=0)\n",
        "\n",
        "        for g in self_play_bar:\n",
        "            data, reward = self_play_episode(make_env=make_env,\n",
        "                                            net=net,\n",
        "                                            num_sims=NUM_SIMS,\n",
        "                                            tau=TAU,\n",
        "                                            cpuct=CPUCT,\n",
        "                                            device=device,\n",
        "                                            verbose=verbose)\n",
        "\n",
        "            for state, pi in data:\n",
        "                # Convert state to one-hot encoding\n",
        "                replay_buffer.add(state=state,\n",
        "                                mcts_policy=pi,\n",
        "                                value=reward)\n",
        "\n",
        "            if verbose:\n",
        "                tqdm.write(f\"Episode {episode+1}. Self-play episode {g+1} finished with reward {reward}. Buffer size: {len(replay_buffer)}\")\n",
        "            \n",
        "            self_play_bar.set_postfix(reward=reward, buffer=len(replay_buffer))\n",
        "            \n",
        "            \n",
        "        # ------------- Training -------------\n",
        "        if len(replay_buffer) < batch_size:\n",
        "            continue\n",
        "        \n",
        "        net.train()\n",
        "        \n",
        "        experiences = replay_buffer.sample(batch_size)\n",
        "        dataset = ReplayDataset(experiences, obs_space=env.observation_space)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        \n",
        "        # Train the network\n",
        "        train(net=net,\n",
        "              dataloader=dataloader,\n",
        "              device=device,\n",
        "              optimizer=optimizer,\n",
        "              scheduler=scheduler,\n",
        "              epoch_start=0, \n",
        "              epoch_stop=20,\n",
        "              cpu=0)\n",
        "        \n",
        "        # ------------- Evaluation -------------\n",
        "        if (episode + 1) % eval_interval == 0:\n",
        "            success_rate = evaluate(net, make_env, num_episodes=num_eval)\n",
        "            print(f\"Episode {episode+1}. Success rate: {success_rate:.2f}\")\n",
        "            \n",
        "            # Save the best model\n",
        "            if success_rate >= target_sr:\n",
        "                best_net = copy.deepcopy(net)\n",
        "                torch.save(best_net.state_dict(), os.path(f\"models/best_model_{episode+1}.pth\"))\n",
        "                print(f\"Best model saved at episode {episode+1}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode 1/20 — Self-play:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode 1/20 — Self-play:  40%|████      | 4/10 [00:22<00:38,  6.37s/it, buffer=16, reward=-1]"
          ]
        }
      ],
      "source": [
        "\n",
        "train_pipeline(net=net,\n",
        "                make_env=make_env,\n",
        "                optimizer=optimizer,\n",
        "                scheduler=scheduler,\n",
        "                buffer_size=BUFFER_SIZE,\n",
        "                sample_size=2048,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                num_episodes=MAX_EPISODES,\n",
        "                num_self_play=NUM_SELF_PLAY,\n",
        "                eval_interval=EVAL_INTERVAL,\n",
        "                num_eval=NUM_EVAL,\n",
        "                target_sr=TARGET_SR,\n",
        "                device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
