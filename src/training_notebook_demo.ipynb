{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qPaUL1ZltkFd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Discrete, MultiDiscrete\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import deque, defaultdict\n",
        "from tqdm import tqdm\n",
        "from train import AlphaLoss\n",
        "from environments.frozen_lake_manipulation_environment import FrozenLakeManipulationEnv\n",
        "from environments.gripper_environment import GripperDiscretisedEnv\n",
        "from data_loading import to_one_hot_encoding, ReplayBuffer, ReplayDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w9RZ1UOrtkFg"
      },
      "outputs": [],
      "source": [
        "# MCTS / AlphaZero params\n",
        "NUM_SIMS     = 5000       # MCTS simulations/iterations per self-play step\n",
        "NUM_SELF_PLAY = 10       # number of self-play games to generate per epoch/episode\n",
        "CPUCT        = 1.41       # PUCT exploration constant\n",
        "TAU          = 1.0       # temperature for π = N^(1/τ)\n",
        "# Training params\n",
        "BATCH_SIZE   = 128\n",
        "LR           = 1e-3\n",
        "EVAL_INTERVAL= 1       # eval every self-play games\n",
        "TARGET_SR    = 0.95      # stop when success rate ≥ 95%\n",
        "REGULARIZATION = 1e-4    # L2 regularization weight decay constant\n",
        "MAX_EPISODES = 20 # max number of self-play episodes\n",
        "\n",
        "NUM_EVAL     = 50\n",
        "BUFFER_SIZE   = 20000\n",
        "SAMPLE_SIZE   = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ae98cEP9tkFg"
      },
      "outputs": [],
      "source": [
        "class AlphaZeroNet(nn.Module):\n",
        "    def __init__(self, n_states, n_actions, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(n_states, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # policy head\n",
        "        self.policy_head = nn.Linear(hidden_dim, n_actions)\n",
        "        # value head\n",
        "        self.value_head  = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: one-hot or feature vector of shape (batch, n_states)\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        p = F.log_softmax(self.policy_head(h), dim=1)  # log-probs\n",
        "        v = torch.tanh(self.value_head(h))             # in [-1,1]\n",
        "        return p, v.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxPpY3j7tkFh",
        "outputId": "df0d9156-ce4a-483c-ffa3-228592439574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# --- Main Execution ---\n",
        "def make_env():\n",
        "    # return gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n",
        "    return FrozenLakeManipulationEnv()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "env    = make_env()\n",
        "nA = env.action_space.n\n",
        "\n",
        "if isinstance(env.observation_space, Discrete):\n",
        "    nS = env.observation_space.n\n",
        "else:\n",
        "    # Assuming the observation space is a tuple of (states, ..., states, holding/not_holding) \n",
        "    nS = (len(env.observation_space.sample()) - 1) * env.n_states + 1\n",
        "net    = AlphaZeroNet(nS, nA).to(device)\n",
        "optimizer   = optim.Adam(net.parameters(), lr=LR, weight_decay=REGULARIZATION)\n",
        "scheduler   = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kbcu_KVE7u2Z"
      },
      "outputs": [],
      "source": [
        "class LearnedMCTSNode:\n",
        "    def __init__(self, \n",
        "                 state,\n",
        "                 make_env,\n",
        "                 net,\n",
        "                 parent=None, \n",
        "                 action=None, \n",
        "                 prior=0.0,\n",
        "                 cpuct=1.41,\n",
        "                 device='cpu',\n",
        "                 verbose=False):\n",
        "        \n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.action = action            # action taken to reach this node\n",
        "        self.prior = prior              # prior probability of this action\n",
        "        self.children = {}\n",
        "        \n",
        "        self.N = defaultdict(int)       # visit counts per action\n",
        "        self.W = defaultdict(float)     # total reward per action\n",
        "        self.Q = defaultdict(float)     # average reward per action (Q = W/N)\n",
        "        self.reward = 0.0\n",
        "        self.terminal = False\n",
        "        \n",
        "        self.puct_constant = cpuct\n",
        "        \n",
        "        self.make_env = make_env\n",
        "        self.env = make_env()\n",
        "        self.net = net\n",
        "        self.device = device\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return len(self.children) == 0\n",
        "\n",
        "    def puct(self):\n",
        "        \"\"\"Calculate the PUCT value for this node.\n",
        "\n",
        "        Returns:\n",
        "            puct_value (float): The PUCT value for this node.\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.parent.N[self.action] == 0:\n",
        "            return float(\"inf\")\n",
        "        \n",
        "        total_N = sum(self.parent.N.values())\n",
        "        \n",
        "        exploitation = self.parent.Q[self.action]\n",
        "        exploration = self.puct_constant * self.prior * math.sqrt(total_N) / (1 + self.parent.N[self.action])\n",
        "        \n",
        "        return exploitation + exploration\n",
        "        \n",
        "\n",
        "    def best_puct_child(self):\n",
        "        return max(self.children.values(), key=lambda child: child.puct())\n",
        "\n",
        "    def best_child(self):\n",
        "        return max(self.children.values(), key=lambda child: self.Q[child.action])\n",
        "\n",
        "    def selection(self):\n",
        "        \"\"\"Traverse the tree to select a promising node to expand.\n",
        "        \n",
        "        Returns:\n",
        "            - node  (MCTSNode): The selected node to expand.\n",
        "            - is_goal (bool): True if the node is a goal state.\n",
        "        \"\"\"\n",
        "        node = self\n",
        "        while not node.is_leaf():\n",
        "            node = node.best_puct_child()\n",
        "            if self.verbose:\n",
        "                print(f\"Selected node {node.state} with visits {node.parent.N[node.action]} and value {node.parent.Q[node.action]}\")\n",
        "        \n",
        "        return node\n",
        "    \n",
        "    \n",
        "    def expand(self):\n",
        "        \"\"\"Expand the current node by simulating the environment and adding child nodes.\n",
        "\n",
        "        Returns:\n",
        "            child (LearnedMCTSNode): The child node that is a goal state, if any.\n",
        "        \"\"\"\n",
        "        \n",
        "        # s_tensor = F.one_hot(torch.tensor(self.state), self.env.observation_space.n).float().to(self.device)\n",
        "        s_tensor = to_one_hot_encoding(self.state, self.env.observation_space).float().to(self.device)\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            logp, _ = self.net(s_tensor.unsqueeze(0))\n",
        "            p = torch.exp(logp).cpu().numpy()[0]\n",
        "        \n",
        "        for action in range(self.env.action_space.n):\n",
        "            env_copy = self.make_env()\n",
        "            env_copy.reset()\n",
        "            env_copy.unwrapped.s = self.state\n",
        "            \n",
        "            obs, reward, terminated, truncated, _ = env_copy.step(action)\n",
        "\n",
        "            # Only add child if it is not a (non successful) terminal state or if it is not the same state\n",
        "            # if reward == 0 and terminated or self.state == obs:\n",
        "            #     continue\n",
        "\n",
        "            child = LearnedMCTSNode(obs,  \n",
        "                                    make_env=self.make_env,\n",
        "                                    parent=self,\n",
        "                                    action=action, \n",
        "                                    prior=p[action],\n",
        "                                    cpuct=self.puct_constant,\n",
        "                                    device=self.device,\n",
        "                                    net=self.net,\n",
        "                                    verbose=self.verbose)\n",
        "            \n",
        "            self.children[action] = child\n",
        "\n",
        "            if terminated or obs == self.state:\n",
        "                child.terminal = True\n",
        "                child.reward = reward if reward == 1 else -1\n",
        "                \n",
        "            if reward == 1:\n",
        "                return child\n",
        "            \n",
        "        if self.verbose:\n",
        "            print(f\"Expanded node {self.state} with children: {[c.state for c in self.children.values()]}\")\n",
        "        \n",
        "        return None\n",
        "    \n",
        "\n",
        "    def evaluation(self):\n",
        "        \"\"\"Evaluate the current node using the neural network.\n",
        "        Returns:\n",
        "            value (float): The value of the current node.\n",
        "        \"\"\"\n",
        "        # s_tensor = F.one_hot(torch.tensor(self.state), self.env.observation_space.n).float().to(self.device)\n",
        "        s_tensor = to_one_hot_encoding(self.state, self.env.observation_space).float().to(self.device)\n",
        "        with torch.no_grad():\n",
        "            _, value = self.net(s_tensor.unsqueeze(0))\n",
        "            \n",
        "        return value.item() \n",
        "\n",
        "    def backpropagation(self, value):\n",
        "        \"\"\"Propagate the simulation result back up the tree.\"\"\"\n",
        "        node = self\n",
        "        while node:\n",
        "            parent = node.parent\n",
        "            if parent:\n",
        "                parent.N[node.action] += 1\n",
        "                parent.W[node.action] += value\n",
        "                parent.Q[node.action] = parent.W[node.action] / parent.N[node.action]\n",
        "                node = parent\n",
        "            else:\n",
        "                break            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def run_mcts(root_node,\n",
        "             tau=1.0, \n",
        "             num_sims=1000,\n",
        "             pipeline_verbose=False):\n",
        "    \"\"\"Run MCTS simulations from the given node.\"\"\"\n",
        "    \n",
        "    root_node.expand()\n",
        "    \n",
        "    for _ in range(num_sims):\n",
        "        node = root_node.selection()\n",
        "\n",
        "        if pipeline_verbose:\n",
        "            print(f\"\\nSELECTED NODE: {node.state}, with visits {node.parent.N[node.action]} and value {node.parent.Q[node.action]}\\n\")\n",
        "        \n",
        "        if node.terminal:\n",
        "            if pipeline_verbose: \n",
        "                print(f\"Terminal node reached: {node.parent.state} -> {node.state}, with reward {node.reward}\")\n",
        "            node.backpropagation(node.reward)\n",
        "            continue\n",
        "        \n",
        "        # Check if the node had been visited before\n",
        "        if node.parent.N[node.action] > 0:\n",
        "            # If the node has been visited before, expand it\n",
        "            goal_node = node.expand()\n",
        "            \n",
        "            if pipeline_verbose:\n",
        "                print(f\"\\nEXPANDED NODE: {node.state}, with children {[c.state for c in node.children.values()]}\")\n",
        "        \n",
        "            # If the node is a goal state, select it, otherwise select a random child\n",
        "            node = goal_node if goal_node is not None else node.best_puct_child()\n",
        "            if pipeline_verbose:\n",
        "                print(f\"selected node {node.state} from children {[c.state for c in node.parent.children.values()]}.\")\n",
        "                print(f\"is goal node: {goal_node is not None}\\n\")\n",
        "            \n",
        "            \n",
        "        # If the node is a terminal state, use its reward as the value\n",
        "        if node.terminal:\n",
        "            value = node.reward\n",
        "            if pipeline_verbose:\n",
        "                print(f\"BACKPROPAGATING REWARD: {value} from terminal node {node.state}\")\n",
        "        else:\n",
        "            value = node.evaluation() # get value from NN\n",
        "            if pipeline_verbose:\n",
        "                print(f\"BACKPROPAGATING VALUE: {value} from non-terminal node {node.state}\")\n",
        "            \n",
        "        node.backpropagation(value)\n",
        "        \n",
        "        \n",
        "        \n",
        "    counts = np.array([root_node.N[a] for a in range(root_node.env.action_space.n)])\n",
        "    \n",
        "    counts = counts**(1 / tau)\n",
        "    \n",
        "    pi = counts / counts.sum()\n",
        "    \n",
        "    return pi\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.07192478 0.36635814 0.06785789 0.36285    0.06658941 0.06441979]\n"
          ]
        }
      ],
      "source": [
        "root_node = LearnedMCTSNode(state=(0,1,0),\n",
        "                            make_env=make_env,\n",
        "                            net=net,\n",
        "                            cpuct=CPUCT,\n",
        "                            device=device,\n",
        "                            verbose=False)\n",
        "\n",
        "pi = run_mcts(root_node, tau=2., num_sims=5000)\n",
        "print(pi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action probabilities: [0.06464873 0.37371588 0.06464873 0.36547027 0.06642026 0.06509613]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Action probabilities: {pi}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def self_play_episode(\n",
        "    make_env,\n",
        "    net,\n",
        "    num_sims=100,\n",
        "    tau=1.,\n",
        "    cpuct=1.41,\n",
        "    device='cpu',\n",
        "    verbose=False\n",
        "):\n",
        "    data = []\n",
        "    env = make_env()\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        root_node = LearnedMCTSNode(state=state,\n",
        "                                    make_env=make_env,\n",
        "                                    net=net,\n",
        "                                    cpuct=cpuct,\n",
        "                                    device=device,\n",
        "                                    verbose=verbose)\n",
        "        \n",
        "        pi = run_mcts(root_node, tau=tau, num_sims=num_sims)\n",
        "        \n",
        "        action = np.random.choice(np.arange(len(pi)), p=pi)\n",
        "        \n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        \n",
        "        data.append((state, pi))\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            done = True\n",
        "            \n",
        "        state = next_state\n",
        "        \n",
        "    return data, reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(net, dataloader, device,\n",
        "          optimizer,\n",
        "          scheduler,\n",
        "          epoch_start=0, epoch_stop=20, cpu=0):\n",
        "    \"\"\"\n",
        "    Train the AlphaZero network using MCTS-generated dataset.\n",
        "\n",
        "    Args:\n",
        "        net: Neural network model.\n",
        "        dataset: Training dataset (raw data to be wrapped with board_data).\n",
        "        device: torch.device (e.g., 'cuda' or 'cpu').\n",
        "        optimizer: torch.optim optimizer (e.g., Adam).\n",
        "        scheduler: torch.optim.lr_scheduler instance.\n",
        "        epoch_start: Starting epoch index.\n",
        "        epoch_stop: Stopping epoch index.\n",
        "        cpu: Random seed / CPU identifier.\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(cpu)\n",
        "    net.train()\n",
        "\n",
        "    # Use custom loss function\n",
        "    criterion = AlphaLoss()\n",
        "\n",
        "    losses_per_epoch = []\n",
        "\n",
        "    # Outer progress bar for epochs\n",
        "    epoch_bar = tqdm(range(epoch_start, epoch_stop), desc=\"Epochs\", position=0)\n",
        "    for epoch in epoch_bar:\n",
        "        scheduler.step()  # Step the learning rate scheduler\n",
        "\n",
        "        total_loss = 0.0\n",
        "        losses_per_batch = []\n",
        "\n",
        "        # Inner progress bar for batches\n",
        "        batch_bar = tqdm(enumerate(dataloader, 0),\n",
        "                         total=len(dataloader),\n",
        "                         desc=f\"Epoch {epoch + 1}\",\n",
        "                         leave=False,\n",
        "                         position=1)\n",
        "\n",
        "        for i, data in batch_bar:\n",
        "            state, policy, value = data\n",
        "\n",
        "            # Move tensors to GPU or CPU\n",
        "            state = state.to(device).float()\n",
        "            policy = policy.to(device).float()\n",
        "            value = value.to(device).float()\n",
        "\n",
        "            # Forward + backward + optimization step\n",
        "            optimizer.zero_grad()\n",
        "            policy_pred, value_pred = net(state)\n",
        "            loss = criterion(value_pred[:, 0], value, policy_pred, policy)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track total loss for this batch\n",
        "            total_loss += loss.item()\n",
        "            batch_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "            # Periodic logging every 10 batches\n",
        "            if i % 10 == 9:\n",
        "                avg_loss = total_loss / 10\n",
        "                tqdm.write(f'[Epoch {epoch + 1}, Batch {i + 1}] Avg Loss: {avg_loss:.4f}')\n",
        "                tqdm.write(f'Policy: GT {policy[0].argmax().item()}, Pred {policy_pred[0].argmax().item()}')\n",
        "                tqdm.write(f'Value:  GT {value[0].item()}, Pred {value_pred[0, 0].item()}')\n",
        "                losses_per_batch.append(avg_loss)\n",
        "                total_loss = 0.0\n",
        "\n",
        "        # Epoch-level loss tracking\n",
        "        if losses_per_batch:\n",
        "            epoch_avg_loss = sum(losses_per_batch) / len(losses_per_batch)\n",
        "            losses_per_epoch.append(epoch_avg_loss)\n",
        "            epoch_bar.set_postfix(avg_epoch_loss=epoch_avg_loss)\n",
        "\n",
        "        # Early stopping criterion (very conservative)\n",
        "        if len(losses_per_epoch) > 100:\n",
        "            recent = sum(losses_per_epoch[-4:-1]) / 3\n",
        "            earlier = sum(losses_per_epoch[-16:-13]) / 3\n",
        "            if abs(recent - earlier) <= 0.01:\n",
        "                tqdm.write(\"Early stopping criterion met.\")\n",
        "                break\n",
        "\n",
        "    # Final loss vs. epoch plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(222)\n",
        "    ax.scatter([e for e in range(1, len(losses_per_epoch) + 1)], losses_per_epoch)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss per batch\")\n",
        "    ax.set_title(\"Loss vs Epoch\")\n",
        "    os.makedirs(\"model_data/\", exist_ok=True)\n",
        "    plt.savefig(os.path.join(\"model_data/\", f\"Loss_vs_Epoch_{datetime.datetime.today().strftime('%Y-%m-%d')}.png\"))\n",
        "    tqdm.write(\"Finished Training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "      \n",
        "def evaluate(net, make_env, num_episodes=50): \n",
        "    \"\"\"Evaluate the trained model on the environment.\n",
        "\n",
        "    Args:\n",
        "        net: Neural network model.\n",
        "        make_env: Function to create a new environment instance.\n",
        "        num_episodes: Number of episodes to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        success_rate: Success rate of the model in the environment.\n",
        "    \"\"\"\n",
        "    \n",
        "    success_count = 0\n",
        "    net.eval()\n",
        "    \n",
        "    with tqdm(total=num_episodes, desc=\"Evaluating\", position=0) as pbar:\n",
        "        for episode in range(num_episodes):\n",
        "            env = make_env()\n",
        "            state, _ = env.reset()\n",
        "            \n",
        "            node = LearnedMCTSNode(state=state,\n",
        "                                    make_env=make_env,\n",
        "                                    net=net,\n",
        "                                    device=device)\n",
        "              \n",
        "            done = False\n",
        "            \n",
        "            while not node.is_leaf():\n",
        "                logp, _ = net(to_one_hot_encoding(state, env.observation_space).float().to(device).unsqueeze(0))\n",
        "                p = torch.exp(logp).cpu().numpy()[0]\n",
        "                action = np.argmax(p)\n",
        "                node = node.children[action]\n",
        "                \n",
        "            if node.terminal and node.reward == 1:\n",
        "                success_count += 1\n",
        "            \n",
        "            pbar.set_postfix(success_rate=success_count / (episode + 1))\n",
        "            pbar.update(1)\n",
        "            \n",
        "            \n",
        "    success_rate = success_count / num_episodes\n",
        "    \n",
        "    return success_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_pipeline( net,\n",
        "                    make_env,\n",
        "                    optimizer,\n",
        "                    scheduler,\n",
        "                    buffer_size=20000,\n",
        "                    sample_size=2048,\n",
        "                    batch_size=128,\n",
        "                    num_episodes=10,\n",
        "                    num_self_play=10,\n",
        "                    eval_interval=1,\n",
        "                    num_eval=200,\n",
        "                    target_sr=0.90,\n",
        "                    device = 'cpu',\n",
        "                    verbose = False):\n",
        "    \n",
        "    env = make_env()\n",
        "    best_net = copy.deepcopy(net)\n",
        "    best_sr = 0.0\n",
        "    \n",
        "    replay_buffer = ReplayBuffer(buffer_size=buffer_size,\n",
        "                                 sample_size=sample_size)\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        \n",
        "        # ------------- Self-play -------------\n",
        "        self_play_bar = tqdm(range(num_self_play),\n",
        "                            desc=f\"Episode {episode+1}/{num_episodes}\",\n",
        "                            position=0)\n",
        "\n",
        "        for g in self_play_bar:\n",
        "            data, reward = self_play_episode(make_env=make_env,\n",
        "                                            net=net,\n",
        "                                            num_sims=NUM_SIMS,\n",
        "                                            tau=TAU,\n",
        "                                            cpuct=CPUCT,\n",
        "                                            device=device,\n",
        "                                            verbose=verbose)\n",
        "\n",
        "            for state, pi in data:\n",
        "                # Convert state to one-hot encoding\n",
        "                replay_buffer.add(state=state,\n",
        "                                mcts_policy=pi,\n",
        "                                value=reward)\n",
        "\n",
        "            if verbose:\n",
        "                tqdm.write(f\"Episode {episode+1}. Self-play episode {g+1} finished with reward {reward}. Buffer size: {len(replay_buffer)}\")\n",
        "            \n",
        "            self_play_bar.set_postfix(reward=reward, buffer=len(replay_buffer))\n",
        "            \n",
        "            \n",
        "        # ------------- Training -------------\n",
        "        if len(replay_buffer) < batch_size:\n",
        "            continue\n",
        "        \n",
        "        net.train()\n",
        "        \n",
        "        experiences = replay_buffer.sample()\n",
        "        dataset = ReplayDataset(experiences, obs_space=env.observation_space)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        \n",
        "        # Train the network\n",
        "        train(net=net,\n",
        "              dataloader=dataloader,\n",
        "              device=device,\n",
        "              optimizer=optimizer,\n",
        "              scheduler=scheduler,\n",
        "              epoch_start=0, \n",
        "              epoch_stop=20,\n",
        "              cpu=0)\n",
        "        \n",
        "        # ------------- Evaluation -------------\n",
        "        if (episode + 1) % eval_interval == 0:\n",
        "            success_rate = evaluate(net, make_env, num_episodes=num_eval)\n",
        "            print(f\"Episode {episode+1}. Success rate: {success_rate:.2f}\")\n",
        "            \n",
        "            # Save the best model\n",
        "            if success_rate >= target_sr and success_rate >= best_sr:\n",
        "                best_net = copy.deepcopy(net)\n",
        "                best_sr = success_rate\n",
        "                torch.save(best_net.state_dict(), os.path(f\"models/best_model_{episode+1}_sr.pth\"))\n",
        "                print(f\"Best model saved at episode {episode+1}.\")\n",
        "                \n",
        "            elif success_rate > best_sr:\n",
        "                best_net = copy.deepcopy(net)\n",
        "                best_sr = success_rate\n",
        "                torch.save(best_net.state_dict(), os.path(f\"models/best_model_{episode+1}.pth\"))\n",
        "                print(f\"Best model updated at episode {episode+1}.\")\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode 1/20 — Self-play:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode 1/20 — Self-play:  70%|███████   | 7/10 [01:25<00:36, 12.27s/it, buffer=31, reward=-1]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_pipeline(net\u001b[38;5;241m=\u001b[39mnet,\n\u001b[1;32m      2\u001b[0m                 make_env\u001b[38;5;241m=\u001b[39mmake_env,\n\u001b[1;32m      3\u001b[0m                 optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m      4\u001b[0m                 scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m      5\u001b[0m                 buffer_size\u001b[38;5;241m=\u001b[39mBUFFER_SIZE,\n\u001b[1;32m      6\u001b[0m                 sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m      7\u001b[0m                 batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m      8\u001b[0m                 num_episodes\u001b[38;5;241m=\u001b[39mMAX_EPISODES,\n\u001b[1;32m      9\u001b[0m                 num_self_play\u001b[38;5;241m=\u001b[39mNUM_SELF_PLAY,\n\u001b[1;32m     10\u001b[0m                 eval_interval\u001b[38;5;241m=\u001b[39mEVAL_INTERVAL,\n\u001b[1;32m     11\u001b[0m                 num_eval\u001b[38;5;241m=\u001b[39mNUM_EVAL,\n\u001b[1;32m     12\u001b[0m                 target_sr\u001b[38;5;241m=\u001b[39mTARGET_SR,\n\u001b[1;32m     13\u001b[0m                 device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "Cell \u001b[0;32mIn[13], line 30\u001b[0m, in \u001b[0;36mtrain_pipeline\u001b[0;34m(net, make_env, optimizer, scheduler, buffer_size, sample_size, batch_size, num_episodes, num_self_play, eval_interval, num_eval, target_sr, device, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m self_play_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(num_self_play),\n\u001b[1;32m     26\u001b[0m                     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m — Self-play\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m                     position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m self_play_bar:\n\u001b[0;32m---> 30\u001b[0m     data, reward \u001b[38;5;241m=\u001b[39m self_play_episode(make_env\u001b[38;5;241m=\u001b[39mmake_env,\n\u001b[1;32m     31\u001b[0m                                     net\u001b[38;5;241m=\u001b[39mnet,\n\u001b[1;32m     32\u001b[0m                                     num_sims\u001b[38;5;241m=\u001b[39mNUM_SIMS,\n\u001b[1;32m     33\u001b[0m                                     tau\u001b[38;5;241m=\u001b[39mTAU,\n\u001b[1;32m     34\u001b[0m                                     cpuct\u001b[38;5;241m=\u001b[39mCPUCT,\n\u001b[1;32m     35\u001b[0m                                     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     36\u001b[0m                                     verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m state, pi \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# Convert state to one-hot encoding\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         replay_buffer\u001b[38;5;241m.\u001b[39madd(state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[1;32m     41\u001b[0m                         mcts_policy\u001b[38;5;241m=\u001b[39mpi,\n\u001b[1;32m     42\u001b[0m                         value\u001b[38;5;241m=\u001b[39mreward)\n",
            "Cell \u001b[0;32mIn[10], line 23\u001b[0m, in \u001b[0;36mself_play_episode\u001b[0;34m(make_env, net, num_sims, tau, cpuct, device, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     16\u001b[0m     root_node \u001b[38;5;241m=\u001b[39m LearnedMCTSNode(state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[1;32m     17\u001b[0m                                 make_env\u001b[38;5;241m=\u001b[39mmake_env,\n\u001b[1;32m     18\u001b[0m                                 net\u001b[38;5;241m=\u001b[39mnet,\n\u001b[1;32m     19\u001b[0m                                 cpuct\u001b[38;5;241m=\u001b[39mcpuct,\n\u001b[1;32m     20\u001b[0m                                 device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     21\u001b[0m                                 verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m---> 23\u001b[0m     pi \u001b[38;5;241m=\u001b[39m run_mcts(root_node, tau\u001b[38;5;241m=\u001b[39mtau, num_sims\u001b[38;5;241m=\u001b[39mnum_sims)\n\u001b[1;32m     25\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(pi)), p\u001b[38;5;241m=\u001b[39mpi)\n\u001b[1;32m     27\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
            "Cell \u001b[0;32mIn[9], line 24\u001b[0m, in \u001b[0;36mrun_mcts\u001b[0;34m(root_node, tau, num_sims, pipeline_verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Check if the node had been visited before\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mN[node\u001b[38;5;241m.\u001b[39maction] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# If the node has been visited before, expand it\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     goal_node \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mexpand()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pipeline_verbose:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEXPANDED NODE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, with children \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[c\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mnode\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mvalues()]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[8], line 87\u001b[0m, in \u001b[0;36mLearnedMCTSNode.expand\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m s_tensor \u001b[38;5;241m=\u001b[39m to_one_hot_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 87\u001b[0m     logp, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(s_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     88\u001b[0m     p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(logp)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn):\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mAlphaZeroNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# x: one-hot or feature vector of shape (batch, n_states)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m---> 14\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(h))\n\u001b[1;32m     15\u001b[0m     p \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_head(h), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# log-probs\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_head(h))             \u001b[38;5;66;03m# in [-1,1]\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "train_pipeline(net=net,\n",
        "                make_env=make_env,\n",
        "                optimizer=optimizer,\n",
        "                scheduler=scheduler,\n",
        "                buffer_size=BUFFER_SIZE,\n",
        "                sample_size=2048,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                num_episodes=MAX_EPISODES,\n",
        "                num_self_play=NUM_SELF_PLAY,\n",
        "                eval_interval=EVAL_INTERVAL,\n",
        "                num_eval=NUM_EVAL,\n",
        "                target_sr=TARGET_SR,\n",
        "                device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
