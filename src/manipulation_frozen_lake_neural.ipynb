{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1LECiNp1BNlC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqBl4c5aBNlE"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MXtOW7kBNlF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import math\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GeQGqjLBNlG"
      },
      "source": [
        "## Neural Network for Policy and Value Function Approximation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXhbZF3rBNlH"
      },
      "outputs": [],
      "source": [
        "class PolicyValueNetwork(nn.Module):\n",
        "    \"\"\"Neural network for policy and value prediction\"\"\"\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyValueNetwork, self).__init__()\n",
        "\n",
        "        # Shared feature extraction layers\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "\n",
        "        # Policy head\n",
        "        self.policy_head = nn.Linear(128, action_dim)\n",
        "\n",
        "        # Value head\n",
        "        self.value_head = nn.Linear(128, 1)\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shared features\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Policy output (log probabilities)\n",
        "        policy = F.log_softmax(self.policy_head(x), dim=-1)\n",
        "\n",
        "        # Value output (scalar)\n",
        "        value = torch.tanh(self.value_head(x))\n",
        "\n",
        "        return policy, value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a_0hlhFBNlH"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MCTSNode:\n",
        "    \"\"\"Node for Monte Carlo Tree Search with neural guidance\"\"\"\n",
        "    def __init__(self, state, parent=None, action=None, prior=0, env=None,\n",
        "                 policy_net=None, value_net=None, device='cpu'):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.action = action\n",
        "        self.prior = prior  # Prior probability from policy network\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.total_value = 0.0  # Cumulative value from simulations\n",
        "        self.env = env\n",
        "        self.policy_net = policy_net\n",
        "        self.value_net = value_net\n",
        "        self.device = device\n",
        "        self.expanded = False\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return not self.expanded\n",
        "\n",
        "    def uct_score(self, exploration_weight=1.41):\n",
        "        \"\"\"Calculate UCT score for node selection\"\"\"\n",
        "        if self.visits == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        # Use value network prediction as heuristic\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(self.state).unsqueeze(0).to(self.device)\n",
        "            _, value = self.value_net(state_tensor)\n",
        "            heuristic = value.item()\n",
        "\n",
        "        exploitation = self.total_value / self.visits\n",
        "        exploration = exploration_weight * self.prior * math.sqrt(self.parent.visits) / (1 + self.visits)\n",
        "\n",
        "        return exploitation + exploration + 0.1 * heuristic  # Combine with value net prediction\n",
        "\n",
        "    def select_child(self):\n",
        "        \"\"\"Select child with highest UCT score\"\"\"\n",
        "        return max(self.children, key=lambda child: child.uct_score())\n",
        "\n",
        "    def expand(self):\n",
        "        \"\"\"Expand the node using policy network predictions\"\"\"\n",
        "        if self.expanded:\n",
        "            return\n",
        "\n",
        "        # Get action probabilities from policy network\n",
        "        state_tensor = torch.FloatTensor(self.state).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            action_probs, _ = self.policy_net(state_tensor)\n",
        "            action_probs = torch.exp(action_probs).cpu().numpy().flatten()\n",
        "\n",
        "        # Create child nodes for all possible actions\n",
        "        for action in range(self.env.action_space.n):\n",
        "            # Create a copy of the environment to simulate the action\n",
        "            env_copy = copy.deepcopy(self.env)\n",
        "            env_copy.reset()\n",
        "            env_copy.unwrapped.s = self.state\n",
        "\n",
        "            # Take the action\n",
        "            next_state, reward, terminated, truncated, _ = env_copy.step(action)\n",
        "\n",
        "            # Skip invalid states (where state doesn't change)\n",
        "            if np.array_equal(next_state, self.state):\n",
        "                continue\n",
        "\n",
        "            # Create child node with prior from policy network\n",
        "            child = MCTSNode(\n",
        "                next_state,\n",
        "                parent=self,\n",
        "                action=action,\n",
        "                prior=action_probs[action],\n",
        "                env=self.env,\n",
        "                policy_net=self.policy_net,\n",
        "                value_net=self.value_net,\n",
        "                device=self.device\n",
        "            )\n",
        "            self.children.append(child)\n",
        "\n",
        "        self.expanded = True\n",
        "\n",
        "    def rollout(self, max_depth=10000):\n",
        "        \"\"\"Perform a rollout (simulation) from this node\"\"\"\n",
        "        env_copy = copy.deepcopy(self.env)\n",
        "        env_copy.reset()\n",
        "        env_copy.unwrapped.s = self.state\n",
        "\n",
        "        total_reward = 0\n",
        "        depth = 0\n",
        "\n",
        "        while depth < max_depth:\n",
        "            # Use policy network for rollout actions (with some randomness)\n",
        "            state_tensor = torch.FloatTensor(env_copy.unwrapped.s).unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                action_probs, _ = self.policy_net(state_tensor)\n",
        "                action_probs = torch.exp(action_probs).cpu().numpy().flatten()\n",
        "\n",
        "            # Add some noise for exploration\n",
        "            action = np.random.choice(len(action_probs), p=action_probs)\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, terminated, truncated, _ = env_copy.step(action)\n",
        "            total_reward += reward\n",
        "            depth += 1\n",
        "\n",
        "            if total_reward == 1 or terminated or truncated:\n",
        "                return total_reward\n",
        "\n",
        "            # # Update state\n",
        "            # env_copy.unwrapped.s = next_state\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "    def backpropagate(self, value):\n",
        "        \"\"\"Backpropagate the simulation result up the tree\"\"\"\n",
        "        self.visits += 1\n",
        "        self.total_value += value\n",
        "\n",
        "        if self.parent:\n",
        "            self.parent.backpropagate(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlGfY3smBNlJ"
      },
      "outputs": [],
      "source": [
        "class NeuralMCTS:\n",
        "    \"\"\"Neural Monte Carlo Tree Search algorithm\"\"\"\n",
        "    def __init__(self, env, policy_net, value_net, device='cpu',\n",
        "                 num_simulations=1000, exploration_weight=1.0,\n",
        "                 rollout_depth=10000, lr=0.001):\n",
        "        self.env = env\n",
        "        self.policy_net = policy_net\n",
        "        self.value_net = value_net\n",
        "        self.device = device\n",
        "        self.num_simulations = num_simulations\n",
        "        self.exploration_weight = exploration_weight\n",
        "        self.rollout_depth = rollout_depth\n",
        "\n",
        "        # Optimizers\n",
        "        self.policy_optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "        self.value_optimizer = optim.Adam(value_net.parameters(), lr=lr)\n",
        "\n",
        "        # Experience buffer\n",
        "        self.buffer = deque(maxlen=10000)\n",
        "\n",
        "    def search(self, state):\n",
        "        \"\"\"Perform MCTS from given state\"\"\"\n",
        "        root = MCTSNode(\n",
        "            state,\n",
        "            env=self.env,\n",
        "            policy_net=self.policy_net,\n",
        "            value_net=self.value_net,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        for _ in range(self.num_simulations):\n",
        "            node = root\n",
        "\n",
        "            # Selection\n",
        "            while not node.is_leaf():\n",
        "                node = node.select_child()\n",
        "\n",
        "            # Expansion\n",
        "            if node.visits > 0:  # Only expand nodes that have been visited before\n",
        "                node.expand()\n",
        "\n",
        "                # If we expanded into a terminal state, use the actual reward\n",
        "                if len(node.children) == 0:  # Terminal state\n",
        "                    value = 0  # Terminal states get 0 value (we already got the reward)\n",
        "                else:\n",
        "                    # Perform a rollout from the expanded node\n",
        "                    value = node.rollout(self.rollout_depth)\n",
        "            else:\n",
        "                # For unvisited nodes, use value network prediction\n",
        "                state_tensor = torch.FloatTensor(node.state).unsqueeze(0).to(self.device)\n",
        "                with torch.no_grad():\n",
        "                    _, value = self.value_net(state_tensor)\n",
        "                    value = value.item()\n",
        "\n",
        "                # Also do a rollout to get more accurate value estimate\n",
        "                rollout_value = node.rollout(self.rollout_depth)\n",
        "                value = 0.5 * value + 0.5 * rollout_value  # Combine estimates\n",
        "\n",
        "            # Backpropagation\n",
        "            node.backpropagate(value)\n",
        "\n",
        "            # Store experience for training\n",
        "            if node.parent is not None:  # Not the root node\n",
        "                self.buffer.append((\n",
        "                    node.parent.state,\n",
        "                    node.action,\n",
        "                    value,\n",
        "                    node.state\n",
        "                ))\n",
        "\n",
        "        return root\n",
        "\n",
        "    def update_networks(self, batch_size=32):\n",
        "        \"\"\"Update policy and value networks using experience\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample batch\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, values, next_states = zip(*batch)\n",
        "\n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        values = torch.FloatTensor(values).to(self.device)\n",
        "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
        "\n",
        "        # Update policy network\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        log_probs, _ = self.policy_net(states)\n",
        "        policy_loss = -torch.mean(log_probs.gather(1, actions.unsqueeze(1)) * values.unsqueeze(1))\n",
        "        policy_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "\n",
        "        # Update value network\n",
        "        self.value_optimizer.zero_grad()\n",
        "        _, pred_values = self.value_net(states)\n",
        "        value_loss = F.mse_loss(pred_values.squeeze(), values)\n",
        "        value_loss.backward()\n",
        "        self.value_optimizer.step()\n",
        "\n",
        "        return policy_loss.item(), value_loss.item()\n",
        "\n",
        "    def get_action(self, state, temperature=1.0):\n",
        "        \"\"\"Get action from MCTS search\"\"\"\n",
        "        root = self.search(state)\n",
        "\n",
        "        # Get visit counts for each action\n",
        "        visit_counts = np.zeros(self.env.action_space.n)\n",
        "        for child in root.children:\n",
        "            visit_counts[child.action] = child.visits\n",
        "\n",
        "        # Apply temperature to visit counts\n",
        "        if temperature == 0:\n",
        "            action = np.argmax(visit_counts)\n",
        "        else:\n",
        "            visit_probs = visit_counts ** (1/temperature)\n",
        "            visit_probs /= visit_probs.sum()\n",
        "            action = np.random.choice(len(visit_probs), p=visit_probs)\n",
        "\n",
        "        return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhwDrhx8BNlK"
      },
      "outputs": [],
      "source": [
        "class ManipulationEnv(gym.Env):\n",
        "    \"\"\"Custom GridWorld environment with an agent and movable box.\"\"\"\n",
        "    def __init__(self, grid_size=4, agent_start=0, box_start=1, goal_agent=13, goal_box=15, holes=None):\n",
        "        super().__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.n_states = grid_size * grid_size\n",
        "        self.agent_start = agent_start\n",
        "        self.box_start = box_start\n",
        "        self.goal_agent = goal_agent\n",
        "        self.goal_box = goal_box\n",
        "        self.agent_pos = agent_start\n",
        "        self.box_pos = box_start\n",
        "        self.holding = False\n",
        "        self.holes = holes if holes is not None else {5, 7, 11, 12}\n",
        "\n",
        "        # Actions: up, down, left, right, grab, release\n",
        "        self.action_space = spaces.Discrete(6)\n",
        "        self.observation_space = spaces.MultiDiscrete([self.n_states, self.n_states, 2])\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = self.agent_start\n",
        "        self.box_pos = self.box_start\n",
        "        self.holding = False\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return np.array([self.agent_pos, self.box_pos, int(self.holding)])\n",
        "\n",
        "    def _to_coord(self, pos):\n",
        "        return divmod(pos, self.grid_size)\n",
        "\n",
        "    def _to_index(self, row, col):\n",
        "        return row * self.grid_size + col\n",
        "\n",
        "    def _move(self, pos, action):\n",
        "        row, col = self._to_coord(pos)\n",
        "        if action == 0 and row > 0: row -= 1        # up\n",
        "        elif action == 1 and row < self.grid_size - 1: row += 1  # down\n",
        "        elif action == 2 and col > 0: col -= 1      # left\n",
        "        elif action == 3 and col < self.grid_size - 1: col += 1  # right\n",
        "        return self._to_index(row, col)\n",
        "\n",
        "    def step(self, action):\n",
        "        if action in [0, 1, 2, 3]:  # movement\n",
        "            new_pos = self._move(self.agent_pos, action)\n",
        "            if new_pos in self.holes:\n",
        "                return self._get_obs(), 0.0, True, False, {}\n",
        "            if self.holding:\n",
        "                self.agent_pos = new_pos\n",
        "                self.box_pos = new_pos\n",
        "            else:\n",
        "                self.agent_pos = new_pos\n",
        "\n",
        "        elif action == 4:  # grab\n",
        "            if self.agent_pos == self.box_pos:\n",
        "                self.holding = True\n",
        "\n",
        "        elif action == 5:  # release\n",
        "            self.holding = False\n",
        "\n",
        "        # If agent or box is in a hole, terminate\n",
        "        if self.agent_pos in self.holes or self.box_pos in self.holes:\n",
        "            return self._get_obs(), 0.0, True, False, {}\n",
        "\n",
        "        # Check goal condition\n",
        "        success = (\n",
        "            self.agent_pos == self.goal_agent and\n",
        "            self.box_pos == self.goal_box and\n",
        "            not self.holding\n",
        "        )\n",
        "        reward = 1.0 if success else 0.0\n",
        "        return self._get_obs(), reward, success, False, {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAg60aAKBNlK"
      },
      "outputs": [],
      "source": [
        "def train_loop(env, mcts, episodes=100, max_steps=50, update_freq=1):\n",
        "    \"\"\"Training loop for Neural MCTS with tqdm progress tracking\"\"\"\n",
        "    rewards = []\n",
        "    policy_losses = []\n",
        "    value_losses = []\n",
        "    test_rewards = []\n",
        "    success_rate = 0\n",
        "\n",
        "    # Create progress bar\n",
        "    pbar = tqdm(range(episodes), desc=\"Training\", unit=\"episode\")\n",
        "\n",
        "    for episode in pbar:\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "\n",
        "        while not done and step < max_steps:\n",
        "            # Get action from MCTS\n",
        "            action = mcts.get_action(state, temperature=1.0)\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Store experience\n",
        "            mcts.buffer.append((state, action, reward, next_state))\n",
        "\n",
        "            # Update networks periodically\n",
        "            if step % update_freq == 0:\n",
        "                policy_loss, value_loss = mcts.update_networks()\n",
        "                if policy_loss is not None:\n",
        "                    policy_losses.append(policy_loss)\n",
        "                    value_losses.append(value_loss)\n",
        "\n",
        "            state = next_state\n",
        "            step += 1\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "\n",
        "        print(\"DEBUG MESSAGE: FINISHED EPISODE\")\n",
        "\n",
        "        # Update progress bar description\n",
        "        avg_reward = np.mean(rewards[-10:]) if len(rewards) >= 10 else np.mean(rewards)\n",
        "        pbar.set_postfix({\n",
        "            'ep_reward': f\"{episode_reward:.2f}\",\n",
        "            'avg_reward': f\"{avg_reward:.2f}\",\n",
        "            'buffer': len(mcts.buffer)\n",
        "        })\n",
        "\n",
        "        # Periodic testing and logging\n",
        "        if episode % 10 == 0:\n",
        "            test_reward = test_policy(env, mcts)\n",
        "            test_rewards.append(test_reward)\n",
        "            pbar.write(f\"Episode {episode:4d} | \"\n",
        "                      f\"Train Reward: {episode_reward:.2f} | \"\n",
        "                      f\"Avg 10: {avg_reward:.2f} | \"\n",
        "                      f\"Test Reward: {test_reward:.2f}\")\n",
        "\n",
        "    return rewards, policy_losses, value_losses, test_rewards\n",
        "\n",
        "def test_policy(env, mcts, num_tests=10):\n",
        "    \"\"\"Test the current policy greedily\"\"\"\n",
        "    total_reward = 0\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = mcts.get_action(state, temperature=0)  # Greedy\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "\n",
        "        total_reward += episode_reward\n",
        "\n",
        "    return total_reward / num_tests\n",
        "\n",
        "def plot_results(rewards, policy_losses, value_losses):\n",
        "    \"\"\"Plot training results\"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Rewards\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(rewards)\n",
        "    plt.title('Episode Rewards')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "\n",
        "    # Policy Loss\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(policy_losses)\n",
        "    plt.title('Policy Loss')\n",
        "    plt.xlabel('Update Step')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    # Value Loss\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(value_losses)\n",
        "    plt.title('Value Loss')\n",
        "    plt.xlabel('Update Step')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9M0meLLBNlL",
        "outputId": "410d58f8-791c-421d-c2c9-ff9969981049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/50 [00:00<?, ?episode/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/50 [02:51<?, ?episode/s, ep_reward=0.00, avg_reward=0.00, buffer=1e+4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG MESSAGE: FINISHED EPISODE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/50 [13:11<?, ?episode/s, ep_reward=0.00, avg_reward=0.00, buffer=1e+4]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_7351/3102560351.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m rewards, policy_losses, value_losses = train_loop(\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmcts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmcts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_7351/3443452057.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(env, mcts, episodes, max_steps, update_freq)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Periodic testing and logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mtest_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mtest_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             pbar.write(f\"Episode {episode:4d} | \"\n",
            "\u001b[0;32m/tmp/ipykernel_7351/3443452057.py\u001b[0m in \u001b[0;36mtest_policy\u001b[0;34m(env, mcts, num_tests)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Greedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_7351/2688319543.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state, temperature)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;34m\"\"\"Get action from MCTS search\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Get visit counts for each action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_7351/2688319543.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Expansion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_7351/1632198427.py\u001b[0m in \u001b[0;36mselect_child\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;34m\"\"\"Select child with highest UCT score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muct_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_7351/1632198427.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(child)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;34m\"\"\"Select child with highest UCT score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muct_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_7351/1632198427.py\u001b[0m in \u001b[0;36muct_score\u001b[0;34m(self, exploration_weight)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mheuristic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_7351/2798551200.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Value output (scalar)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# 1) Create env and device\n",
        "env = ManipulationEnv()                              # ← your env constructor\n",
        "state_dim  = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "policy_net = PolicyValueNetwork(state_dim, action_dim).to(device)\n",
        "value_net = PolicyValueNetwork(state_dim, action_dim).to(device)\n",
        "\n",
        "# Create MCTS\n",
        "mcts = NeuralMCTS(\n",
        "    env=env,\n",
        "    policy_net=policy_net,\n",
        "    value_net=value_net,\n",
        "    device=device,\n",
        "    num_simulations=5000,\n",
        "    exploration_weight=1.0,\n",
        "    rollout_depth=10000,\n",
        "    lr=0.001\n",
        ")\n",
        "\n",
        "# Train\n",
        "rewards, policy_losses, value_losses = train_loop(\n",
        "    env=env,\n",
        "    mcts=mcts,\n",
        "    episodes=50,\n",
        "    max_steps=20,\n",
        "    update_freq=5\n",
        ")\n",
        "\n",
        "# Plot results\n",
        "plot_results(rewards, policy_losses, value_losses)\n",
        "\n",
        "# Final test\n",
        "final_reward = test_policy(env, mcts, num_tests=20)\n",
        "print(f\"Final Test Reward (greedy): {final_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn_LnuuzBNlN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_apQJjr3BNlN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}